<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Softmax and Cross Entropy Loss]]></title>
    <url>%2F2018%2F04%2F11%2FSoftmax-and-Cross-Entropy-Loss%2F</url>
    <content type="text"><![CDATA[introduction and derivative of Softmax and Cross Entropy Loss. Softmax functionthe softmax function takes N-dimensional vector of real numbers and transforms it into a vector of real number in range of (0,1) which means the probability of each classification. p_i=\frac{e^{a_i}}{\sum_{k=1}^Ne^{a_k}}it is easy to know $\sum_{k=1}^Np_i=1$ . the code of softmax function in python: 123def softmax(X): exps = np.exp(X) return exps / np.sum(exps) sometimes the exp operation can overshot the limit of the floating point numbers in python, so we need to normalize the value by multiply a constant C. p_i=\frac{e^{a_i}}{\sum_{k=1}^Ne^{a_k}} =\frac{Ce^{a_i}}{C\sum_{k=1}^Ne^{a_k}} =\frac{e^{a_i+log(C)}}{\sum_{k=1}^Ne^{a_k+log(C)}}Generally, we max $log(C)=-max(a)$, so we need make a little change of our code: 123def softmax(X): exps = np.exp(X-np.max(X)) return exps / np.sum(exps) Cross Entropy Lossthe defination of cross entropy loss is: L=-\sum_iy_i log(p_i)note: $p_i$ means the softmax value of i-th neuron in fully connected layer; if the true label of input sample is $j$, then $y_{i=j}=1;y_{i\neq j}=0$. the code of loss function in python: 12345678def cross_entropy(X, y): """ X is the output of fully connected layer(N-dimensional vector) y is true label(integer) """ p = softmax(X) loss = -np.log(p[y]) return loss Backward propagationwe need to compute the derivative of cross entropy loss: \frac{\partial L}{\partial a_i}=\frac{\partial L}{\partial p_j}\frac{\partial p_j}{\partial a_i}it is easy to compute $\frac{\partial L}{\partial p_i}$ \frac{\partial L}{\partial p_i}=-\sum_iy_i\frac{1}{p_i}\tag1then compute $\frac{\partial p_j}{\partial a_i}$, we need to divide it into two situations： if $i=j$ \begin{align*} \frac{\partial p_j}{\partial a_i}&=\frac{\partial p_i}{\partial a_i}=\frac{\partial{\frac{e^{a_i}}{\sum_{k=1}^Ne^{a_k}}}}{\partial a_i}=\frac{\sum_{k=1}^{N}e^{a_i}e^{a_k}-(e^{a_i})^2}{(\sum_{k=1}^Ne^{a_k})^2}\\ &=\frac{e^{a_i}}{\sum_{k=1}^Ne^{a_k}}\left(1-\frac{e^{a_i}}{\sum_{k=1}^Ne^{a_k}}\right)\\ &=p_i(1-p_i) \end{align*} If $i\neq j$ \frac{\partial p_j}{\partial a_i}=\frac{\partial{\frac{e^{a_j}}{\sum_{k=1}^Ne^{a_k}}}}{\partial a_i}= -\frac{e^{a_j}}{\sum_{k=1}^Ne^{a_k}}\frac{e^{a_i}}{\sum_{k=1}^Ne^{a_k}}=-p_jp_i\tag2combine the result of (1) and (2) \begin{align*} \frac{\partial L}{\partial a_i}& = \left(-\sum_iy_i\frac{1}{p_i}\right)\frac{\partial p_i}{\partial a_i}\\ &=-\frac{y_i}{p_i}p_i(1-p_i)+\sum_{i\neq j}\frac{y_i}{p_i}p_ip_j\\ &=-y_i+y_ip_i+\sum_{i\neq j}y_ip_i\\ &=-y_i+p_i\sum_iy_i\\ &=p_i-y_i \end{align*}code in python 12345678def delta_entropy(X,y): """ X is the output of fully connected layer(N-dimensional vector) y is true labels(integer) """ grad = softmax(X) grad[y] -= 1 return grad]]></content>
      <categories>
        <category>deep learning</category>
      </categories>
      <tags>
        <tag>softmax</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CNN反向传播算法]]></title>
    <url>%2F2018%2F03%2F21%2FCNN%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[全文翻译自 Notes on Convolutional Neural Networks 论文链接 释部分为自己的补充说明，可能存在错误 1 介绍本文档讨论CNN的推导和实现，并加上一些简单的扩展。相比权值，卷积神经网络涉及到更多的连接。其架构本身实现了正则化。此外，CNN在某种程度上实现了平移不变性。这种特殊的神经网络假设我们希望通过数据驱动的方式学习过滤器，用来提取输入特征。推导是在二维的数据和卷积的基础上展开的，但可以很容易的推广到任意维度。 我们先对经典的BP算法做一个描述，然后推导2D卷积神经网络中卷积层和子采样层（池化层）的BP权值更新方法。在整个过程中，我们强调了视线的效率，给出MATLAB代码片段配合方程讲解。然后讨论如何自动组合前一层的feature map，最后是feature map的稀疏组合。 免责声明：这个笔记可能有错误。 feature map 就是特征矩阵，一张图片的每个像素点的RGB值可分别组成矩阵（R矩阵，G矩阵，B矩阵），这些就是feature map 2 全连接的反向传播算法一个典型的卷积神经网络，开始是卷积层和子采样层交替，最后几层（最接近输出）是全连接的一维网络（全连接层）。当你准备将2D feature map作为一维全连接网络的输入时，直接将所有特征连接为一个一维向量，然后就可以利用BP算法。因此在讨论CNN之前，这里先对BP算法进行解释。 2.1 前向传播 Feedforward Pass在接下来的推导中，我们考虑平方和误差函数，对于一个有c个类别和N个训练样本的多分类问题，计算公式如下： E^N=\frac{1}{2}\sum_{n=1}^{N}\sum_{k=1}^{c}(t_k^n-y_k^n)^2$t_k^n$是第n个样本对应标签的第k个维度的值，$y_k^n$表示第n个样本对应的网络输出的第k个值。对于多分类问题，输出一般为“one-of-c”形式。如果样本$x^n$属于第k类，那么$t_k^n$为正，$t^n$的其他维度即$t_{k\neq1}^n$为0或负数。这取决于输出激活函数的选择，此处不做讨论。 因为整个训练集的误差为每个训练样本的误差和，所以我们先讨论一个样本。第n个样本的误差如下： E^N=\frac{1}{2}\sum_{n=1}^{N}\sum_{k=1}^c(t_k^n-y_k^n)^2=\frac{1}{2}||t^n-y^n||_2^2\tag{1}在全连接层，我们可以根据反向传播规则计算E对网络权值的偏导数。用$l$代表当前层，输出层被指定为$L$层，输入层为第1层，那么当前层的输出为 x^l=f(u^l), with \space u^l=W^lx^{l-1}+b^l\tag{2}输出激活函数$f(\cdot)$一般选择logistic(sigmoid)函数$f(x)=(1+e^{-\beta x})^{-1}$或者hyperbolic tangent函数$f(x)=a\space tanh(bx)$。logistic函数将$[-\infty,\infty]$映射到$[0,1]$，hyperbolic tangent函数将$[-\infty,\infty]$映射到$[-a,a]$，因此hyperbolic tangent函数的输出平均值一般趋于0，sigmoid函数的输出平均为非零。但是如果将训练数据归一化到均值为0方差为1，可以在梯度下降时增加收敛。对于归一化的数据集来说，hyperbolic tangent函数是较好的选择。LeCun推荐$a=1.7159,b=2/3$，所以最大非线性点在$f(\pm1)=\pm1$，因此对预期训练目标进行正则化将会避免训练饱和。 2.2 反向传播 Backpropagation Pass通过网络反向传播的误差可以看作每个神经元关于偏差扰动的敏感度（sensitivities），通过链式法则可得： 敏感度就是误差对目标的偏导数 \begin{equation} \frac{\partial{E}}{\partial{b}}=\frac{\partial{E}}{\partial{u}}\frac{\partial{u}}{\partial{b}}=\delta \end{equation}\tag{3}因为$\frac{\partial{u}}{\partial{b}}=1$，所以bias的敏感度和误差对一个神经元的输入的偏导数是相等的。利用这个关系将高层误差反向传播到底层，使用下面的递推关系式： \delta^t=(W^{l+1})^T\delta^{l+1}\circ f'(u^t)\tag{4}其中$\circ$表示对应位置元素相乘，从公式（1）可以看到，输出层神经元的敏感度会有一些不同： \delta^L=f'(u^L)\circ(y^n-t^n)最后，对每个神经元运用delta规则进行更行，即对神经元的输入使用delta进行缩放。用向量的形式表述就是：输入向量（前一层的输出）和灵敏度向量之间的一个外积： \frac{\partial{E}}{\partial{W^l}}=x^{l-1}(\delta^l)^T\tag{5} \bigtriangleup W^l=-\eta\frac{\partial{E}}{\partial{W^l}}\tag{6}更新偏置值（bias）也是相同的原理，在实际中通常为每个权值赋予一个特定的学习率$\eta_{ij}$。 3 卷积神经网络 Convolutional Neural Networks 公式符号释意 在本文中卷积核有多个，每个卷积核和feature map有多个维度，例如一张图片有RGB值，那么就有三个维度 $i$——卷积核或者feature的第几维度$j$——第几个卷积核$up()$——上采样函数$down()$——下采样函数$l$——当前层 通常卷积层和子采样层交替以减少计算时间并逐步建立空间和结构不变性。为了同时保持特异性，需要小的子采样因子。这并不是什么新鲜的概念，但简洁有效。这种模型在哺乳动物的视觉皮层非常常见，在过去十年，听觉神经科学发现，这些相同的设计范例可以在多种不同动物皮层的主要和带状听觉区域中找到。 分级分析和学习架构可能仍然是听觉领域的关键。 3.1 卷积层 Convolution Layers我们现在推导卷积层的BP更行。在一个卷积层，上一层的feature map与一个卷积核进行卷积运算，然后通过一个激活函数获得输出feature map。每个输出map可能是多个卷积结果的结合，公式表述： x_j^l=f\left(\sum_{i\in M_j}x_i^{l-1}*k_{ij}^{l}+b_j^l\right)$M_j$表示输入maps的集合，在MATLAB中卷积的运算需采用”valid”模式。通常情况下，输入maps选择一对或者三个。后面我们会讨论如何自动选择需要组合的特征map。每一个输出map都会加上一个偏执项$b$，但是对于一个特定的输出map，输入maps会和相同的卷积核进行卷积运算，也就是说，如果输出map $j$ 和 $k$ 都是由map $i$通过卷积运算的结果，那么对应的卷积核是不同的。 一张图片有RGB三个颜色通道，则对应的卷积核也是三维的，输出的feature map就是三个卷积结果的结合 3.1.1 Computing the Gradients 我们假设每个卷积层$l$后面都接一个子采样层 $l+1$。在使用BP算法计算 $l$ 层每个节点的敏感度时，我们需要先对下一层（$l+1$层）连接当前层（$l$层）的节点的敏感度求和，再乘以这些连接对应的权值（连接$l$层和$l+1$层的权值），最后乘以当前层$l$的该神经元节点的输入$u$的激活函数$f$的导数值。因为下采样的存在，采样层的一个像素（神经元）对应的灵敏度$\delta$对应于卷积层（上一层）的输出map的一块像素（采样窗口大小）。因此，$l$层中的map的每个节点只与$l+1$层中相应map的一个节点连接。为了有效计算$l$层的敏感度，我们可以对下采样层的敏感，度map进行“上采样(upsample)”操作，这样可以使下采样层的敏感度map的大小和卷积层的map大小相同，然后将$l$层的激活值偏导map和$l+1$层的上采样敏感度map对应元素相乘。因为下采样层map的权值均是一个常数$\beta$（见3.2节），所以我们只需要将上一步得到的结果乘以 $ \beta$ 就可以得到$l$层的敏感度map $\delta^l$。我们可以在卷积层的每个feature map进行相同的计算过程： \delta_j^l=\beta_j^{l+1}\left(f'(u_j^l)\circ up(\delta_j^{l+1})\right)其中的 $up(\cdot)$表示上采样操作，如果下采样过程的采样因子为$n$，只需将每个像素点在水平方向和垂直方向复制$n$次就可以实现，一种有效的实现方式是利用 Kronecker product: up(x)=x\otimes1_{n\times n} Ref:克罗内克积 现在对于一个给定的map，我们已经可以计算其敏感度map，然后我们可以简单的对敏感度map的所有节点求和计算出其bias（偏置项）梯度: \frac{\partial{E}}{\partial{b_j}}=\sum_{u,v}(\delta_j^l)_{uv}最后，使用BP算法计算权重（卷积核）的梯度，很多权重在连接中是共享的，因此我们需要对所有与该权值有联系（权值共享的连接）的连接对该点求梯度，然后对这些梯度进行求和，就像计算bias梯度一样： \frac{\partial{E}}{\partial{k_{ij}^l}}=\sum_{u,v}(\delta_j^l)_{uv}(p_i^{l-1})_{uv}\tag{7}其中$(p_i^{l-1})_{uv}$是$x_i^{l-1}$在卷积运算时和$k_{ij}^l$按元素相乘得到输出map的$(u,v)$位置值的patch。看上去好像我们我刻意追踪输入map中哪些patch与输出map中的哪些像素（及其对应的敏感map）相对应，但是公式(7)可以通过卷积运算获得: \frac{\partial{E}}{\partial{k_{ij}^l}}=rot180(conv2(x_i^{l-1},rot180(\delta_j^l),'valid'))这里我们对$\delta$敏感度map做旋转操作是为了这样就可以进行互相关计算而不是卷积，之后将输出旋转回来，以便当我们在前馈阶段执行卷积时，卷积核将具有正确的方向。 3.2 子采样层 Sub-sampling Layers子采样层对输入maps进行downsample操作（池化），输入有N个maps，输出也会有N个maps，只是每个maps变小了，公式表述如下： x_j^l=f\left(\beta_j^ldown(x_j^{l-1})+b_j^l\right)其中的$down(\cdot)$是下采样函数。典型的操作一般是对输入图像的不同$n\times n$的块的像素进行求和。这样输出图像在两个维度上都缩小了n倍。每个输出map都有一个属于自己的乘性偏置β和一个加性偏置b。 3.2.1 Computing the Gradients 这里的困难在于如何计算敏感度maps。只要我们能得到敏感度maps，需要更新的只有bias参数 $\beta$ 和 $b$ 。如果子采样层的下一层是全连接层，那么子采样层的敏感度maps只需要利用BP算法就可以容易获得。所以我们假设子采样层的上一层和下一层都是卷积层。 当我们在3.1.1节计算卷积核梯度时，我们需要找到输入map中patch和输出map的像素的对应关系。这里就是必须找到当前层的敏感度map中那个patch与下一层的敏感度矩阵的的给定像素对应，这样就可以像公式 (4)那样应用$\delta$递推。 当然，将输入patch和输出像素之间连接相乘的权重恰好是（旋转的）卷积核的权重。利用卷积可以实现该计算： \delta_j^l=f'(u_j^l)\circ conv2(\delta_j^{l+1},rot180(k_j^{l+1}),full)如前所述，我们旋转内核是为了是卷积函数实施互相关计算。同时，我们需要对卷积边界进行“full”处理，Matlab的卷积函数会自动执行这个过程，对缺少的输入像素进行补零操作。 现在我们可以很容易的计算 $b$ 和 $\beta$ 的梯度，加性基 $b$ 的处理和之前一样，将敏感度map中所有元素相加即可 ： \frac{\partial E}{\partial b_j}=\sum_{u,v}(\delta_j^l)_{uv}对于乘性偏置$\beta$ ，参与了前向传播时下采样map的运算，所以在前向计算时将这些maps保存，这样避免了在反向传播时重复计算。定义： d_j^l=down(x_j^{l-1})$\beta$ 梯度计算公式如下： \frac{\partial E}{\partial \beta_j}=\sum_{u,v}(\delta_j^l) 补充说明：对rot180的解释 假设第$l-1$层输出$x^l$是一个3x3矩阵，第$l$层卷积核$k^l$是一个2x2的矩阵，步幅为1，输出一个2x2的矩阵$z^l$，则有（简化$b^l=0$，单卷积核) x^{l-1}*k^{l}=z^{l}矩阵形式表达： \left( \begin{array}{} x_{11}&x_{12}&x_{13}\\ x_{21}&x_{22}&x_{23}\\ x_{31}&x_{32}&x_{33} \end{array} \right) * \left( \begin{array}{} k_{11}&k_{12}\\ k_{21}&k_{22} \end{array} \right) = \left( \begin{array}{} z_{11}&z_{12}\\ z_{21}&z_{22} \end{array} \right)根据卷积的定义可得： z_{11}=x_{11}k_{11}+x_{12}k_{12}+x_{21}k_{21}+x_{22}k_{22}\\ z_{12}=x_{12}k_{11}+x_{13}k_{12}+x_{22}k_{21}+x_{23}k_{22}\\ z_{21}=x_{21}k_{11}+x_{22}k_{12}+x_{31}k_{21}+x_{32}k_{22}\\ z_{22}=x_{22}k_{11}+x_{23}k_{12}+x_{32}k_{21}+x_{33}k_{22}第$l$层的梯度误差为$\delta^l$，反向传导： \frac{\partial{E}}{\partial{k^l}}=\frac{\partial{E}}{\partial{z^l}}\frac{\partial{z^l}}{\partial{k^l}}=\delta^l\frac{z^l}{k^l}因此对于卷积核$k^l$的梯度为第$l$层的敏感度（梯度）乘上$\frac{\partial{z^l}}{\partial{k^l}}$，分别计算可得 \bigtriangledown{k_{11}}=\delta_{11}x_{11}+\delta_{12}x_{22}+\delta_{21}x_{21}+\delta_{22}x_{22}\\ \bigtriangledown{k_{12}}=\delta_{11}x_{12}+\delta_{12}x_{13}+\delta_{21}x_{22}+\delta_{22}x_{23}\\ \bigtriangledown{k_{21}}=\delta_{11}x_{21}+\delta_{12}x_{22}+\delta_{21}x_{31}+\delta_{22}x_{32}\\ \bigtriangledown{k_{22}}=\delta_{11}x_{22}+\delta_{12}x_{23}+\delta_{21}x_{32}+\delta_{22}x_{33}上面4个式子用矩阵卷积形式表示： \left( \begin{array}{} x_{11}&x_{12}&x_{13}\\ x_{21}&x_{22}&x_{23}\\ x_{31}&x_{32}&x_{33} \end{array} \right) * \left( \begin{array}{} \delta_{11}&\delta_{12}\\ \delta_{21}&\delta_{22} \end{array} \right) = \left( \begin{array}{} \bigtriangledown k_{11}&\bigtriangledown k_{12}\\ \bigtriangledown k_{21}&\bigtriangledown k_{22} \end{array} \right)公式化表达为： \frac{\partial{E}}{\partial{k^l}}=x^{l-1}*\delta^l这里需要注意，在论文中进行了两次旋转，这是因为在MATLAB中conv2函数在计算卷积时除了会对矩阵进行“0”扩展，还会将卷积核进行旋转，然后再计算。例如： 123456789101112131415161718&gt; a =&gt; 1 1 1&gt; 1 1 1&gt; 1 1 1&gt; k =&gt; 1 2 3&gt; 4 5 6&gt; 7 8 9&gt;&gt; &gt;&gt; convn(a,k,'full')&gt; ans =&gt;&gt; 1 3 6 5 3&gt; 5 12 21 16 9&gt; 12 27 45 33 18&gt; 11 24 39 28 15&gt; 7 15 24 17 9&gt; &gt; 因此在编写代码时需要注意保持一致（要么旋转，要么不旋转）。 在3.2节计算 $\delta^l_j$ 也是同理 利用反向传播： \frac{\partial{E}}{\partial{x^{l-1}}}=\frac{\partial{E}}{\partial{z^l}}\frac{\partial{z^l}}{\partial{x^{l-1}}}=\delta^l\frac{z^l}{x^{l-1}}利用上式，可得： \bigtriangledown{x_{11}}=\delta_{11}k_{11}\\ \bigtriangledown{x_{12}}=\delta_{11}k_{12}+\delta_{12}k_{11}\\ \bigtriangledown{x_{13}}=\delta_{12}k_{12}\\ \bigtriangledown{x_{21}}=\delta_{11}k_{21}+\delta_{21}k_{11}\\ \bigtriangledown{x_{22}}=\delta_{11}k_{22}+\delta_{12}k_{21}+\delta_{21}k_{12}+\delta_{22}k_{11}\\ \bigtriangledown{x_{23}}=\delta_{12}k_{22}+\delta_{22}k_{12}\\ \bigtriangledown{x_{31}}=\delta_{21}k_{21}\\ \bigtriangledown{x_{32}}=\delta_{21}k_{22}+\delta_{22}k_{21}\\ \bigtriangledown{x_{33}}=\delta_{22}k_{22}上面九个式子用矩阵卷积形式表示： \left( \begin{array}{} 0&0&0&0\\ 0&\delta_{11}&\delta_{12}&0\\ 0&\delta_{21}&\delta_{22}&0\\ 0&0&0&0 \end{array} \right) * \left( \begin{array}{} k_{22}&k_{21}\\ k_{12}&k_{11} \end{array} \right) = \left( \begin{array}{} \bigtriangledown x_{11}&\bigtriangledown x_{12}&\bigtriangledown x_{13}\\ \bigtriangledown x_{21}&\bigtriangledown x_{22}&\bigtriangledown x_{23}\\ \bigtriangledown x_{31}&\bigtriangledown x_{32}&\bigtriangledown x_{33} \end{array} \right)公式化表达为： \frac{\partial{E}}{\partial{x^{l-1}}}=\delta^l * rot180(k^l) 3.3 学习特征maps组合 Learning Combinations of Feature Maps通常，对不同的maps进行卷积并对结果求和获得一个输出map，往往能取得不错的效果。在一些文献中，通过人工选择输入maps进行组合。但是我们可以尝试通过训练获得这个组合。让 $a_{ij}$ 表示得到第 $j$ 个输出map中第 $i$ 个输入map的权重，那么第 $j$ 个输出map的定义如下： x_j^l=f\left( \sum_{i=1}^{N_{in}}a_{ij}(x_i^{l-1}*k_i^l)+b_j^l\right)同时需满足以下约束： \sum_ia_ij=1,and \space 0\leq a_{ij}\leq1这些约束可以通过将变量$a_{ij}$表示为softmax形式来加强： a_{ij}=\frac{exp(c_{ij})}{\sum_kexp(c_{kj})}因为对于固定的 $j$ 来说，每组权值 $c_{ij}$ 都和其他组权值相独立，所以为了方便描述，我们把下标 $j$ 去掉，只考虑单个map的更新，其他map的更新方式是相同的过程，只是索引 $j$ 不同。 softmax函数的导数： \frac{\partial a_k}{\partial c_i}=\delta_{ki}a_i-a_ia_k\tag{8}这里的 $\delta$ 是 kronecker delta，参照公式 (1) 我们可以得到在 $l$ 层误差对 $a_i$ 的偏导： \frac{\partial E}{\partial a_i}=\frac{\partial E}{\partial u^l}\frac{\partial u^l}{\partial a_i}=\sum_{u,v}(\delta^l \circ (x_i^{l-1}*k_i^l))_{uv}这里的$\delta^l$对应具有输入 $u$ 的输出map的敏感度map。和前面一样，这里的卷积运算也是“valid”类型，目的是使结果和sensitivity map大小匹配。最后使用链式法则计算损失函数对权值 $c_i$ 的偏导数： \begin{align} \frac{\partial E}{\partial c_i}&=\sum_k\frac{\partial E}{\partial a_k}\frac{\partial a_k}{\partial c_i}\tag{9}\\ &=a_i\left( \frac{\partial E}{\partial a_i}-\sum_k\frac{\partial E}{\partial a_k}a_k\right)\tag{10} \end{align} 3.3.1 Enforcing Sparse Combinations 为了给 $a_i$ 增加稀疏约束（限制一个输出map只与某些而不是全部输入map相连接），我们在代价函数中添加正则项惩罚 $\Omega(a)$ 。这样就可以使某些权值趋于0，最后只有部分输入maps参与输出map相连接，代价函数为： \widetilde{E}^n=E^n + \lambda\sum{i,j}|(a){ij}|\tag{11}然后求这个正则项对 $c_i$梯度的影响： \frac{\partial\Omega}{\partial a_i}=\lambda sign(a_i)\tag{12}结合公式 (8) 的结果： \begin{align} \frac{\partial\Omega}{\partial c_i}& = \sum_k\frac{\partial\Omega}{\partial a_k}\frac{\partial a_k}{\partial c_i} \\ &= \lambda\left(|a_i|-a_i\sum_k|a_k|\right) \end{align}最后结合公式 (13) 和公式 (9) ，可以求的权重 $c_i$ 的梯度： \frac{\partial\widetilde{E}^n}{\partial c_i} = \frac{\partial E^n}{\partial c_i} +\frac{\partial \Omega}{\partial c_i}3.4 加快MATLAB训练速度 Making it Fast with MATLAB 与CNN关系不大，不做翻译，可看原文 4 实际训练问题 Practical Training Issues (Incomplete) 与CNN关系不大，不做翻译，可看原文]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ZELDA-男人的浪漫]]></title>
    <url>%2F2018%2F03%2F19%2FZELDA-%E7%94%B7%E4%BA%BA%E7%9A%84%E6%B5%AA%E6%BC%AB%2F</url>
    <content type="text"><![CDATA[如果一款游戏，如果能让你回到儿时，你会去玩吗？ 游戏时间 100 小时，四神兽和boss全灭，120神庙解锁一半，支线任务完成1/4。 拿到游戏到现在过去了10天，除了实习时间，我几乎都在玩这款游戏，这段时间我没有看动漫，甚至连手机都懒得碰。直到现在，在天气晴朗的时候，登上海拉尔大陆的顶端远眺，留给我的依旧是无数秘密。 我无法忘记游戏开始走出出生地的震撼，一个广阔的大陆等着你探索。作为一个普通玩家，或许无法从专业性的角度去阐述这款游戏的优秀，但是我可以慢慢的回想我在游戏中的故事。 开放世界、沙盒玩法可以说是游戏界经常提到的概念，在我以前玩的游戏中，我按照游戏设计者设计好的路线去解锁游戏，在这个过程中，总是会有一种压迫感，游戏设计者在不停的推着我前进。终于，游戏主线支线结束，站在广阔的地图上，我却不知道干什么。 荒野之息却完全不同，游戏剧情“老套”，主线就是打魔王救公主。在游戏的过程中，我却忘记了主线，沉迷于爬山，炸鱼，采矿，做菜等等等等。看到一座很高的山，我费劲力气爬上去，只为看到更好的风景。为发现隐藏在海拉尔大陆的120个神庙而惊喜，进入神庙后又是另一番体验。就连npc都设计的非常巧妙，记得有一个npc，她对我说：“别摸鱼了，快去拯救世界”，令人爆笑。如果这些还不够，还有900个“呀哈哈”等着你去寻找，可能在石头下，也可能在山顶上。 在大概两天后，我终于想起了公主，于是我开始进入主线，四大神兽每个都有不同的故事和解谜方式，不会让人厌烦。打败四大神兽后，我又开始收集13个回忆，慢慢的找到了所有林克和公主的故事，一个傲娇的公主是怎么慢慢成长，怎么喜欢上我的（笑）。接着就是面对最终boss加农了，这时候我却不想推进了，我不想结束这段旅行，还有那么多神庙没有解锁，那么多支线没有做，现在打boss总有种半途而废的感觉。不过游戏总有分别，又过了两天，我带着全村最好的剑，最好的弓，最好的盾，最好的药冲到boss，然后boss就死了，一段回忆后游戏戛然而止······ 在这段时间，做梦的时候我总是会梦到游戏剧情，在室外的时候，看到高楼大厦，我也不经意的会思考怎么爬上去，爬上去会看到什么样的风景。现在我大概知道为什么会沉迷这款游戏了，大概是这款游戏满足了我所有的浪漫。 在童年时候，我会为钓到小鱼小虾而激动，会为爬到大坝上而激动。那时候会做梦，觉得自己是个大侠、冒险家。和小伙伴们去野外（隔壁村）探险，会去招惹村口的凶狗，会骑姥姥家养的小花狗。实际上，直到现在我还想做这些事，还对其乐此不疲，只不过没人陪我了。是的，我长大了，虽然还在校园，但烦恼却不少。所以，我感谢这款游戏，让我找到了冒险的感觉，为游戏中每一个新事物，为前方而满怀期待！ 最后，塞尔达天下第一！]]></content>
      <categories>
        <category>杂谈</category>
      </categories>
      <tags>
        <tag>游戏</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2017年度总结]]></title>
    <url>%2F2018%2F02%2F21%2F2017%E5%B9%B4%E5%BA%A6%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[山重水复疑无路，柳暗花明又一村——陆游 如果让我选出我的2017年度汉字，那么“选择”当之无愧。2017年充满了选择，大三的尾巴与大四的开始，或读研或工作或出国，每个人都必须作出选择。人生是由无数个选择组成的，就像一棵决策树，所有的决策最终决定了你的人生。 2014年，面临大学与专业的选择，我义无反顾。 2017年，面临保研和工作的选择，我万分纠结。 17年的开始，刚到学校的我和大家一样开始努力的找实习。大约是3-5月那段时间，每天都在痛苦的笔试面试，同时在牛客网上刷题并查阅相关面试经验。当时心理压力很大，经常失眠，每天都在忐忑中等待结果。在刷题的过程中会突然陷入恐慌之中，负面情绪仿佛要溢出一般。 很快，残酷的现实让我认识到自己水平之差和准备不足，总是在终面的时候被刷。算法题越刷越多，会的越多却越恐慌，越认识到自己的不足。项目也基本上是课程项目，很难让人眼前一亮。当时身边只有两种人，大神和普通人。大神们拿offer拿到手软，普通人则心惊胆战，认真备战。而弱渣显然是不存在的，因为弱渣连毕业都成问题，是不会去找实习的，而这种人在计算机专业少到可以忽略不计。 在计算机学院，残酷的竞争让我认识到自己的水平之低，一个普通人，可怜的普通人。在这里很能体会到以前高中中等生的心情，普普通通，拼命挣扎。现在想想，当时那段日子非常感谢BB的陪伴，一起努力找实习，互相吹牛安慰，也算是很好的压力排泄方式。 最后阿里四面后被拒，华为二面被拒，腾讯二面被拒，网易笔试被刷，招商银行拿到offer，仔细想想自己好像也只投了这几个公司，毕竟自己信息不足，等到投简历的时候很多公司校招都已经结束了。实习季后期学业紧张，于是放弃了继续面试投入到紧张的学习当中。 时间过得很快，在考试前与家里有些争执，关于读研和工作的问题。我一心想要工作，或许是想一雪前耻，又或许是考虑到程序员本身的问题，是经验更重要还是学历更重要。思绪的复杂是很难用文字表述的，虽然自己很想成为一名科研人士，在计算机前沿探索，然而现实是很残酷的。不是所有人都适合科研，甚至只有少数人才能在领域内取得成果。逃避是弱者的选择，然而我却选择成为了一名弱者。 在大三考试周结束，我并没有去实习，而是去了实验室开始毕业设计的准备，指导我的博士生学长是一个很优秀的人，在他的指导下我很快的完成了毕业设计工作。在关于未来道理的选择上，学长强烈建议我们继续深造，或许是学长的原因，我决定尽可能的保研。最终在一边准备笔试面试，一边做毕设的过程中，我成功保研。 在整个寝室的命运被决定的那一天，空气都变得清新了。四个人中三个保研，一个工作。于是我们进行了简单的聚餐和game，向那段煎熬的日子say goodbye。 之后的日子如同流水账，每天白天实习，晚上玩耍。时间飞逝，我追不上它的脚步，于是站在原地休息，遮住双眼。典型的“鸵鸟算法”，我运用的很熟练。 2017，不那么愉快，连空气都是苦涩的。一个普通人的挣扎，写成年度总结都是那么普通。除了挣扎，2017还充满着孤独，在大三上学期和萌萌同学说再见后，我继续着孤独。直到今天我都无法衡量孤独给我带来的快乐和悲伤的重量，他们是均等的吗？ 2018，希望自己在又一村中努力，山穷水复可以拿来回忆，但是再来一次可就不好了。]]></content>
      <categories>
        <category>生活点滴</category>
      </categories>
      <tags>
        <tag>年度总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[optimize load time of google map with large number of markers]]></title>
    <url>%2F2018%2F01%2F31%2Foptimize-load-time-of-google-map-with-larger-number-of-markers%2F</url>
    <content type="text"><![CDATA[sometimes we need to display a large number of markers on google map，maybe more than 5000, the speed is slow. we can solve this problem by “cluster”. but in some project we need to show all markers, this article is not another cluster approach.]]></content>
      <categories>
        <category>前端</category>
      </categories>
      <tags>
        <tag>google map</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习-决策树]]></title>
    <url>%2F2018%2F01%2F15%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%86%B3%E7%AD%96%E6%A0%91%2F</url>
    <content type="text"><![CDATA[机器学习——周志华 读书笔记 第四章——决策树 4.1 基本流程基本思想：分而治之，每次选择最优属性进行划分 4.2 划分选择目标：随着划分过程的不断进行，决策树的分支结点所包含的样本尽可能属于同一类别，即结点的纯度（purity）越来越高。 4.2.1 信息增益（information gain)信息熵（information entropy）：度量样本集合纯度常用指标，熵越小，纯度越高，假定当前集合D中第k类样本所占的比例为 $p_k(k=1,2,…,|y|)$，则D的信息熵定义为 Ent(D) = - \sum_{k=1}^{|y|}p_{k}log{2}p_{k}信息增益：可以认为是划分前后的熵差，信息增益越大，则意味划分所获的纯度提升越大，即属性“越好”。假定离散属性 a 有 V 个可能的取值${a_{1},a_{2},…a_{v}}$，若采用 a 对样本集 D 进行划分，则会产生 V 个分支结点，其中第 v 个分支结点包含了 D 中所有在属性 a 上取值为 $a^{v}$ 的样本，记为$D^{v}$，可以计算出$D^{v}$的信息熵，再考虑不同分支结点所包含的样本数不同，给分支结点赋予权重$|D^v|/|D|$，即样本数越多的分支影响越大，于是可计算出用属性 a 对样本集 D 进行划分所获得的信息增益: Gain(D,a) = Ent(D) - \sum_{v=1}^{V}\frac{|D^v|}{|D|}Ent(D^v)ID3 决策树学习算法 [Quinlan, 1986]即采用该算法来选择划分属性。 4.2.2 增益率（gain ratio）使用信息增益准则的决策树，会对可取值数目较多的属性有所偏好，这种偏好会弱化决策树的泛化能力。 为了减少这种偏好带来的不利影响，C4.5 决策树算法 [Quinlan, 1986]采用增益率来选择最优划分属性，增益率定义如下： Gain\_ratio(D,a) = \dfrac{Gain(D,a)}{IV(a)}其中 IV(a) = - \sum_{v=1}^{V}\frac{|D^v|}{|D|}\log{2}^{\dfrac{|D^{v}|}{|D|}}属于属性 a 的固有值（intrinsic value），属性 a 的取值数目越多，则 IV(a) 越大。 tips：增益率会对可取值数目较少的属性有所偏好，因此，C4.5算法并不是选择增益率最大的候选划分属性，而是采用启发式[Quinlan, 1993]： 从候选属性中找出信息增益高于平均的属性组成集合 W 从 W 中选出增益率最高的作为最优划分属性 4.2.3 基尼指数（Gini index）CART 决策树算法[Breiman et al.,1984] 使用基尼系数来选择划分属性，数据集的纯度采用基尼值度量： \begin{equation} \begin{aligned}Gini(D) & = \sum_{k=1}^{|y|}\sum_{k^{'}\not{=}k}{p_{k}p_{k^{'}}}\\ & =1-\sum_{k=1}^{|y|}p_{k}^{2} \end{aligned} \end{equation}Gini(D)越小，数据集D的纯度越高，属性 a 的基尼指数定义为 Gini\_index(D,a) = \sum_{v=1}^{V}\frac{|D|}{|D^v|}Gini(D^v) 注意：在原书上此部分讲解较少，CART决策树算法采用二分递归分割技术，每次将当前结点分割为两个样本集，最终生成的是一棵二叉树。因此上述的Gini(D)公式的 y = 2：若属性为离散值且可能的取值大于2，则针对每个可能的取值a，根据样本对 k = a 测试的是否将样本分为两类，计算出Gini值和Gini_index，然后选择使Gini_index最小的值作为划分基准；若属性为连续值，参考下一节离散值处理. 在候选属性集合A中，选择哪个使得划分后基尼指数最小的属性作为最优划分属性，即 $a_* = \arg min_{a\in{A}}Gini_index(D,a)$. 4.3 剪枝处理（pruning）目的：降低过拟合的风险 基本策略：预剪枝（prepruning）、后剪枝（post-pruning） 采用留出法，将数据集分为训练集和验证集。 4.3.1 预剪枝在决策树生成过程中，对每个结点在划分前先进行估计，若当前结点的划分不能带来决策树泛化能力的提升，则停止划分并将当前结点标记为叶结点。 计算划分前在验证集上的精度S1 计算划分后在验证集上的精度S2 若S1 &lt; S2，则划分，反正则不划分，归为叶结点 4.3.2 后剪枝先从训练集生成一棵完整的决策树，然后自底向上地对非叶结点进行考察，若将该结点对应的子树换成叶结点能带来泛化性能提升，则将该字数换成叶结点。 生成完整决策树，然后自底向上执行下面三步 计算当前结点在验证集上的精度S1 计算将当前结点领衔的分支剪除后的精度S2 若S1 &lt; S2 ，则剪枝（将该结点的分支剪除，替换为叶结点），否则不剪枝 4.4 连续与缺失值4.4.1 连续值处理对于连续属性，需要进行离散化。最简单的策略是采用二分法（bi-partition），这正是C4.5决策树算法中采用的机制[Quinlan,1993]. CART决策树算法也采用该方法 给定样本集D和连续属性a，假定a在D上出现n个不同的取值，将这些值从小到大排序，记为${a^1,a^2,…,a^n}$. 基于划分点 t 可将D分为子集$D_t^-$和$D_t^+$，其中$D_t^-$包含那些在属性 a 上取值不大于 t 的样本，而 $D_t^+$ 则包含那些在属性 a 上取值大于 t 的样本。对于连续属性 a，我们可考察包含 n-1 个元素的候选划分点集合 T_a = \left\{\frac{a^i+a^{i+1}}{2} | 1 \le i \le n-1\right\}即把区间$[a^i,a^{i+1})$的中位数$\frac{a^i+a^{i+1}}{2}$作为候选划分点，然后，可以像离散属性值一样来考察这些划分点，选择最优的划分点进行样本集合划分。 4.4.2 缺失值处理针对不完整样本，若样本出现大量缺失，简单的放弃是对数据极大的浪费，因此需要考虑利用有缺失属性值的训练样本。 给定训练集 D 和属性 a，令 $\widetilde{D}$ 表示 D 中在属性 a 上没有缺失值的样本子集，假定属性 a 有 V 个可取值 ${a^1,a^2,…,a^V}$, 样本有 y 个类，假设我们为每个样本 x 赋予一个权重 $w_x$ ，定义： \rho=\frac{\sum_{x\in{\widetilde{D}}}w_x}{\sum_{x\in{D}}w_x} \\ \widetilde{p}_k=\frac{\sum_{x\in{\widetilde{D}_k}}w_x}{\sum_{x\in{\widetilde{D}}}w_x} (1\le k \le |y|) \\ \rho=\frac{\sum_{x\in{\widetilde{D}^v}}w_x}{\sum_{x\in{\widetilde{D}}}w_x} (1\le v \le V)对属性a，$\rho$ 表示无缺失样本所占的比例，$\widetilde{p}_k$ 表示无缺失样本中第 k 类所占的比例， $\widetilde{r}_v$ 表示无缺失样本中在属性 a 上取值 $a^v$ 的样本所占的比例，显然： \sum_{k=1}^{|y|}\widetilde{p}_k = 1, \sum_{v=1}^{V}\widetilde{r}_v=1基于上述定义，信息增益计算公式推广为 \begin{equation} \begin{aligned} Gain(D,a)&=\rho\times Gain(\widetilde{D},a) \\ &=\rho \times \left(Ent(\widetilde{D})-\sum_{v=1}^V\widetilde{r}_vEnt(\widetilde{D}^v)\right) \end{aligned} \end{equation}其中 Ent(\widetilde{D}) = -\sum_{k=1}^{|y|}\widetilde{p}_klog_2\widetilde{p}_k若样本 x 在划分属性 a 上的取值已知，则将x划入与其取值对应的子结点，且缺中不变。若样本在属性 a 上取值未知，则将 x 同时划分到所有子结点，且样本权值在与属性值 $a^v$ 对应的子结点中调整为 $\widetilde{r}_v\cdot{w_x}$, 也就是让同一个样本以不同的概率划分到不同的子结点。 C4.5 算法采用了上述解决方案[Quinlan,1993]. 4.5 多变量决策树将每个属性视为坐标空间中的一个坐标轴，则 d 个属性描述的样本就对应了 d 维空间的一个数据点，对样本分类就相当于在这个坐标空间寻找不同类样本的分类边界。传统的单变量决策树形成的分类边界有一个明显的特点：轴平行（axis-parallel），即分类边界是由若干个与坐标轴平行的分段组成。 若采用斜的划分边界，则决策树模型将会简化，这就是多变量决策树（multivariate decision tree），每个非叶节点就是一个线性分类器。如下图 个人总结ID3\ID4.5\CART对比ID3缺点 ID3算法不能处理具有连续值的属性（由于ID3以信息增益为准则选择划分属性，对可取值多的属性有所偏好，这样一来，用二分法进行连续属性的离散化处理时，可取值多的属性就越有可能成为分裂属性，而这样其实是没有意义的） ID3算法不能处理属性具有缺失值的样本 算法会生成很深的树，容易产生过拟合现象 算法一般会优先选择有较多属性值的特征，因为属性值多的特征会有相对较大的信息增益 ID4.5是对ID3的改进，修正了其对较多属性值的偏好。C4.5还弥补了ID3中不能处理特征属性值连续的问题。但是，其存在如下缺点 算法低效，在构造树的过程中，需要对数据集进行多次的顺序扫描和排序，因而导致算法的低效 内存受限，适合于能够驻留于内存的数据集，当训练集大得无法在内存容纳时程序无法运行 CART是二叉树，采用二元切分，即可用于分类也可用于回归。]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>西瓜书</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringMVC Controllers 说明文档]]></title>
    <url>%2F2018%2F01%2F05%2FSpringMVC-Controllers-%E8%AF%B4%E6%98%8E%E6%96%87%E6%A1%A3%2F</url>
    <content type="text"><![CDATA[翻译自官方文档，并加上了自己的理解，解释更加直白。 官方文档地址：Spring Web MVC 1 Declaration使用@Controller注解标记一个类，这个类就是一个SpringMVC Controller对象。 123456789@Controllerpublic class HelloController &#123; @GetMapping("/hello") public String handle(Model model) &#123; model.addAttribute("message", "Hello World!"); return "index"; &#125;&#125; 在XML文件进行配置，告诉Spring应该到哪里去找Controller控制器，加上如下一行，base-package 即controller所在位置 1&lt;context:component-scan base-package="org.example.web"/&gt; 完整XML配置文件 12345678910111213141516&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:p="http://www.springframework.org/schema/p" xmlns:context="http://www.springframework.org/schema/context" xsi:schemaLocation=" http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd"&gt; &lt;context:component-scan base-package="org.example.web"/&gt; &lt;!-- ... --&gt;&lt;/beans&gt; 2 Request Mapping@RequestMapping 注解用于映射Request请求与与controller的处理方法。该注解有多个参数可以配置Request请求的属性，如URL，HTTP method，request parameters，headers，media types。 @RequestMapping 可以用于类也可以用于类方法。当@RequestMapping 标记在Controller 类上的时候，里面使用@RequestMapping 标记的方法的请求地址都是相对于类上的@RequestMapping 而言的；当Controller 类上没有标记@RequestMapping 注解时，方法上的@RequestMapping 都是绝对路径。最终路径都是相对于跟路径”/“的。通过这种组合的方法可以限制Request的匹配。 在SpringMVC 4.3 引入了组合注解来简化@RequestMapping的写法。 @GetMapping @PostMapping @PutMapping @DeleteMapping @PatchMapping 如@GetMapping等价于@RequestMapping(method = RequestMethod.GET)，组合注解通常用于method上，如下 123456789101112131415@RestController@RequestMapping("/persons")class PersonController &#123; @GetMapping("/&#123;id&#125;") public Person getPerson(@PathVariable Long id) &#123; // ... &#125; @PostMapping @ResponseStatus(HttpStatus.CREATED) public void add(@RequestBody Person person) &#123; // ... &#125;&#125; 2.1 URI patterns我们可以使用通配符去匹配request ?匹配一个字符 *匹配一个路径段中的零个或多个字符 **匹配零个或多个路径段 可以使用@PathVariable来声明URI变量并获取其值: 1234@GetMapping("/owners/&#123;ownerId&#125;/pets/&#123;petId&#125;")public Pet findPet(@PathVariable Long ownerId, @PathVariable Long petId) &#123; // ...&#125; URI变量即可以在类层级声明，也可以在方法层级声明: 123456789@Controller@RequestMapping("/owners/&#123;ownerId&#125;")public class OwnerController &#123; @GetMapping("/pets/&#123;petId&#125;") public Pet findPet(@PathVariable("ownerId") Long ownerId, @PathVariable Long petId) &#123; // ... &#125;&#125; URI变量会进行自动类型转换或者抛出TypeMismatchException异常，对于简单的类型，如int, long, Data，默认自动转换，对于复杂类型在此不做详解。 URI变量作为参数时，有两种声明方式： 显性声明，如上代码块： @PathVariable(&quot;ownerId&quot;)，这种声明方式明确规定使用的是URI模版里的ownerId变量。 直接使用@PathVariable，如上代码块@PathVariable Long petId，这种情况下会默认去URI模版寻找跟参数名相同的变量，但只能在使用debug模式才可以。 Request还支持正则匹配，语法格式：{varName:regex}，用regex声明了一个URI变量varName，例如： 1234@GetMapping("/&#123;name:[a-z-]+&#125;-&#123;version:\\d\\.\\d\\.\\d&#125;&#123;ext:\\.[a-z]+&#125;")public void handle(@PathVariable String version, @PathVariable String ext) &#123; // ...&#125; 2.2 Pattern comparison当有多个patterns（模版）匹配到URL时，通过AntPathMatcher.getPatternComparator(String path)去获取最合适的patterns。 对于每一个pattern，根据URI变量和通配符的个数计算出分数，分数越低优先度越高。相同分数则较长者优先度高。 默认映射模版/**不参与比较，优先度最低。 2.3 Matrix variablesMatrix variables可以出现在任意路径段，每个matrix variable由 “;” 分割，例如/cars;color=red;year=2012。多个值既可以用 “,” 分割，如color=red,green,blue，也可以重复变量名，如color=red;color=green;color=blue。 如果一个URL可能含有matrix variables，那么请求映射模版必须使用URI模版去表示。这样可以确保匹配正确，即使matrix variables的位置不固定或不存在。 如下例子，获取matrix variable “q” 12345678// GET /pets/42;q=11;r=22@GetMapping("/pets/&#123;petId&#125;")public void findPet(@PathVariable String petId, @MatrixVariable int q) &#123; // petId == 42 // q == 11&#125; 多个路径段包含matrix variables的情况： 12345678910// GET /owners/42;q=11/pets/21;q=22@GetMapping("/owners/&#123;ownerId&#125;/pets/&#123;petId&#125;")public void findPet( @MatrixVariable(name="q", pathVar="ownerId") int q1, @MatrixVariable(name="q", pathVar="petId") int q2) &#123; // q1 == 11 // q2 == 22&#125; 可以设置matrix variable的required属性，required = false表示该参数不是必须存在的，同时可以设置defaultValue赋予默认值。如下： 1234567// GET /pets/42@GetMapping("/pets/&#123;petId&#125;")public void findPet(@MatrixVariable(required=false, defaultValue="1") int q) &#123; // q == 1&#125; 可以将所有的matrix variables放置于一个Map中： 12345678910// GET /owners/42;q=11;r=12/pets/21;q=22;s=23@GetMapping("/owners/&#123;ownerId&#125;/pets/&#123;petId&#125;")public void findPet( @MatrixVariable MultiValueMap&lt;String, String&gt; matrixVars, @MatrixVariable(pathVar="petId"") MultiValueMap&lt;String, String&gt; petMatrixVars) &#123; // matrixVars: ["q" : [11,22], "r" : 12, "s" : 23] // petMatrixVars: ["q" : 22, "s" : 23]&#125; 最后注意：matrix variables默认是不启用的，因此我们需要在xml文件中进行配置: 12345678910111213&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:mvc="http://www.springframework.org/schema/mvc" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation=" http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc.xsd"&gt; &lt;mvc:annotation-driven enable-matrix-variables="true"/&gt;&lt;/beans&gt; 2.4 Consumable media types利用Content-Type对请求匹配范围进行限制，从而缩小请求的映射范围。 1234@PostMapping(path = "/pets", consumes = "application/json")public void addPet(@RequestBody Pet pet) &#123; // ...&#125; consumes 属性支持否定表达，!text/plain表示除了 “text/plain” 的所有content type。 consumes可以声明在class层级，与其他request mapping attributes不同的是，当声明在class层级时，method层级的consumes属性会覆盖而不是扩展class层级的声明。 2.5 Producible media types利用Accept对请求匹配范围进行限制，从而缩小请求的映射范围。类似2.4: 12345@GetMapping(path = "/pets/&#123;petId&#125;", produces = "application/json;charset=UTF-8")@ResponseBodypublic Pet getPet(@PathVariable String petId) &#123; // ...&#125; 2.6 params, method, headers params 属性用于指定请求参数 method 属性用于限制能够访问的方法类型 //和组合注解@GetMapping等类似 headers 属性用于指定请求头信息 三者都可以缩小请求的映射范围，支持否定表达。 12345678910111213@GetMapping(path = "/pets/&#123;petId&#125;", params = "myParam=myValue")public void findPet(@PathVariable String petId) &#123; // ...&#125;@GetMapping(path = "/pets", headers = "myHeader=myValue")public void findPet1(@PathVariable String petId) &#123; // ...&#125;//和findPet1等价@RequestMapping(path = "/pets", headers = "myHeader=myValue", method = RequestMethod. GET)public void findPet2(@PathVariable String petId) &#123; // ...&#125; 3 Handler Methods3.1 Method Arguments 引用自官方文档 Controller method argument Description WebRequest, NativeWebRequest Generic access to request parameters, request &amp; session attributes, without direct use of the Servlet API. javax.servlet.ServletRequest, javax.servlet.ServletResponse Choose any specific request or response type — e.g. ServletRequest, HttpServletRequest, or Spring’s MultipartRequest, MultipartHttpServletRequest. javax.servlet.http.HttpSession Enforces the presence of a session. As a consequence, such an argument is never null.Note: Session access is not thread-safe. Consider setting theRequestMappingHandlerAdapter‘s “synchronizeOnSession” flag to “true” if multiple requests are allowed to access a session concurrently. javax.servlet.http.PushBuilder Servlet 4.0 push builder API for programmatic HTTP/2 resource pushes. Note that per Servlet spec, the injected PushBuilder instance can be null if the client does not support that HTTP/2 feature. java.security.Principal Currently authenticated user; possibly a specific Principal implementation class if known. HttpMethod The HTTP method of the request. java.util.Locale The current request locale, determined by the most specific LocaleResolver available, in effect, the configured LocaleResolver/LocaleContextResolver. Java 6+: java.util.TimeZoneJava 8+: java.time.ZoneId The time zone associated with the current request, as determined by a LocaleContextResolver. java.io.InputStream, java.io.Reader For access to the raw request body as exposed by the Servlet API. java.io.OutputStream, java.io.Writer For access to the raw response body as exposed by the Servlet API. @PathVariable For access to URI template variables. See URI patterns. @MatrixVariable For access to name-value pairs in URI path segments. See Matrix variables. @RequestParam For access to Servlet request parameters. Parameter values are converted to the declared method argument type. See @RequestParam. @RequestHeader For access to request headers. Header values are converted to the declared method argument type. See @RequestHeader. @RequestBody For access to the HTTP request body. Body content is converted to the declared method argument type using HttpMessageConverters. See @RequestBody. HttpEntity&lt;B&gt; For access to request headers and body. The body is converted with HttpMessageConverters. See HttpEntity. @RequestPart For access to a part in a “multipart/form-data” request. See @RequestPart and Multipart requests. java.util.Map, org.springframework.ui.Model, org.springframework.ui.ModelMap For access and updates of the implicit model that is exposed to the web view. RedirectAttributes Specify attributes to use in case of a redirect — i.e. to be appended to the query string, and/or flash attributes to be stored temporarily until the request after redirect. See Redirect attributes and Flash attributes. Command or form object (with optional @ModelAttribute) Command object whose properties to bind to request parameters — via setters or directly to fields, with customizable type conversion, depending on @InitBinder methods and/or the HandlerAdapter configuration (see the webBindingInitializer property onRequestMappingHandlerAdapter).Command objects along with their validation results are exposed as model attributes, by default using the command class name - e.g. model attribute “orderAddress” for a command object of type “some.package.OrderAddress”. @ModelAttribute can be used to customize the model attribute name. Errors, BindingResult Validation results for the command/form object data binding; this argument must be declared immediately after the command/form object in the controller method signature. SessionStatus For marking form processing complete which triggers cleanup of session attributes declared through a class-level @SessionAttributesannotation. UriComponentsBuilder For preparing a URL relative to the current request’s host, port, scheme, context path, and the literal part of the servlet mapping also taking into account Forwarded and X-Forwarded-* headers. @SessionAttribute For access to any session attribute; in contrast to model attributes stored in the session as a result of a class-level @SessionAttributesdeclaration. @RequestAttribute For access to request attributes. 3.2 Return Values 引用自官方文档 Controller method return value Description @ResponseBody The return value is converted through HttpMessageConverters and written to the response. See @ResponseBody. HttpEntity&lt;B&gt;, ResponseEntity&lt;B&gt; The return value specifies the full response including HTTP headers and body be converted through HttpMessageConverters and written to the response. See HttpEntity. HttpHeaders For returning a response with headers and no body. String A view name to be resolved with ViewResolver‘s and used together with the implicit model — determined through command objects and @ModelAttributemethods. The handler method may also programmatically enrich the model by declaring a Modelargument (see above). View A View instance to use for rendering together with the implicit model — determined through command objects and @ModelAttribute methods. The handler method may also programmatically enrich the model by declaring a Model argument (see above). java.util.Map, org.springframework.ui.Model Attributes to be added to the implicit model with the view name implicitly determined through a RequestToViewNameTranslator. ModelAndView object The view and model attributes to use, and optionally a response status. void A method with a void return type (or null return value) is considered to have fully handled the response if it also has a ServletResponse, or an OutputStream argument, or an @ResponseStatus annotation. The same is true also if the controller has made a positive ETag or lastModified timestamp check (see @Controller caching for details).If none of the above is true, a voidreturn type may also indicate “no response body” for REST controllers, or default view name selection for HTML controllers. Callable&lt;V&gt; Produce any of the above return values asynchronously in a Spring MVC managed thread. DeferredResult&lt;V&gt; Produce any of the above return values asynchronously from any thread — e.g. possibly as a result of some event or callback. ListenableFuture, java.util.concurrent.CompletionStage, java.util.concurrent.CompletableFuture Alternative to DeferredResult as a convenience for example when an underlying service returns one of those. ResponseBodyEmitter, SseEmitter Emit a stream of objects asynchronously to be written to the response withHttpMessageConverter‘s; also supported as the body of a ResponseEntity. StreamingResponseBody Write to the response OutputStream asynchronously; also supported as the body of aResponseEntity. Reactive types — Reactor, RxJava, or others via ReactiveAdapterRegistry Alternative to `DeferredResultwith multi-value streams (e.g. Flux, Observable) collected to a List.For streaming scenarios — .e.g. text/event-stream, application/json+stream,SseEmitter and ResponseBodyEmitter are used instead, where ServletOutputStream blocking I/O is performed on a Spring MVC managed thread and back pressure applied against the completion of each write.See Reactive return values. Any other return type A single model attribute to be added to the implicit model with the view name implicitly determined through a RequestToViewNameTranslator; the attribute name may be specified through a method-level @ModelAttribute or otherwise a name is selected based on the class name of the return type. 3.3 @RequestParam使用 @RequestParam 绑定 HttpServletRequest 请求参数到控制器方法参数： 1234567891011121314151617@Controller@RequestMapping("/pets")@SessionAttributes("pet")public class EditPetForm &#123; // ... @GetMapping public String setupForm(@RequestParam("petId") int petId, ModelMap model) &#123; Pet pet = this.clinic.loadPet(petId); model.addAttribute("pet", pet); return "petForm"; &#125; // ...&#125; 绑定的参数默认必须存在，可以通过required属性修改，如@RequestParam(name=&quot;id&quot;, required=false)。 如果控制器方法参数类型不是string，将会执行自动类型转换。 3.4 @RequestHeader使用@RequestHeader绑定绑定 HttpServletRequest 头信息到控制器方法参数。 一个request header例子: 123456Host localhost:8080Accept text/html,application/xhtml+xml,application/xml;q=0.9Accept-Language fr,en-gb;q=0.7,en;q=0.3Accept-Encoding gzip,deflateAccept-Charset ISO-8859-1,utf-8;q=0.7,*;q=0.7Keep-Alive 300 下面的例子演示了如何获取 Accept-Encoding和 Keep-Alive 的值： 12345@RequestMapping("/displayHeaderInfo.do")public void displayHeaderInfo(@RequestHeader("Accept-Encoding") String encoding, @RequestHeader("Keep-Alive") long keepAlive) &#123; //...&#125; 如果控制器方法参数类型不是string，将会执行自动类型转换。 3.5 @CookieValue使用@CookieValue绑定绑定 HttpServletRequest 的cookie信息到控制器方法参数。 假设http request中有如下cookie信息： 1JSESSIONID=415A4AC178C59DACE0B2C9CA727CDD84 下面的例子演示了如何获取JESSIONID的值： 1234@RequestMapping("/displayHeaderInfo.do")public void displayHeaderInfo(@CookieValue("JSESSIONID") String cookie) &#123; //...&#125; 3.6 @ModelAttribute//TODO]]></content>
      <categories>
        <category>SpringMVC</category>
      </categories>
      <tags>
        <tag>Controllers</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[google map 比例尺算法分析]]></title>
    <url>%2F2017%2F12%2F24%2Fgoogle-map-%E6%AF%94%E4%BE%8B%E5%B0%BA%E7%AE%97%E6%B3%95%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[比例尺即地图右下角显示地图距离与实际距离比例的控件，由于Google map自带比例尺控件存在的局限性——无法调整位置和格式，所以通过此文章，介绍比例尺算法及具体实现。 什么是比例尺比例尺是表示图上距离比实地距离缩小的程度，因此也叫缩尺。用公式表示为：比例尺=图上距离/实地距离。在Google map上比例尺显示在右下角，如下图 在创建地图时只需增加设置项 1scaleControl: true 如下例 遗憾的是这样添加的比例尺会存在于右下角，而且不像其他控件一样可以调整位置。如果我们希望修改其位置或者样式，就会无从下手。 自制比例尺实现一个比例尺的关键在于如何获取到地图距离与实际距离的比例和缩放等级及维度之间的关系。google官方api未提供相关函数，因此我们需要自己计算。核心公式为 1ScaleValue = 156543.03392 * Math.cos(latLng.lat() * Math.PI / 180) / Math.pow(2, zoom) 其中zoom为当前缩放等级，latLng.lat()即目标点维度值。该公式是在地球半径为6378137m的基础上计算的，这个值即google地图所采用的值。 有了计算公式后，我们还需要一张表——缩放等级和比例尺对应表，也就是在什么样的缩放等级下使用多大的比例尺，表格如下: 12345678910111213141516171819202122232425262728Zoom Scale0 10000km1 5000km2 2000km3 1000km4 500km5 200km6 200km7 100km8 50km9 20km10 10km11 5km12 2km13 1km14 500m15 200m16 200m17 100m18 50m19 20m20 10m21 5m22 2m23 1m24 1m25 1m26 1m 通过监听地图变化事件（缩放和平移），获取当前屏幕中心点缩放等级和维度获取到scale和scalevalue，那么比例尺的长度（px） = scale/scalevalue。 获取当前比例尺长度的核心代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293/** * 根据缩放等级和维度获取KM数(m数)和像素 */function setScaleInfos(zoomLevel, lat, map) &#123; // 缩放等级-比例尺 var zoomList = [&#123; text: "10000KM", value: 10000 * 1000 &#125;, &#123; text: "5000KM", value: 5000 * 1000 &#125;, &#123; text: "2000KM", value: 2000 * 1000 &#125;, &#123; text: "1000KM", value: 1000 * 1000 &#125;, &#123; text: "500KM", value: 500 * 1000 &#125;, &#123; text: "200KM", value: 200 * 1000 &#125;, &#123; text: "200KM", value: 200 * 1000 &#125;, &#123; text: "100KM", value: 100 * 1000 &#125;, &#123; text: "50KM", value: 50 * 1000 &#125;, &#123; text: "20KM", value: 20 * 1000 &#125;, &#123; text: "10KM", value: 10 * 1000 &#125;, &#123; text: "5KM", value: 5000 &#125;, &#123; text: "2KM", value: 2000 &#125;, &#123; text: "1KM", value: 1000 &#125;, &#123; text: "500m", value: 500 &#125;, &#123; text: "200m", value: 200 &#125;, &#123; text: "200m", value: 200 &#125;, &#123; text: "100m", value: 100 &#125;, &#123; text: "50m", value: 50 &#125;, &#123; text: "20m", value: 20 &#125;, &#123; text: "10m", value: 10 &#125;, &#123; text: "5m", value: 5 &#125;, &#123; text: "2m", value: 2 &#125;, &#123; text: "1m", value: 1 &#125;, &#123; text: "1m", value: 1 &#125;, &#123; text: "1m", value: 1 &#125;, &#123; text: "1m", value: 1 &#125;]; // 宽度 var pxValue = Math.floor(zoomList[zoomLevel].value / (156543.03392 * Math.cos(lat * Math.PI / 180) / Math.pow(2, zoomLevel))); // 更新经纬度数据 $W.id("scaleText").innerHTML = zoomList[zoomLevel].text; $W.id("scaleSize").style.width = pxValue + "px";&#125;; 下面是通过上述思路实现的例子，在地图右下角实现一个比例尺。]]></content>
      <categories>
        <category>前端</category>
      </categories>
      <tags>
        <tag>google map</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[链表考点剖析]]></title>
    <url>%2F2017%2F10%2F30%2F%E9%93%BE%E8%A1%A8%E8%80%83%E7%82%B9%E5%89%96%E6%9E%90%2F</url>
    <content type="text"><![CDATA[链表，最为基础的数据结构，同时也是面试中最容易出现的考察点。很多人都觉得链表如此简单，以至于在面试不断的深入考察中出现纰漏。一旦在链表上出现差错，那么也意味着你基础的羸弱，直接导致面试的失败。 在大三一年实习和后来校招的面试中，遇到了不下于五次链表的题目，题目有难有易，很难保证第一次接触能够完美的给出解答。因此总结了链表相关题目和解法，如有补充请留言。 定义全文的链表节点定义如下，无哑节点，基于C++实现。 12345struct ListNode&#123; int m_nKey; ListNode * m_pNext;&#125;; 题目求节点数目最基础的题目，直接遍历，注意循环停止条件。 12345678910111213unsigned int GetThisLength(ListNode *pHead)&#123; if (pHead == NULL) return 0; ListNode *pCurrent = pHead; unsigned int nlength = 0; while (pCurrent != NULL) &#123; nlength++; pCurrent = pCurrent-&gt;m_pNext; &#125; return nlength;&#125; 反转链表一道非常经典的面试题，简单的同时也非常容易出错，分为递归和非递归解法。 递归解法：每次递归返回的是已经反转好的链表的头结点，注意好跳出条件即可。 非递归解法：遍历链表，已遍历的部分已反转，将正在遍历的节点当做头结点连接到已反转的部分。 123456789101112131415161718192021222324252627//递归解法ListNode * ReverseList(ListNode *pHead)&#123; if (pHead == NULL || pHead-&gt;m_pNext == NULL) return pHead; ListNode *newHead = ReverseList(pHead-&gt;next); pHead-&gt;next-&gt;next = newHead-&gt;next; pHead-&gt;next = NULL; return newHead;&#125;//非递归解法ListNode * ReverseList(ListNode *pHead)&#123; if (pHead == NULL || pHead-&gt;m_pNext == NULL) return pHead; ListNode *pReverseHead = NULL; ListNode *pCurrent = pHead; while (pCurrent != NULL) &#123; ListNode *temp = pCurrent; pCurrent = pCurrent-&gt;m_pNext; temp-&gt;m_pNext = pReverseHead; pReverseHead = temp; &#125; return pReverseHead;&#125; 找倒数第k个节点双指针解题，这中思考方式在后面的题目经常出现。第一个指针先走k步，然后两个指针一起遍历，这样第一个指针到最后一个节点的时候，第二个指针即指向第k个节点。 12345678910111213141516171819ListNode * RGetKthNode(ListNode * pHead, unsigned int k)&#123; if (k == 0 || pHead == NULL) return NULL; ListNode *pAhead = pHead; ListNode *pBehind = pHead; while (pAhead != NULL&amp;&amp;k &gt; 1) &#123; pAhead = pAhead-&gt;m_pNext; k--; &#125; if (k &gt; 1 || pAhead == NULL) return NULL; while (pAhead-&gt;m_pNext != NULL) &#123; pAhead = pAhead-&gt;m_pNext; pBehind = pBehind-&gt;m_pNext; &#125;&#125; 查找中间节点无需先获取长度，只需要一次遍历即可解决问题，和上一题类似思路，两个指针，分别为“快”指针和“慢”指针。快指针每次“走两步”，慢指针每次”走一步“，这样快指针走过的节点始终是慢指针的两倍。快指针到尾节点时，返回慢指针即可。 123456789101112131415ListNode * GetMiddleNode(ListNode * pHead)&#123; if (pHead == NULL || pHead-&gt;m_pNext == NULL) return pHead; ListNode *pAhead = pHead; ListNode *pBehind = pHead; while (pAhead-&gt;m_pNext != NULL) &#123; pAhead = pAhead-&gt;m_pNext; pBehind = pBehind-&gt;m_pNext; if (pAhead-&gt;m_pNext != NULL) pAhead = pAhead-&gt;m_pNext; &#125; return pBehind;&#125; 倒序打印链表借助栈实现，遍历时将节点值存储到栈中。 1234567891011121314void PrintList(ListNode *pHead)&#123; stack&lt;ListNode *&gt; s; while (pHead != NULL) &#123; s.push(pHead); pHead = pHead-&gt;m_pNext; &#125; while (!s.empty()) &#123; cout &lt;&lt; s.top() &lt;&lt; "\t"; s.pop(); &#125;&#125; 合并两个链表遍历对比节点值，直接合并。 12345678910111213141516171819202122232425262728293031323334353637383940414243ListNode *MergeSortedLit(ListNode *pHead1, ListNode *pHead2)&#123; if (pHead1 == NULL) return pHead1; if (pHead2 == NULL) return pHead2; ListNode *pHeadMerged = NULL; if (pHead1-&gt;m_nKey &lt; pHead2-&gt;m_nKey) &#123; pHeadMerged = pHead1; //pHeadMerged-&gt;m_pNext = NULL; pHead1 = pHead1-&gt;m_pNext; &#125; else &#123; pHeadMerged = pHead2; //pHeadMerged-&gt;m_pNext = NULL; pHead2 = pHead2-&gt;m_pNext; &#125; ListNode *pTemp = pHeadMerged; while (pHead1 != NULL &amp;&amp; pHead2 != NULL) &#123; if (pHead1-&gt;m_nKey &lt; pHead2-&gt;m_nKey) &#123; pTemp-&gt;m_pNext = pHead1; pHead1 = pHead1-&gt;m_pNext; pTemp = pTemp-&gt;m_pNext; //pTemp-&gt;m_pNext = NULL; &#125; else &#123; pTemp-&gt;m_pNext = pHead2; pHead2 = pHead2-&gt;m_pNext; pTemp = pTemp-&gt;m_pNext; //pTemp-&gt;m_pNext = NULL; &#125; &#125; if (pHead1 != NULL) pTemp-&gt;m_pNext = pHead1; else if (pHead2 != NULL) pTemp-&gt;m_pNext = pHead2; return pHeadMerged;&#125; 判断单链表是否有环注意该链表有可能是首尾相接，也可能是只有后面一段是环。 经典的快慢指针解题思路，快指针每次走两步，慢指针每次走一步。如果快慢指针相遇则存在环，否则不存在环。 12345678910111213bool HasCircle(ListNode *pHead)&#123; ListNode *pFast = pHead; ListNode *pSlow = pHead; while (pFast != NULL&amp;&amp;pFast-&gt;m_pNext != NULL) &#123; pFast = pFast-&gt;m_pNext-&gt;m_pNext; pSlow = pSlow-&gt;m_pNext; if (pFast == pSlow) return true; &#125; return false;&#125; 判断两个链表是否相交如果两个链表相交，那么他们在相交点之后必定成为同一条链表，所以只需要判断两个链表的尾节点是否相同即可。 12345678910bool IsIntersected(ListNode *pHead1, ListNode *pHead2)&#123; if (pHead1 == NULL&amp;&amp;pHead2 == NULL) return false; while (pHead1-&gt;m_pNext != NULL) pHead1 = pHead1-&gt;m_pNext; while (pHead2-&gt;m_pNext != NULL) pHead2 = pHead2-&gt;m_pNext; return pHead1 == pHead2;&#125; 求两个单链表相交的第一个节点先获取两个链表的长度，然后获取长度差值len，双指针策略，较长的链表的指针先走len步，然后两个指针一起遍历，相遇即跳出，跳出点即相交的第一个节点。 1234567891011121314151617181920212223242526272829303132333435363738394041ListNode * GetFirstCommonNode(ListNode *pHead1, ListNode *pHead2)&#123; if (pHead1 == NULL || pHead2 == NULL) return NULL; int len1 = 1; ListNode *pTail1 = pHead1; while (pTail1-&gt;m_pNext != NULL) &#123; len1++; pTail1 = pTail1-&gt;m_pNext; &#125; int len2 = 1; ListNode *pTail2 = pHead2; while (pTail1-&gt;m_pNext != NULL) &#123; len2++; pTail2 = pTail2-&gt;m_pNext; &#125; if (pTail1 != pTail2) return NULL; ListNode * pNode1 = pHead1; ListNode * pNode2 = pHead2; if (len1 &gt; len2) &#123; int k = len1 - len2; while (k--) pNode1 = pNode1-&gt;m_pNext; &#125; else &#123; int k = len1 - len2; while (k--) pNode1 = pNode1-&gt;m_pNext; &#125; while (pNode1!=pNode2) &#123; pNode1 = pNode1-&gt;m_pNext; pNode2 = pNode2-&gt;m_pNext; &#125; return pNode1;&#125; 已知有环，求入环后第一个节点这题将判断链表是否有环和求两个单链表相交的第一个节点相结合，在快慢指针相遇点断开成两条新的链表，之后两个链表相交的第一个节点就是两个单链表相交的第一个节点。 参考图 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051ListNode * GetFirstNodeInCycle(ListNode *pHead)&#123; if (pHead == NULL || pHead-&gt;m_pNext == NULL) return NULL; ListNode *pFast = pHead; ListNode *pSlow = pHead; while (pFast != NULL&amp;&amp;pFast-&gt;m_pNext != NULL) &#123; pFast = pFast-&gt;m_pNext-&gt;m_pNext; pSlow = pSlow-&gt;m_pNext; if (pFast == pSlow) break; &#125; if (pFast == NULL || pFast-&gt;m_pNext == NULL) return NULL; ListNode *pAssumedTail = pFast; ListNode *pHead1 = pHead; ListNode *pHead2 = pAssumedTail-&gt;m_pNext; ListNode *pNode1 = pHead1; int len1 = 1; while (pNode1 != pAssumedTail) &#123; len1++; pNode1 = pNode1-&gt;m_pNext; &#125; ListNode *pNode2 = pHead2; int len2 = 1; while (pNode2 != pAssumedTail) &#123; len2++; pNode1 = pNode2-&gt;m_pNext; &#125; if (len1 &gt; len2) &#123; int k = len1 - len2; while (k--) pNode1 = pNode1-&gt;m_pNext; &#125; else &#123; int k = len2 - len1; while (k--) pNode2 = pNode2-&gt;m_pNext; &#125; while (pNode1 != pNode2) &#123; pNode1 = pNode1-&gt;m_pNext; pNode2 = pNode2-&gt;m_pNext; &#125; return pNode1;&#125; O(1)删除一个节点一般来说删除一个节点的时间复杂度是O(n)，此处的删除是一种投机的方法，将待删除的节点的下一个节点的值赋予当前节点，然后删除下一个节点。当然如果需要删除的节点是尾节点，还是需要遍历才能删除。 12345678910111213141516171819202122232425262728293031void DeleteNode(ListNode *pHead, ListNode *pToBeDeleted)&#123; if (pToBeDeleted == NULL) return; if (pToBeDeleted-&gt;m_pNext != NULL) &#123; ListNode *pNext = pToBeDeleted-&gt;m_pNext; pToBeDeleted-&gt;m_nKey = pNext-&gt;m_nKey; pToBeDeleted-&gt;m_pNext = pNext-&gt;m_pNext; delete pNext; pNext = NULL; &#125; else &#123; if (pHead == pToBeDeleted) &#123; delete pToBeDeleted; pHead = NULL; pToBeDeleted = NULL; &#125; else &#123; ListNode *pTemp = pHead; while (pTemp-&gt;m_pNext != pToBeDeleted) pTemp = pTemp-&gt;m_pNext; delete pToBeDeleted; pTemp-&gt;m_pNext = NULL; pToBeDeleted = NULL; &#125; &#125;&#125; 考点考点多集中在双指针，快慢指针，遍历等方面。在设计链表题目时，切记要考虑空链表，遍历跳出点以及该链表是否存在哑结点。在做题前需要向面试官询问，并在代码中恰当处理，否则很容易出错。 附录全部代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310#include&lt;iostream&gt;#include&lt;stack&gt;using namespace std;struct ListNode&#123; int m_nKey; ListNode * m_pNext;&#125;;int main()&#123; return 0;&#125;//求节点个数unsigned int GetThisLength(ListNode *pHead)&#123; if (pHead == NULL) return 0; ListNode *pCurrent = pHead; unsigned int nlength = 0; while (pCurrent != NULL) &#123; nlength++; pCurrent = pCurrent-&gt;m_pNext; &#125; return nlength;&#125;//反转链表ListNode * ReverseList(ListNode *pHead)&#123; if (pHead == NULL || pHead-&gt;m_pNext == NULL) return pHead; ListNode *pReverseHead = NULL; ListNode *pCurrent = pHead; while (pCurrent != NULL) &#123; ListNode *temp = pCurrent; pCurrent = pCurrent-&gt;m_pNext; temp-&gt;m_pNext = pReverseHead; pReverseHead = temp; &#125; return pReverseHead;&#125;ListNode * ReverseList(ListNode *pHead)&#123; if (pHead == NULL || pHead-&gt;m_pNext == NULL) return pHead; ListNode *newHead = ReverseList(pHead-&gt;next); pHead-&gt;next-&gt;next = newHead-&gt;next; pHead-&gt;next = NULL; return pReverseHead;&#125;//找倒数第k个节点ListNode * RGetKthNode(ListNode * pHead, unsigned int k)&#123; if (k == 0 || pHead == NULL) return NULL; ListNode *pAhead = pHead; ListNode *pBehind = pHead; while (pAhead != NULL&amp;&amp;k &gt; 1) &#123; pAhead = pAhead-&gt;m_pNext; k--; &#125; if (k &gt; 1 || pAhead == NULL) return NULL; while (pAhead-&gt;m_pNext != NULL) &#123; pAhead = pAhead-&gt;m_pNext; pBehind = pBehind-&gt;m_pNext; &#125;&#125;//查找中间结点ListNode * GetMiddleNode(ListNode * pHead)&#123; if (pHead == NULL || pHead-&gt;m_pNext == NULL) return pHead; ListNode *pAhead = pHead; ListNode *pBehind = pHead; while (pAhead-&gt;m_pNext != NULL) &#123; pAhead = pAhead-&gt;m_pNext; pBehind = pBehind-&gt;m_pNext; if (pAhead-&gt;m_pNext != NULL) pAhead = pAhead-&gt;m_pNext; &#125; return pBehind;&#125;//倒序打印链表void PrintList(ListNode *pHead)&#123; stack&lt;ListNode *&gt; s; while (pHead != NULL) &#123; s.push(pHead); pHead = pHead-&gt;m_pNext; &#125; while (!s.empty()) &#123; cout &lt;&lt; s.top() &lt;&lt; "\t"; s.pop(); &#125;&#125;//合并两个链表ListNode *MergeSortedLit(ListNode *pHead1, ListNode *pHead2)&#123; if (pHead1 == NULL) return pHead1; if (pHead2 == NULL) return pHead2; ListNode *pHeadMerged = NULL; if (pHead1-&gt;m_nKey &lt; pHead2-&gt;m_nKey) &#123; pHeadMerged = pHead1; //pHeadMerged-&gt;m_pNext = NULL; pHead1 = pHead1-&gt;m_pNext; &#125; else &#123; pHeadMerged = pHead2; //pHeadMerged-&gt;m_pNext = NULL; pHead2 = pHead2-&gt;m_pNext; &#125; ListNode *pTemp = pHeadMerged; while (pHead1 != NULL &amp;&amp; pHead2 != NULL) &#123; if (pHead1-&gt;m_nKey &lt; pHead2-&gt;m_nKey) &#123; pTemp-&gt;m_pNext = pHead1; pHead1 = pHead1-&gt;m_pNext; pTemp = pTemp-&gt;m_pNext; //pTemp-&gt;m_pNext = NULL; &#125; else &#123; pTemp-&gt;m_pNext = pHead2; pHead2 = pHead2-&gt;m_pNext; pTemp = pTemp-&gt;m_pNext; //pTemp-&gt;m_pNext = NULL; &#125; &#125; if (pHead1 != NULL) pTemp-&gt;m_pNext = pHead1; else if (pHead2 != NULL) pTemp-&gt;m_pNext = pHead2; return pHeadMerged;&#125;//判断单链表是否有环bool HasCircle(ListNode *pHead)&#123; ListNode *pFast = pHead; ListNode *pSlow = pHead; while (pFast != NULL&amp;&amp;pFast-&gt;m_pNext != NULL) &#123; pFast = pFast-&gt;m_pNext-&gt;m_pNext; pSlow = pSlow-&gt;m_pNext; if (pFast == pSlow) return true; &#125; return false;&#125;//判断两个链表是否相交bool IsIntersected(ListNode *pHead1, ListNode *pHead2)&#123; if (pHead1 == NULL&amp;&amp;pHead2 == NULL) return false; while (pHead1-&gt;m_pNext != NULL) pHead1 = pHead1-&gt;m_pNext; while (pHead2-&gt;m_pNext != NULL) pHead2 = pHead2-&gt;m_pNext; return pHead1 == pHead2;&#125;//求两个单链表相交的第一个节点ListNode * GetFirstCommonNode(ListNode *pHead1, ListNode *pHead2)&#123; if (pHead1 == NULL || pHead2 == NULL) return NULL; int len1 = 1; ListNode *pTail1 = pHead1; while (pTail1-&gt;m_pNext != NULL) &#123; len1++; pTail1 = pTail1-&gt;m_pNext; &#125; int len2 = 1; ListNode *pTail2 = pHead2; while (pTail1-&gt;m_pNext != NULL) &#123; len2++; pTail2 = pTail2-&gt;m_pNext; &#125; if (pTail1 != pTail2) return NULL; ListNode * pNode1 = pHead1; ListNode * pNode2 = pHead2; if (len1 &gt; len2) &#123; int k = len1 - len2; while (k--) pNode1 = pNode1-&gt;m_pNext; &#125; else &#123; int k = len1 - len2; while (k--) pNode1 = pNode1-&gt;m_pNext; &#125; while (pNode1!=pNode2) &#123; pNode1 = pNode1-&gt;m_pNext; pNode2 = pNode2-&gt;m_pNext; &#125; return pNode1;&#125;//已知有环，求入环后第一个节点ListNode * GetFirstNodeInCycle(ListNode *pHead)&#123; if (pHead == NULL || pHead-&gt;m_pNext == NULL) return NULL; ListNode *pFast = pHead; ListNode *pSlow = pHead; while (pFast != NULL&amp;&amp;pFast-&gt;m_pNext != NULL) &#123; pFast = pFast-&gt;m_pNext-&gt;m_pNext; pSlow = pSlow-&gt;m_pNext; if (pFast == pSlow) break; &#125; if (pFast == NULL || pFast-&gt;m_pNext == NULL) return NULL; ListNode *pAssumedTail = pFast; ListNode *pHead1 = pHead; ListNode *pHead2 = pAssumedTail-&gt;m_pNext; ListNode *pNode1 = pHead1; int len1 = 1; while (pNode1 != pAssumedTail) &#123; len1++; pNode1 = pNode1-&gt;m_pNext; &#125; ListNode *pNode2 = pHead2; int len2 = 1; while (pNode2 != pAssumedTail) &#123; len2++; pNode1 = pNode2-&gt;m_pNext; &#125; if (len1 &gt; len2) &#123; int k = len1 - len2; while (k--) pNode1 = pNode1-&gt;m_pNext; &#125; else &#123; int k = len2 - len1; while (k--) pNode2 = pNode2-&gt;m_pNext; &#125; while (pNode1 != pNode2) &#123; pNode1 = pNode1-&gt;m_pNext; pNode2 = pNode2-&gt;m_pNext; &#125; return pNode1;&#125;//O(1)删除一个节点void DeleteNode(ListNode *pHead, ListNode *pToBeDeleted)&#123; if (pToBeDeleted == NULL) return; if (pToBeDeleted-&gt;m_pNext != NULL) &#123; ListNode *pNext = pToBeDeleted-&gt;m_pNext; pToBeDeleted-&gt;m_nKey = pNext-&gt;m_nKey; pToBeDeleted-&gt;m_pNext = pNext-&gt;m_pNext; delete pNext; pNext = NULL; &#125; else &#123; if (pHead == pToBeDeleted) &#123; delete pToBeDeleted; pHead = NULL; pToBeDeleted = NULL; &#125; else &#123; ListNode *pTemp = pHead; while (pTemp-&gt;m_pNext != pToBeDeleted) pTemp = pTemp-&gt;m_pNext; delete pToBeDeleted; pTemp-&gt;m_pNext = NULL; pToBeDeleted = NULL; &#125; &#125;&#125;]]></content>
      <categories>
        <category>笔试面试</category>
      </categories>
      <tags>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java类与成员访问控制]]></title>
    <url>%2F2017%2F10%2F27%2Fjava%E7%B1%BB%E4%B8%8E%E6%88%90%E5%91%98%E8%AE%BF%E9%97%AE%E6%8E%A7%E5%88%B6%2F</url>
    <content type="text"><![CDATA[java访问控制是基础中的基础，有public/private/protected/default四个类型，一般分为类的访问控制和成员访问控制两个类型。 修饰类修饰类只能使用public和default，不可以声明为protected或private。用public修饰的类任何情况下都可以访问。用default即不加任何修饰词，权限为包访问权限，在同一个包内的类可以访问。 在修饰类的时候有以下几点需要注意 每个编译单元（文件）只能有一个public类，如果有一个以上的public类，编译器会报错 实际上类可以既不是public也可以不失default，这涉及到内部类，此处不介绍。 修饰成员 权限修饰符 同类 同包 不同包的子类 不同包的非子类 public Y Y Y Y protected Y Y Y N default Y Y N N private Y N N N 有趣的类比public：全世界共享default：只属于中国这个国家，权限收缩protected：属于中国这个国家，当然不在中国的国人也有权使用private：只属于单一的中国人]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo 双线部署及seo优化]]></title>
    <url>%2F2017%2F10%2F27%2Fhexo-%E5%8F%8C%E7%BA%BF%E9%83%A8%E7%BD%B2%E5%8F%8Aseo%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[前言断断续续大概半个月，终于把个人博客搭建好，整个过程虽然简单，但是也有很多”坑“。在查询资料的过程中，阅读的博文质量参差不齐，因此想细致的写出我搭建的过程，供大家参考。文章重点在于双线部署及seo，最开始的部署和搭建将简要略过。 安装官方文档永远是最清楚正确的 https://hexo.io/zh-cn/docs/index.html 主题选择hexo的很大原因是因为其主题和插件很多，而且配置方便，这里选择的主题是大名鼎鼎的next，强烈推荐使用。next的插件配置非常便捷，而且设计符合大众审美。 配置过程详见官方文档 http://theme-next.iissnan.com/ 双线部署由于大部分人使用hexo都是将其部署在github上，省去了服务器的钱。由于“国情”原因，github的访问速度较慢，所以才有了双线部署得必要性。将国内访问流量导向coding（国内类似github的网站），国外流量导向github，从而提高访问质量。 域名申请在部署之前，我们必须申请一个域名，便宜的域名1元/年，我的域名是在阿里云买的。进入阿里云官网，点击图片中的域名注册，根据自己的需求选择想要的域名。我的域名为yinjianwen.site，在部署得时候我将博客部署在该域名的二级域名blog下，因此以后访问地址为blog.yinjianwen.site。当然你也可以直接部署在主站。 coding进入官网coding，注册账号并登录。配置好SSH公钥，这里默认你会使用git，如果不会，可以查询相关资料。 新建一个项目，名字格式为yourname.coding.me，其中yourname即为你的用户名。 选择该项目，进入代码-&gt;pages服务，绑定你之前注册的域名。 github与上述过程类似，登陆之后，点击页面右上角的加号，选择New repository，在Repository name下填写yourname.github.io，其中yourname即为你的用户名。 域名解析配置进入阿里云官网，在控制台选择域名服务，进入解析页面。然后即可进行配置。 配置的意思是，blog子域名下的世界流量（国外）访问yinxiaojian.github.io，而国内流量访问yinxiaojian.coding.me。 hexo配置最后我们需要在本地的hexo项目中进行相关配置，打开主项目下的_config.yml文件，在其中任意位置添加如下代码 12345deploy: type: git repo: github: https://github.com/yinxiaojian/yinxiaojian.github.io.git,master coding: https://coding.net/yinxiaojian/yinxiaojian.coding.me.git,master 将其中的yinxiaojian修改为你的coding和github用户名，这行代码的意思是在你通过本地deploy```指令时，会将本地hexo同步到这两个网站。1234之后需要在项目-&gt;source中添加文件CNAME，注意文件名大写且没有后缀。文件中写入你申请的域名，比如我的文件中写入的是```blog.yinjianwen.site 实际上由于github不想coding一样可以自己绑定域名，因此我们需要自己上传CNAME文件。 部署在上述操作完成后，输入以上命令，将本地项目推送到github和coding，双线部署完成。 123hexo cleanhexo generatehexo deploy]]></content>
      <categories>
        <category>技术分享</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
</search>

<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"blog.yinjianwen.site","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="全文翻译自 Notes on Convolutional Neural Networks  论文链接 释部分为自己的补充说明，可能存在错误">
<meta property="og:type" content="article">
<meta property="og:title" content="CNN反向传播算法">
<meta property="og:url" content="http://blog.yinjianwen.site/2018/03/21/CNN%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95/index.html">
<meta property="og:site_name" content="CodingxWriting">
<meta property="og:description" content="全文翻译自 Notes on Convolutional Neural Networks  论文链接 释部分为自己的补充说明，可能存在错误">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://ws3.sinaimg.cn/large/005PPQ5Ily1g0ymwrjv8jg30m50iqaob.gif">
<meta property="article:published_time" content="2018-03-21T03:19:00.000Z">
<meta property="article:modified_time" content="2019-03-11T02:14:35.810Z">
<meta property="article:author" content="尹健文">
<meta property="article:tag" content="deep learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://ws3.sinaimg.cn/large/005PPQ5Ily1g0ymwrjv8jg30m50iqaob.gif">

<link rel="canonical" href="http://blog.yinjianwen.site/2018/03/21/CNN%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>CNN反向传播算法 | CodingxWriting</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">CodingxWriting</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Across the Great Wall we can reach every corner in the world.</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://blog.yinjianwen.site/2018/03/21/CNN%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Author">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="CodingxWriting">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          CNN反向传播算法
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2018-03-21 11:19:00" itemprop="dateCreated datePublished" datetime="2018-03-21T11:19:00+08:00">2018-03-21</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2019-03-11 10:14:35" itemprop="dateModified" datetime="2019-03-11T10:14:35+08:00">2019-03-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/artificial-intelligence/" itemprop="url" rel="index"><span itemprop="name">artificial intelligence</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <blockquote>
<p>全文翻译自 Notes on Convolutional Neural Networks </p>
<p><a href="http://web.mit.edu/jvb/www/papers/cnn_tutorial.pdf" target="_blank" rel="noopener">论文链接</a></p>
<p>释部分为自己的补充说明，可能存在错误</p>
</blockquote>
<a id="more"></a>
<h3 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1 介绍"></a>1 介绍</h3><p>本文档讨论CNN的推导和实现，并加上一些简单的扩展。相比权值，卷积神经网络涉及到更多的连接。其架构本身实现了正则化。此外，CNN在某种程度上实现了平移不变性。这种特殊的神经网络假设我们希望通过数据驱动的方式学习过滤器，用来提取输入特征。推导是在二维的数据和卷积的基础上展开的，但可以很容易的推广到任意维度。</p>
<p>我们先对经典的BP算法做一个描述，然后推导2D卷积神经网络中卷积层和子采样层（池化层）的BP权值更新方法。在整个过程中，我们强调了视线的效率，给出MATLAB代码片段配合方程讲解。然后讨论如何自动组合前一层的feature map，最后是feature map的稀疏组合。</p>
<p>免责声明：这个笔记可能有错误。</p>
<blockquote>
<p>feature map 就是特征矩阵，一张图片的每个像素点的RGB值可分别组成矩阵（R矩阵，G矩阵，B矩阵），这些就是feature map</p>
</blockquote>
<h3 id="2-全连接的反向传播算法"><a href="#2-全连接的反向传播算法" class="headerlink" title="2 全连接的反向传播算法"></a>2 全连接的反向传播算法</h3><p>一个典型的卷积神经网络，开始是卷积层和子采样层交替，最后几层（最接近输出）是全连接的一维网络（全连接层）。当你准备将2D feature map作为一维全连接网络的输入时，直接将所有特征连接为一个一维向量，然后就可以利用BP算法。因此在讨论CNN之前，这里先对BP算法进行解释。</p>
<h4 id="2-1-前向传播-Feedforward-Pass"><a href="#2-1-前向传播-Feedforward-Pass" class="headerlink" title="2.1 前向传播 Feedforward Pass"></a>2.1 前向传播 Feedforward Pass</h4><p>在接下来的推导中，我们考虑平方和误差函数，对于一个有c个类别和N个训练样本的多分类问题，计算公式如下：</p>
<script type="math/tex; mode=display">
E^N=\frac{1}{2}\sum_{n=1}^{N}\sum_{k=1}^{c}(t_k^n-y_k^n)^2</script><p>$t_k^n$是第n个样本对应标签的第k个维度的值，$y_k^n$表示第n个样本对应的网络输出的第k个值。对于多分类问题，输出一般为“one-of-c”形式。如果样本$x^n$属于第k类，那么$t_k^n$为正，$t^n$的其他维度即$t_{k\neq1}^n$为0或负数。这取决于输出激活函数的选择，此处不做讨论。</p>
<p>因为整个训练集的误差为每个训练样本的误差和，所以我们先讨论一个样本。第n个样本的误差如下：</p>
<script type="math/tex; mode=display">
E^N=\frac{1}{2}\sum_{n=1}^{N}\sum_{k=1}^c(t_k^n-y_k^n)^2=\frac{1}{2}||t^n-y^n||_2^2\tag{1}</script><p>在全连接层，我们可以根据反向传播规则计算E对网络权值的偏导数。用$l$代表当前层，输出层被指定为$L$层，输入层为第1层，那么当前层的输出为</p>
<script type="math/tex; mode=display">
x^l=f(u^l), with \space u^l=W^lx^{l-1}+b^l\tag{2}</script><p>输出激活函数$f(\cdot)$一般选择logistic(sigmoid)函数$f(x)=(1+e^{-\beta x})^{-1}$或者hyperbolic tangent函数$f(x)=a\space tanh(bx)$。logistic函数将$[-\infty,\infty]$映射到$[0,1]$，hyperbolic tangent函数将$[-\infty,\infty]$映射到$[-a,a]$，因此hyperbolic tangent函数的输出平均值一般趋于0，sigmoid函数的输出平均为非零。但是如果将训练数据归一化到均值为0方差为1，可以在梯度下降时增加收敛。对于归一化的数据集来说，hyperbolic tangent函数是较好的选择。LeCun推荐$a=1.7159,b=2/3$，所以最大非线性点在$f(\pm1)=\pm1$，因此对预期训练目标进行正则化将会避免训练饱和。</p>
<h4 id="2-2-反向传播-Backpropagation-Pass"><a href="#2-2-反向传播-Backpropagation-Pass" class="headerlink" title="2.2 反向传播 Backpropagation Pass"></a>2.2 反向传播 Backpropagation Pass</h4><p>通过网络反向传播的误差可以看作每个神经元关于偏差扰动的敏感度（sensitivities），通过链式法则可得：</p>
<blockquote>
<p>敏感度就是误差对目标的偏导数</p>
</blockquote>
<script type="math/tex; mode=display">
\begin{equation}
\frac{\partial{E}}{\partial{b}}=\frac{\partial{E}}{\partial{u}}\frac{\partial{u}}{\partial{b}}=\delta
\end{equation}\tag{3}</script><p>因为$\frac{\partial{u}}{\partial{b}}=1$，所以bias的敏感度和误差对一个神经元的输入的偏导数是相等的。利用这个关系将高层误差反向传播到底层，使用下面的递推关系式：</p>
<script type="math/tex; mode=display">
\delta^t=(W^{l+1})^T\delta^{l+1}\circ f'(u^t)\tag{4}</script><p>其中$\circ$表示对应位置元素相乘，从公式（1）可以看到，输出层神经元的敏感度会有一些不同：</p>
<script type="math/tex; mode=display">
\delta^L=f'(u^L)\circ(y^n-t^n)</script><p>最后，对每个神经元运用delta规则进行更行，即对神经元的输入使用delta进行缩放。用向量的形式表述就是：输入向量（前一层的输出）和灵敏度向量之间的一个外积：</p>
<script type="math/tex; mode=display">
\frac{\partial{E}}{\partial{W^l}}=x^{l-1}(\delta^l)^T\tag{5}</script><script type="math/tex; mode=display">
\bigtriangleup W^l=-\eta\frac{\partial{E}}{\partial{W^l}}\tag{6}</script><p>更新偏置值（bias）也是相同的原理，在实际中通常为每个权值赋予一个特定的学习率$\eta_{ij}$。</p>
<h3 id="3-卷积神经网络-Convolutional-Neural-Networks"><a href="#3-卷积神经网络-Convolutional-Neural-Networks" class="headerlink" title="3 卷积神经网络 Convolutional Neural Networks"></a>3 卷积神经网络 Convolutional Neural Networks</h3><blockquote>
<p>公式符号释意</p>
<p>在本文中卷积核有多个，每个卷积核和feature map有多个维度，例如一张图片有RGB值，那么就有三个维度</p>
<p>$i$——卷积核或者feature的第几维度<br>$j$——第几个卷积核<br>$up()$——上采样函数<br>$down()$——下采样函数<br>$l$——当前层</p>
</blockquote>
<p>通常卷积层和子采样层交替以减少计算时间并逐步建立空间和结构不变性。为了同时保持特异性，需要小的子采样因子。这并不是什么新鲜的概念，但简洁有效。这种模型在哺乳动物的视觉皮层非常常见，在过去十年，听觉神经科学发现，这些相同的设计范例可以在多种不同动物皮层的主要和带状听觉区域中找到。 分级分析和学习架构可能仍然是听觉领域的关键。</p>
<h4 id="3-1-卷积层-Convolution-Layers"><a href="#3-1-卷积层-Convolution-Layers" class="headerlink" title="3.1 卷积层 Convolution Layers"></a>3.1 卷积层 Convolution Layers</h4><p>我们现在推导卷积层的BP更行。在一个卷积层，上一层的feature map与一个卷积核进行卷积运算，然后通过一个激活函数获得输出feature map。每个输出map可能是多个卷积结果的结合，公式表述：</p>
<script type="math/tex; mode=display">
x_j^l=f\left(\sum_{i\in M_j}x_i^{l-1}*k_{ij}^{l}+b_j^l\right)</script><p>$M_j$表示输入maps的集合，在MATLAB中卷积的运算需采用”valid”模式。通常情况下，输入maps选择一对或者三个。后面我们会讨论如何自动选择需要组合的特征map。每一个输出map都会加上一个偏执项$b$，但是对于一个特定的输出map，输入maps会和相同的卷积核进行卷积运算，也就是说，如果输出map $j$ 和 $k$ 都是由map $i$通过卷积运算的结果，那么对应的卷积核是不同的。</p>
<blockquote>
<p>一张图片有RGB三个颜色通道，则对应的卷积核也是三维的，输出的feature map就是三个卷积结果的结合</p>
<p><img src="https://ws3.sinaimg.cn/large/005PPQ5Ily1g0ymwrjv8jg30m50iqaob.gif" alt="CNN_02"></p>
</blockquote>
<p>3.1.1 Computing the Gradients</p>
<p>我们假设每个卷积层$l$后面都接一个子采样层 $l+1$。在使用BP算法计算 $l$ 层每个节点的敏感度时，我们需要先对下一层（$l+1$层）连接当前层（$l$层）的节点的敏感度求和，再乘以这些连接对应的权值（连接$l$层和$l+1$层的权值），最后乘以当前层$l$的该神经元节点的输入$u$的激活函数$f$的导数值。因为下采样的存在，采样层的一个像素（神经元）对应的灵敏度$\delta$对应于卷积层（上一层）的输出map的一块像素（采样窗口大小）。因此，$l$层中的map的每个节点只与$l+1$层中相应map的一个节点连接。为了有效计算$l$层的敏感度，我们可以对下采样层的敏感，度map进行“上采样(upsample)”操作，这样可以使下采样层的敏感度map的大小和卷积层的map大小相同，然后将$l$层的激活值偏导map和$l+1$层的上采样敏感度map对应元素相乘。因为下采样层map的权值均是一个常数$\beta$（见3.2节），所以我们只需要将上一步得到的结果乘以 $ \beta$ 就可以得到$l$层的敏感度map $\delta^l$。我们可以在卷积层的每个feature map进行相同的计算过程：</p>
<script type="math/tex; mode=display">
\delta_j^l=\beta_j^{l+1}\left(f'(u_j^l)\circ up(\delta_j^{l+1})\right)</script><p>其中的 $up(\cdot)$表示上采样操作，如果下采样过程的采样因子为$n$，只需将每个像素点在水平方向和垂直方向复制$n$次就可以实现，一种有效的实现方式是利用 Kronecker product:</p>
<script type="math/tex; mode=display">
up(x)=x\otimes1_{n\times n}</script><blockquote>
<p>Ref:<a href="https://zh.wikipedia.org/zh/克罗内克积" target="_blank" rel="noopener">克罗内克积</a></p>
</blockquote>
<p>现在对于一个给定的map，我们已经可以计算其敏感度map，然后我们可以简单的对敏感度map的所有节点求和计算出其bias（偏置项）梯度:</p>
<script type="math/tex; mode=display">
\frac{\partial{E}}{\partial{b_j}}=\sum_{u,v}(\delta_j^l)_{uv}</script><p>最后，使用BP算法计算权重（卷积核）的梯度，很多权重在连接中是共享的，因此我们需要对所有与该权值有联系（权值共享的连接）的连接对该点求梯度，然后对这些梯度进行求和，就像计算bias梯度一样：</p>
<script type="math/tex; mode=display">
\frac{\partial{E}}{\partial{k_{ij}^l}}=\sum_{u,v}(\delta_j^l)_{uv}(p_i^{l-1})_{uv}\tag{7}</script><p>其中$(p_i^{l-1})_{uv}$是$x_i^{l-1}$在卷积运算时和$k_{ij}^l$按元素相乘得到输出map的$(u,v)$位置值的patch。看上去好像我们我刻意追踪输入map中哪些patch与输出map中的哪些像素（及其对应的敏感map）相对应，但是公式(7)可以通过卷积运算获得:</p>
<script type="math/tex; mode=display">
\frac{\partial{E}}{\partial{k_{ij}^l}}=rot180(conv2(x_i^{l-1},rot180(\delta_j^l),'valid'))</script><p>这里我们对$\delta$敏感度map做旋转操作是为了这样就可以进行互相关计算而不是卷积，之后将输出旋转回来，以便当我们在前馈阶段执行卷积时，卷积核将具有正确的方向。</p>
<h4 id="3-2-子采样层-Sub-sampling-Layers"><a href="#3-2-子采样层-Sub-sampling-Layers" class="headerlink" title="3.2 子采样层 Sub-sampling Layers"></a>3.2 子采样层 Sub-sampling Layers</h4><p>子采样层对输入maps进行downsample操作（池化），输入有N个maps，输出也会有N个maps，只是每个maps变小了，公式表述如下：</p>
<script type="math/tex; mode=display">
x_j^l=f\left(\beta_j^ldown(x_j^{l-1})+b_j^l\right)</script><p>其中的$down(\cdot)$是下采样函数。典型的操作一般是对输入图像的不同$n\times n$的块的像素进行求和。这样输出图像在两个维度上都缩小了n倍。每个输出map都有一个属于自己的乘性偏置β和一个加性偏置b。</p>
<p>3.2.1 Computing the Gradients</p>
<p>这里的困难在于如何计算敏感度maps。只要我们能得到敏感度maps，需要更新的只有bias参数 $\beta$ 和 $b$ 。如果子采样层的下一层是全连接层，那么子采样层的敏感度maps只需要利用BP算法就可以容易获得。所以我们假设子采样层的上一层和下一层都是卷积层。</p>
<p>当我们在3.1.1节计算卷积核梯度时，我们需要找到输入map中patch和输出map的像素的对应关系。这里就是必须找到当前层的敏感度map中那个patch与下一层的敏感度矩阵的的给定像素对应，这样就可以像公式 (4)那样应用$\delta$递推。 当然，将输入patch和输出像素之间连接相乘的权重恰好是（旋转的）卷积核的权重。利用卷积可以实现该计算：</p>
<script type="math/tex; mode=display">
\delta_j^l=f'(u_j^l)\circ conv2(\delta_j^{l+1},rot180(k_j^{l+1}),full)</script><p>如前所述，我们旋转内核是为了是卷积函数实施互相关计算。同时，我们需要对卷积边界进行“full”处理，Matlab的卷积函数会自动执行这个过程，对缺少的输入像素进行补零操作。</p>
<p>现在我们可以很容易的计算 $b$ 和 $\beta$ 的梯度，加性基 $b$ 的处理和之前一样，将敏感度map中所有元素相加即可 ：</p>
<script type="math/tex; mode=display">
\frac{\partial E}{\partial b_j}=\sum_{u,v}(\delta_j^l)_{uv}</script><p>对于乘性偏置$\beta$ ，参与了前向传播时下采样map的运算，所以在前向计算时将这些maps保存，这样避免了在反向传播时重复计算。定义：</p>
<script type="math/tex; mode=display">
d_j^l=down(x_j^{l-1})</script><p>$\beta$ 梯度计算公式如下：</p>
<script type="math/tex; mode=display">
\frac{\partial E}{\partial \beta_j}=\sum_{u,v}(\delta_j^l)</script><blockquote>
<p>补充说明：对rot180的解释</p>
<p>假设第$l-1$层输出$x^l$是一个3x3矩阵，第$l$层卷积核$k^l$是一个2x2的矩阵，步幅为1，输出一个2x2的矩阵$z^l$，则有（简化$b^l=0$，单卷积核)</p>
<script type="math/tex; mode=display">
x^{l-1}*k^{l}=z^{l}</script><p>矩阵形式表达：</p>
<script type="math/tex; mode=display">
\left( \begin{array}{}
x_{11}&x_{12}&x_{13}\\
x_{21}&x_{22}&x_{23}\\
x_{31}&x_{32}&x_{33}
\end{array}
\right)
*
\left( \begin{array}{}
k_{11}&k_{12}\\
k_{21}&k_{22}
\end{array}
\right)
=
\left( \begin{array}{}
z_{11}&z_{12}\\
z_{21}&z_{22}
\end{array}
\right)</script><p>根据卷积的定义可得：</p>
<script type="math/tex; mode=display">
z_{11}=x_{11}k_{11}+x_{12}k_{12}+x_{21}k_{21}+x_{22}k_{22}\\
z_{12}=x_{12}k_{11}+x_{13}k_{12}+x_{22}k_{21}+x_{23}k_{22}\\
z_{21}=x_{21}k_{11}+x_{22}k_{12}+x_{31}k_{21}+x_{32}k_{22}\\
z_{22}=x_{22}k_{11}+x_{23}k_{12}+x_{32}k_{21}+x_{33}k_{22}</script><p>第$l$层的梯度误差为$\delta^l$，反向传导：</p>
<script type="math/tex; mode=display">
\frac{\partial{E}}{\partial{k^l}}=\frac{\partial{E}}{\partial{z^l}}\frac{\partial{z^l}}{\partial{k^l}}=\delta^l\frac{z^l}{k^l}</script><p>因此对于卷积核$k^l$的梯度为第$l$层的敏感度（梯度）乘上$\frac{\partial{z^l}}{\partial{k^l}}$，分别计算可得</p>
<script type="math/tex; mode=display">
\bigtriangledown{k_{11}}=\delta_{11}x_{11}+\delta_{12}x_{22}+\delta_{21}x_{21}+\delta_{22}x_{22}\\
\bigtriangledown{k_{12}}=\delta_{11}x_{12}+\delta_{12}x_{13}+\delta_{21}x_{22}+\delta_{22}x_{23}\\
\bigtriangledown{k_{21}}=\delta_{11}x_{21}+\delta_{12}x_{22}+\delta_{21}x_{31}+\delta_{22}x_{32}\\
\bigtriangledown{k_{22}}=\delta_{11}x_{22}+\delta_{12}x_{23}+\delta_{21}x_{32}+\delta_{22}x_{33}</script><p>上面4个式子用矩阵卷积形式表示：</p>
<script type="math/tex; mode=display">
\left( \begin{array}{}
x_{11}&x_{12}&x_{13}\\
x_{21}&x_{22}&x_{23}\\
x_{31}&x_{32}&x_{33}
\end{array}
\right)
*
\left( \begin{array}{}
\delta_{11}&\delta_{12}\\
\delta_{21}&\delta_{22}
\end{array}
\right)
=
\left( \begin{array}{}
\bigtriangledown k_{11}&\bigtriangledown k_{12}\\
\bigtriangledown k_{21}&\bigtriangledown k_{22}
\end{array}
\right)</script><p>公式化表达为：</p>
<script type="math/tex; mode=display">
\frac{\partial{E}}{\partial{k^l}}=x^{l-1}*\delta^l</script><p>这里需要注意，在论文中进行了两次旋转，这是因为在MATLAB中conv2函数在计算卷积时除了会对矩阵进行“0”扩展，还会将卷积核进行旋转，然后再计算。例如：</p>
<figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">a =</span><br><span class="line">     <span class="number">1</span>     <span class="number">1</span>     <span class="number">1</span></span><br><span class="line">     <span class="number">1</span>     <span class="number">1</span>     <span class="number">1</span></span><br><span class="line">     <span class="number">1</span>     <span class="number">1</span>     <span class="number">1</span></span><br><span class="line">k =</span><br><span class="line">     <span class="number">1</span>     <span class="number">2</span>     <span class="number">3</span></span><br><span class="line">     <span class="number">4</span>     <span class="number">5</span>     <span class="number">6</span></span><br><span class="line">     <span class="number">7</span>     <span class="number">8</span>     <span class="number">9</span></span><br><span class="line"></span><br><span class="line">&gt;&gt; convn(a,k,<span class="string">'full'</span>)</span><br><span class="line"><span class="built_in">ans</span> =</span><br><span class="line"></span><br><span class="line">     <span class="number">1</span>     <span class="number">3</span>     <span class="number">6</span>     <span class="number">5</span>     <span class="number">3</span></span><br><span class="line">     <span class="number">5</span>    <span class="number">12</span>    <span class="number">21</span>    <span class="number">16</span>     <span class="number">9</span></span><br><span class="line">    <span class="number">12</span>    <span class="number">27</span>    <span class="number">45</span>    <span class="number">33</span>    <span class="number">18</span></span><br><span class="line">    <span class="number">11</span>    <span class="number">24</span>    <span class="number">39</span>    <span class="number">28</span>    <span class="number">15</span></span><br><span class="line">     <span class="number">7</span>    <span class="number">15</span>    <span class="number">24</span>    <span class="number">17</span>     <span class="number">9</span></span><br></pre></td></tr></table></figure>
<p>因此在编写代码时需要注意保持一致（要么旋转，要么不旋转）。</p>
<p>在3.2节计算 $\delta^l_j$ 也是同理</p>
<p>利用反向传播：</p>
<script type="math/tex; mode=display">
\frac{\partial{E}}{\partial{x^{l-1}}}=\frac{\partial{E}}{\partial{z^l}}\frac{\partial{z^l}}{\partial{x^{l-1}}}=\delta^l\frac{z^l}{x^{l-1}}</script><p>利用上式，可得：</p>
<script type="math/tex; mode=display">
\bigtriangledown{x_{11}}=\delta_{11}k_{11}\\
\bigtriangledown{x_{12}}=\delta_{11}k_{12}+\delta_{12}k_{11}\\
\bigtriangledown{x_{13}}=\delta_{12}k_{12}\\
\bigtriangledown{x_{21}}=\delta_{11}k_{21}+\delta_{21}k_{11}\\
\bigtriangledown{x_{22}}=\delta_{11}k_{22}+\delta_{12}k_{21}+\delta_{21}k_{12}+\delta_{22}k_{11}\\
\bigtriangledown{x_{23}}=\delta_{12}k_{22}+\delta_{22}k_{12}\\
\bigtriangledown{x_{31}}=\delta_{21}k_{21}\\
\bigtriangledown{x_{32}}=\delta_{21}k_{22}+\delta_{22}k_{21}\\
\bigtriangledown{x_{33}}=\delta_{22}k_{22}</script><p>上面九个式子用矩阵卷积形式表示：</p>
<script type="math/tex; mode=display">
\left( \begin{array}{}
0&0&0&0\\
0&\delta_{11}&\delta_{12}&0\\
0&\delta_{21}&\delta_{22}&0\\
0&0&0&0
\end{array}
\right)
*
\left( \begin{array}{}
k_{22}&k_{21}\\
k_{12}&k_{11}
\end{array}
\right)
=
\left( \begin{array}{}
\bigtriangledown x_{11}&\bigtriangledown x_{12}&\bigtriangledown x_{13}\\
\bigtriangledown x_{21}&\bigtriangledown x_{22}&\bigtriangledown x_{23}\\
\bigtriangledown x_{31}&\bigtriangledown x_{32}&\bigtriangledown x_{33}
\end{array}
\right)</script><p>公式化表达为：</p>
<script type="math/tex; mode=display">
\frac{\partial{E}}{\partial{x^{l-1}}}=\delta^l * rot180(k^l)</script></blockquote>
<h4 id="3-3-学习特征maps组合-Learning-Combinations-of-Feature-Maps"><a href="#3-3-学习特征maps组合-Learning-Combinations-of-Feature-Maps" class="headerlink" title="3.3 学习特征maps组合 Learning Combinations of Feature Maps"></a>3.3 学习特征maps组合 Learning Combinations of Feature Maps</h4><p>通常，对不同的maps进行卷积并对结果求和获得一个输出map，往往能取得不错的效果。在一些文献中，通过人工选择输入maps进行组合。但是我们可以尝试通过训练获得这个组合。让 $a_{ij}$ 表示得到第 $j$ 个输出map中第 $i$ 个输入map的权重，那么第 $j$ 个输出map的定义如下：</p>
<script type="math/tex; mode=display">
x_j^l=f\left( \sum_{i=1}^{N_{in}}a_{ij}(x_i^{l-1}*k_i^l)+b_j^l\right)</script><p>同时需满足以下约束：</p>
<script type="math/tex; mode=display">
\sum_ia_ij=1,and \space 0\leq a_{ij}\leq1</script><p>这些约束可以通过将变量$a_{ij}$表示为softmax形式来加强：</p>
<script type="math/tex; mode=display">
a_{ij}=\frac{exp(c_{ij})}{\sum_kexp(c_{kj})}</script><p>因为对于固定的 $j$ 来说，每组权值 $c_{ij}$ 都和其他组权值相独立，所以为了方便描述，我们把下标 $j$ 去掉，只考虑单个map的更新，其他map的更新方式是相同的过程，只是索引 $j$ 不同。</p>
<p>softmax函数的导数：</p>
<script type="math/tex; mode=display">
\frac{\partial a_k}{\partial c_i}=\delta_{ki}a_i-a_ia_k\tag{8}</script><p>这里的 $\delta$ 是 kronecker delta，参照公式 (1) 我们可以得到在 $l$ 层误差对 $a_i$ 的偏导：</p>
<script type="math/tex; mode=display">
\frac{\partial E}{\partial a_i}=\frac{\partial E}{\partial u^l}\frac{\partial u^l}{\partial a_i}=\sum_{u,v}(\delta^l \circ (x_i^{l-1}*k_i^l))_{uv}</script><p>这里的$\delta^l$对应具有输入 $u$ 的输出map的敏感度map。和前面一样，这里的卷积运算也是“valid”类型，目的是使结果和sensitivity map大小匹配。最后使用链式法则计算损失函数对权值 $c_i$ 的偏导数：</p>
<script type="math/tex; mode=display">
\begin{align}
\frac{\partial E}{\partial c_i}&=\sum_k\frac{\partial E}{\partial a_k}\frac{\partial a_k}{\partial c_i}\tag{9}\\
&=a_i\left( \frac{\partial E}{\partial a_i}-\sum_k\frac{\partial E}{\partial a_k}a_k\right)\tag{10}
\end{align}</script><p> 3.3.1 Enforcing Sparse Combinations</p>
<p>为了给 $a_i$ 增加稀疏约束（限制一个输出map只与某些而不是全部输入map相连接），我们在代价函数中添加正则项惩罚 $\Omega(a)$ 。这样就可以使某些权值趋于0，最后只有部分输入maps参与输出map相连接，代价函数为：</p>
<script type="math/tex; mode=display">
\widetilde{E}^n=E^n + \lambda\sum{i,j}|(a){ij}|\tag{11}</script><p>然后求这个正则项对 $c_i$梯度的影响：</p>
<script type="math/tex; mode=display">
\frac{\partial\Omega}{\partial a_i}=\lambda sign(a_i)\tag{12}</script><p>结合公式 (8) 的结果：</p>
<script type="math/tex; mode=display">
\begin{align}
\frac{\partial\Omega}{\partial c_i}& = \sum_k\frac{\partial\Omega}{\partial a_k}\frac{\partial a_k}{\partial c_i} \\
&=
\lambda\left(|a_i|-a_i\sum_k|a_k|\right)
\end{align}</script><p>最后结合公式 (13) 和公式 (9) ，可以求的权重 $c_i$ 的梯度：</p>
<script type="math/tex; mode=display">
\frac{\partial\widetilde{E}^n}{\partial c_i} = \frac{\partial E^n}{\partial c_i} +\frac{\partial \Omega}{\partial c_i}</script><h4 id="3-4-加快MATLAB训练速度-Making-it-Fast-with-MATLAB"><a href="#3-4-加快MATLAB训练速度-Making-it-Fast-with-MATLAB" class="headerlink" title="3.4 加快MATLAB训练速度 Making it Fast with MATLAB"></a>3.4 加快MATLAB训练速度 Making it Fast with MATLAB</h4><blockquote>
<p>与CNN关系不大，不做翻译，可看原文</p>
</blockquote>
<h3 id="4-实际训练问题-Practical-Training-Issues-Incomplete"><a href="#4-实际训练问题-Practical-Training-Issues-Incomplete" class="headerlink" title="4 实际训练问题 Practical Training Issues (Incomplete)"></a>4 实际训练问题 Practical Training Issues (Incomplete)</h3><blockquote>
<p>与CNN关系不大，不做翻译，可看原文</p>
</blockquote>
<script type="math/tex; mode=display">
</script>
    </div>

    
    
    

      <footer class="post-footer">
          
          <div class="post-tags">
              <a href="/tags/deep-learning/" rel="tag"><i class="fa fa-tag"></i> deep learning</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2018/03/19/ZELDA-%E7%94%B7%E4%BA%BA%E7%9A%84%E6%B5%AA%E6%BC%AB/" rel="prev" title="ZELDA-男人的浪漫">
      <i class="fa fa-chevron-left"></i> ZELDA-男人的浪漫
    </a></div>
      <div class="post-nav-item">
    <a href="/2018/04/11/Softmax-and-Cross-Entropy-Loss/" rel="next" title="Softmax and Cross Entropy Loss">
      Softmax and Cross Entropy Loss <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="gitalk-container"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-介绍"><span class="nav-number">1.</span> <span class="nav-text">1 介绍</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-全连接的反向传播算法"><span class="nav-number">2.</span> <span class="nav-text">2 全连接的反向传播算法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-前向传播-Feedforward-Pass"><span class="nav-number">2.1.</span> <span class="nav-text">2.1 前向传播 Feedforward Pass</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-反向传播-Backpropagation-Pass"><span class="nav-number">2.2.</span> <span class="nav-text">2.2 反向传播 Backpropagation Pass</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-卷积神经网络-Convolutional-Neural-Networks"><span class="nav-number">3.</span> <span class="nav-text">3 卷积神经网络 Convolutional Neural Networks</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-卷积层-Convolution-Layers"><span class="nav-number">3.1.</span> <span class="nav-text">3.1 卷积层 Convolution Layers</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-子采样层-Sub-sampling-Layers"><span class="nav-number">3.2.</span> <span class="nav-text">3.2 子采样层 Sub-sampling Layers</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-学习特征maps组合-Learning-Combinations-of-Feature-Maps"><span class="nav-number">3.3.</span> <span class="nav-text">3.3 学习特征maps组合 Learning Combinations of Feature Maps</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-4-加快MATLAB训练速度-Making-it-Fast-with-MATLAB"><span class="nav-number">3.4.</span> <span class="nav-text">3.4 加快MATLAB训练速度 Making it Fast with MATLAB</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-实际训练问题-Practical-Training-Issues-Incomplete"><span class="nav-number">4.</span> <span class="nav-text">4 实际训练问题 Practical Training Issues (Incomplete)</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Author"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">Author</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">21</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/yinxiaojian" title="GitHub → https://github.com/yinxiaojian" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="/jianwen_yin@163.com" title="E-Mail → jianwen_yin@163.com"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Author</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css">

<script>
NexT.utils.loadComments(document.querySelector('#gitalk-container'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID    : '4d2be4ddd111613247dd',
      clientSecret: 'e35d6fe6539051d9f3972abbc33705957b10716a',
      repo        : 'blog-comment',
      owner       : 'yinxiaojian',
      admin       : ['yinxiaojian'],
      id          : '3caac611deeec6c77ea052d742fe7603',
        language: '',
      distractionFreeMode: true
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
});
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</body>
</html>

<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Convexity, Lipschitzness, and Smoothness</title>
    <url>/2018/06/28/Convexity-Lipschitzness-and-Smoothness/</url>
    <content><![CDATA[<p>introduction from "understandingmachine learning theory algorithms"</p>
<!--- more --->
<p>##Convexity</p>
<h3 id="convexity-set">Convexity set</h3>
<p>A set C in a vector space is convex if for any two vector <span class="math inline">\(\mathbf{u},\mathbf{v}\)</span> in <span class="math inline">\(C\)</span>, the line segment between <span class="math inline">\(\mathbf{u}\)</span> and <span class="math inline">\(\mathbf{v}\)</span> is contained in <span class="math inline">\(C\)</span>. That is, for any <span class="math inline">\(a\in[0,1]\)</span> we have that <span class="math inline">\(\alpha \mathbf{u}+(1-\alpha)\mathbf{v} \in C\)</span>.</p>
<h3 id="convexity-function">Convexity function</h3>
<p>Let <span class="math inline">\(C\)</span> be a convex set. A function <span class="math inline">\(f:C \rightarrow \mathbb{R}\)</span> is convex for every <span class="math inline">\(\mathbf{u},\mathbf{v}\in C\)</span> and <span class="math inline">\(\alpha\in[0,1]\)</span>, <span class="math display">\[
f(\alpha \mathbf{u} + (1-\alpha)\mathbf{v}) \le \alpha f(\mathbf{u}) + (1-\alpha) f(\mathbf{v}).
\]</span> <strong>Important property</strong></p>
<ul>
<li>Every local minimum of the function is also a global minimum.</li>
<li>For every <span class="math inline">\(\mathbf{w}\)</span> we can construct a tangent to <span class="math inline">\(f\)</span> at <span class="math inline">\(\mathbf{w}\)</span> that lies below <span class="math inline">\(f\)</span> everywhere, that is for convex differentiable function,</li>
</ul>
<p><span class="math display">\[
\forall \mathbf{u}, f(\mathbf{u}) \ge f(\mathbf{w}) + \langle \nabla f(\mathbf{w}), \mathbf{u} - \mathbf{w} \rangle.
\]</span></p>
<p><strong>LEMMA</strong></p>
<p>Let <span class="math inline">\(f:\mathbb{R} \rightarrow \mathbb{R}\)</span> be a scalar twice diffrential function, and Let <span class="math inline">\(f&#39;,f&#39;&#39;\)</span> be its first and second derivatives, respectively. Then, the following are equivalent:</p>
<ol type="1">
<li><span class="math inline">\(f\)</span> is convex</li>
<li><span class="math inline">\(f&#39;\)</span> is monotonically nondecreasing</li>
<li><span class="math inline">\(f&#39;&#39;\)</span> is nonnegative</li>
</ol>
<p><strong>CLAIM</strong></p>
<ul>
<li><p>Assume that <span class="math inline">\(f:\mathbb{R}^d \rightarrow \mathbb{R}\)</span>can be written as <span class="math inline">\(f(\mathbf{w}) = g(\langle \mathbf{w}, \mathbf{x}) + y)\)</span>, for some <span class="math inline">\(\mathbf{x} \in \mathbb{R}^d, y\in \mathbb{R}\)</span>. Then, <strong>convexity of <span class="math inline">\(g\)</span> implies the convexity of <span class="math inline">\(f\)</span></strong>.</p>
<p><strong>Meaning</strong>: The composition of a convex scalar function with a linear function yields a convex vector-valued function.</p></li>
<li><p>For <span class="math inline">\(i=1,…,r,let f_i : \mathbb{R}^d \rightarrow \mathbb{R}\)</span> be a convex function. The following functions <span class="math inline">\(g(x)\)</span>from <span class="math inline">\(\mathbb{R}\)</span> to <span class="math inline">\(\mathbb{R}\)</span> are also convex.</p>
<p><span class="math inline">\(g(x)=max_{i\in[r]} f_i(x)\)</span></p>
<p><span class="math inline">\(g(x)=\sum_{i=1}^r w_i f_i(x)\)</span>, where for all <span class="math inline">\(i,w_i \ge 0\)</span>.</p>
<p><strong>Meaning</strong>:The maximum of convex functions is convex and that a weighted sum of convex functions, with nonnegative weights, is also convex.</p></li>
</ul>
<p>##Lipschitzness</p>
<p><strong>DEFINITION</strong>: Let <span class="math inline">\(C \subset \mathbb{R}^d\)</span>. A function <span class="math inline">\(f:\mathbb{R}^d\rightarrow\mathbb{R}^k\)</span> is <span class="math inline">\(\rho\)</span>-Lipschitz over <span class="math inline">\(C\)</span> if for every <span class="math inline">\(\mathbf{w}_1,\mathbf{w}_2 \in C\)</span> we have <span class="math inline">\(\|f(\mathbb{w_1})-f(\mathbb{w_2}) \| \le \rho\|\mathbf{w_1}-\mathbf{w_2}\|\)</span>.</p>
<p>MEANING: if <span class="math inline">\(f\)</span> is everywhere bounded (in absolute value) by <span class="math inline">\(\rho\)</span>, then the function is <span class="math inline">\(\rho\)</span>-Lipschitz.</p>
<p><strong>CLAIM</strong>: Let <span class="math inline">\(f(x) = g_1(g_2(\mathbf{x}))\)</span>, where <span class="math inline">\(g_1\)</span> is <span class="math inline">\(\rho_1\)</span>-Lipschitz and <span class="math inline">\(g_2\)</span> is <span class="math inline">\(\rho_2\)</span>-Lipschitz. Then, <span class="math inline">\(f \space is \space(\rho_1\rho_2)-Lipschitz\)</span>. In particular, if <span class="math inline">\(g_2\)</span> is the linear function, <span class="math inline">\(g_2(\mathbf{x})=\langle \mathbf{v},\mathbf{x}\rangle+b\)</span>, for some <span class="math inline">\(\mathbf{v} \in \mathbb{R}^d, b\in \mathbb{R},then \space f \space (\rho_1\|\mathbf{v}\|)-Lipschitz\)</span>.</p>
<h2 id="smoothness">Smoothness</h2>
<p><strong>DEFINITION</strong>: A differentiable function <span class="math inline">\(f\)</span>: <span class="math inline">\(\mathbb{R}^d \rightarrow \mathbb{R}\)</span> is <span class="math inline">\(\beta\)</span>-smooth if its gradient is <span class="math inline">\(\beta\)</span>-Lipschitz; namely, for all <span class="math inline">\(\mathbf{v},\mathbf{w}\)</span> we have <span class="math inline">\(\|\nabla f(\mathbf{v}) - \nabla f(\mathbf{w}) \| \le \beta\|\mathbf{v} - \mathbf{w} \|\)</span>.</p>
<p><strong>CLAIM</strong>: Let <span class="math inline">\(f(\mathbf{w}) = g(\langle \mathbf{w} , \mathbf{x} \rangle + b), where \space g:\mathbb{R} \rightarrow \mathbb{R}​\)</span> is a <span class="math inline">\(\beta​\)</span>-smooth function, <span class="math inline">\(\mathbf{x} \in \mathbb{R}^d​\)</span>, and <span class="math inline">\(b \in \mathbb{R}​\)</span>. Then, <span class="math inline">\(f \space is \space (\beta \|\mathbf{x}\|)​\)</span>-smooth.</p>
<p>实际上，凸性要求一个函数在局部增长得比线性函数要快，而强凸性则要求其增长得比一个二次函数（二次项系数为m/2）快。</p>
]]></content>
      <categories>
        <category>artificial intelligence</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Deep Interest Network</title>
    <url>/2020/05/11/Deep-Interest-Network/</url>
    <content><![CDATA[<blockquote>
<p>Deep Interest Network for Click-Through Rate Prediction</p>
<p>Deep Interest Evolution Network for Click-Through Rate Prediction</p>
<p>Deep Session Interest Network for Click-Through Rate Prediction</p>
</blockquote>
<!--- more --->
<p>最近阅读了阿里的经典论文：DIN，并对后续的DIEN和DSIN进行了了解，此处做一个简单的记录。</p>
<h4 id="din">DIN</h4>
<p>对于DIN来说，核心在于如何更好的从用户的历史行为中提取合适的兴趣，简单的sum/average pooling是不符合实际场景的。用户在浏览的过程中，可能会对多种类型的商品感兴趣，其兴趣也是多种多样。比如说女生的浏览历史中存在服饰和电子产品，那么在预测裤子这件商品的点击时，历史行为中的服饰和电子产品的重要程度显然是不同的。DIN尝试通过注意力机制为历史行为中的商品赋予不同的权重，从而生成最符合当前预测商品的兴趣。</p>
<p><img src="https://raw.githubusercontent.com/yinxiaojian/blog-img/master/20200516230137.png" /></p>
<h4 id="dien">DIEN</h4>
<p>DIEN考虑到了两个问题：第一个问题是DIN直接使用行为去表示最终的兴趣，这是很困难的。我们需要去挖掘行为背后潜藏的兴趣。第二个问题在于用户某一方面的兴趣是睡着时间变化而变化的，因此我们需要考虑兴趣的动态变化，从而更好的进行预测。</p>
<blockquote>
<p>个人理解：DIEN提出的这两个问题都是很符合实际场景的，首先用户的行为是一个序列，有前后之分。直接使用赋予行为权重并求和，很难体现这一序列信息。论文里将其包装为hidden interest。其次，兴趣本身也是变化的，这里我们可以讲行为作为原始信息，兴趣作为一阶信息，那么兴趣的变化本质上就是二阶信息。</p>
<p>因此论文也采用了GRU去捕捉一阶信息，在此基础上在使用一个GRU去捕捉二阶信息。</p>
</blockquote>
<p>针对问题一，DIEN采用GRU去捕捉行为背后隐藏的兴趣，在此基础上，使用DIN的attention机制获取最符合目标的兴趣，然后使用AUGRU去捕捉兴趣随着时间瓷都的变化。</p>
<p><img src="https://raw.githubusercontent.com/yinxiaojian/blog-img/master/20200516231053.png" /></p>
<h4 id="dsin">DSIN</h4>
<p>DSIN从实际场景出发，提出用户的行为序列往往是由一个个的session组成的，每个session内部较为相似，同时不同session之间可能存在关联，并且在时间尺度上存在先后关系。</p>
<p>因此DSIN讲用户的行为序列在时间尺度上切成一个个session（按照相邻行为时间差切割），然后使用Self-Attention去模拟单个session。在获取单个session的表示后，使用Bi-LSTM去模拟不同session之间的关系。同时其采用了DIN中的attention机制去为不同session赋予不同的权重。</p>
<blockquote>
<p>实际上，DSIN是讲单个session当作一个兴趣或者一个类别去模拟，然后再模拟session与session之间的关系，而attention机制则只是用来求和的。总的来说，DSIN从session尺度考虑问题，并非是很新鲜的想法，而且既然每个session之间差别较大，那么再使用LSTM去模拟session之间的时序关系是否可行？个人认为其不如DIN和DIEN合理。</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/yinxiaojian/blog-img/master/20200516234211.png" /></p>
]]></content>
      <categories>
        <category>paper notes</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title>STUN和TURN服务器搭建</title>
    <url>/2019/03/02/STUN%E5%92%8CTURN%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%90%AD%E5%BB%BA/</url>
    <content><![CDATA[<p>使用CoTurn自建STUN和TURN服务。</p>
<!--- more --->
<h3 id="下载-coturn">下载 <a href="https://github.com/coturn/coturn/blob/master/INSTALL">CoTurn</a></h3>
<p>下载指定版本的CoTurn并解压</p>
<pre><code class="hljs angelscript">wegt https:<span class="hljs-comment">//coturn.net/turnserver/v4.5.0.8/turnserver-4.5.0.8.tar.gz</span>
tar -zxvf turnserver<span class="hljs-number">-4.5</span><span class="hljs-number">.0</span><span class="hljs-number">.8</span>.tar.gz</code></pre>
<h3 id="安装">安装</h3>
<p>安装编译所需依赖</p>
<pre><code class="hljs q">apt-<span class="hljs-built_in">get</span> <span class="hljs-keyword">update</span>
apt-<span class="hljs-built_in">get</span> install -y libssl-<span class="hljs-built_in">dev</span> libevent-<span class="hljs-built_in">dev</span> libpq-<span class="hljs-built_in">dev</span> mysql-client libmysqlclient-<span class="hljs-built_in">dev</span> libhiredis-<span class="hljs-built_in">dev</span> openssl
apt-<span class="hljs-built_in">get</span> install gdebi-core -y
apt-<span class="hljs-built_in">get</span> install sqlite libsqlite3-<span class="hljs-built_in">dev</span> -y</code></pre>
<p>编译安装CoTurn</p>
<pre><code class="hljs angelscript">cd turnserver<span class="hljs-number">-4.5</span><span class="hljs-number">.0</span><span class="hljs-number">.8</span>
./configure
make
make install</code></pre>
<p>查看是否安装成功</p>
<pre><code class="hljs bash"><span class="hljs-built_in">which</span> turnserver</code></pre>
<h3 id="配置">配置</h3>
<p>添加用户</p>
<pre><code class="hljs stylus">turnadmin -<span class="hljs-selector-tag">a</span> –u 用户名 -r 域名 -<span class="hljs-selector-tag">p</span> 密码</code></pre>
<p>例如：添加用户sccs，密码为123456，域名fongda.com（域名可以随意填写）</p>
<pre><code class="hljs stylus">turnadmin -<span class="hljs-selector-tag">a</span> -u sccs -r fongda<span class="hljs-selector-class">.com</span> -<span class="hljs-selector-tag">p</span> <span class="hljs-number">123456</span></code></pre>
<p>输入<code>turnadmin -l</code>检查是否添加用户成功</p>
<h3 id="启动">启动</h3>
<p>前台启动</p>
<pre><code class="hljs armasm"><span class="hljs-symbol">turnserver</span> -L 公网<span class="hljs-built_in">IP</span> -a -f -r 域名</code></pre>
<p>或者在后台运行</p>
<pre><code class="hljs powershell"><span class="hljs-variable">$</span> turnserver <span class="hljs-literal">-L</span> 公网IP <span class="hljs-literal">-o</span> <span class="hljs-literal">-a</span> <span class="hljs-operator">-f</span> <span class="hljs-literal">-r</span> 域名</code></pre>
<h3 id="验证">验证</h3>
<p>使用webrtc官方提供的网址进行验证：</p>
<p>https://webrtc.github.io/samples/src/content/peerconnection/trickle-ice/</p>
<p>输入要验证的STUN和TURN服务器</p>
<figure>
<img src="https://wx3.sinaimg.cn/large/005PPQ5Ily1g0ovehi2xbj31gs0iidhl.jpg" alt="image" /><figcaption aria-hidden="true">image</figcaption>
</figure>
<p>出现Done时表示服务正常运行</p>
<figure>
<img src="https://wx2.sinaimg.cn/large/005PPQ5Ily1g0ovfxi3yzj31ie09maay.jpg" alt="image" /><figcaption aria-hidden="true">image</figcaption>
</figure>
]]></content>
      <categories>
        <category>技术分析</category>
      </categories>
      <tags>
        <tag>WebRTC</tag>
      </tags>
  </entry>
  <entry>
    <title>Softmax and Cross Entropy Loss</title>
    <url>/2018/04/11/Softmax-and-Cross-Entropy-Loss/</url>
    <content><![CDATA[<p>introduction and derivative of Softmax and Cross Entropy Loss.</p>
<a id="more"></a>
<h3 id="softmax-function">Softmax function</h3>
<p>the softmax function takes N-dimensional vector of real numbers and transforms it into a vector of real number in range of (0,1) which means the probability of each classification. <span class="math display">\[
p_i=\frac{e^{a_i}}{\sum_{k=1}^Ne^{a_k}}
\]</span> it is easy to know <span class="math inline">\(\sum_{k=1}^Np_i=1\)</span> .</p>
<p>the code of softmax function in python:</p>
<pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">softmax</span>(<span class="hljs-params">X</span>):</span>
    exps = np.exp(X)
    <span class="hljs-keyword">return</span> exps / np.<span class="hljs-built_in">sum</span>(exps)</code></pre>
<p>sometimes the exp operation can overshot the limit of the floating point numbers in python, so we need to normalize the value by multiply a constant C. <span class="math display">\[
p_i=\frac{e^{a_i}}{\sum_{k=1}^Ne^{a_k}}
=\frac{Ce^{a_i}}{C\sum_{k=1}^Ne^{a_k}}
=\frac{e^{a_i+log(C)}}{\sum_{k=1}^Ne^{a_k+log(C)}}
\]</span> Generally, we max <span class="math inline">\(log(C)=-max(a)\)</span>, so we need make a little change of our code:</p>
<pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">softmax</span>(<span class="hljs-params">X</span>):</span>
    exps = np.exp(X-np.<span class="hljs-built_in">max</span>(X))
    <span class="hljs-keyword">return</span> exps / np.<span class="hljs-built_in">sum</span>(exps)</code></pre>
<p>###Cross Entropy Loss</p>
<p>the defination of cross entropy loss is: <span class="math display">\[
L=-\sum_iy_i log(p_i)
\]</span> note:</p>
<p><span class="math inline">\(p_i\)</span> means the softmax value of i-th neuron in fully connected layer;</p>
<p>if the true label of input sample is <span class="math inline">\(j\)</span>, then <span class="math inline">\(y_{i=j}=1;y_{i\neq j}=0\)</span>.</p>
<p>the code of loss function in python:</p>
<pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">cross_entropy</span>(<span class="hljs-params">X, y</span>):</span>
    <span class="hljs-string">&quot;&quot;&quot;</span>
<span class="hljs-string">    X is the output of fully connected layer(N-dimensional vector)</span>
<span class="hljs-string">    y is true label(integer)</span>
<span class="hljs-string">    &quot;&quot;&quot;</span>
    p = softmax(X)
    loss = -np.log(p[y])
    <span class="hljs-keyword">return</span> loss</code></pre>
<h3 id="backward-propagation">Backward propagation</h3>
<p>we need to compute the derivative of cross entropy loss: <span class="math display">\[
\frac{\partial L}{\partial a_i}=\frac{\partial L}{\partial p_j}\frac{\partial p_j}{\partial a_i}
\]</span> it is easy to compute <span class="math inline">\(\frac{\partial L}{\partial p_i}\)</span> <span class="math display">\[
\frac{\partial L}{\partial p_i}=-\sum_iy_i\frac{1}{p_i}\tag1
\]</span> then compute <span class="math inline">\(\frac{\partial p_j}{\partial a_i}\)</span>, we need to divide it into two situations：</p>
<ol type="1">
<li>if <span class="math inline">\(i=j\)</span></li>
</ol>
<p><span class="math display">\[
\begin{align*}
\frac{\partial p_j}{\partial a_i}&amp;=\frac{\partial p_i}{\partial a_i}=\frac{\partial{\frac{e^{a_i}}{\sum_{k=1}^Ne^{a_k}}}}{\partial a_i}=\frac{\sum_{k=1}^{N}e^{a_i}e^{a_k}-(e^{a_i})^2}{(\sum_{k=1}^Ne^{a_k})^2}\\
&amp;=\frac{e^{a_i}}{\sum_{k=1}^Ne^{a_k}}\left(1-\frac{e^{a_i}}{\sum_{k=1}^Ne^{a_k}}\right)\\
&amp;=p_i(1-p_i)
\end{align*}
\]</span></p>
<ol start="2" type="1">
<li>If <span class="math inline">\(i\neq j\)</span></li>
</ol>
<p><span class="math display">\[
\frac{\partial p_j}{\partial a_i}=\frac{\partial{\frac{e^{a_j}}{\sum_{k=1}^Ne^{a_k}}}}{\partial a_i}=
-\frac{e^{a_j}}{\sum_{k=1}^Ne^{a_k}}\frac{e^{a_i}}{\sum_{k=1}^Ne^{a_k}}=-p_jp_i\tag2
\]</span></p>
<p>combine the result of (1) and (2) <span class="math display">\[
\begin{align*}
\frac{\partial L}{\partial a_i}&amp; = \left(-\sum_iy_i\frac{1}{p_i}\right)\frac{\partial p_i}{\partial a_i}\\
&amp;=-\frac{y_i}{p_i}p_i(1-p_i)+\sum_{i\neq j}\frac{y_i}{p_i}p_ip_j\\
&amp;=-y_i+y_ip_i+\sum_{i\neq j}y_ip_i\\
&amp;=-y_i+p_i\sum_iy_i\\
&amp;=p_i-y_i
\end{align*}
\]</span> code in python</p>
<pre><code class="hljs python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">delta_entropy</span>(<span class="hljs-params">X,y</span>):</span>
    <span class="hljs-string">&quot;&quot;&quot;</span>
<span class="hljs-string">    X is the output of fully connected layer(N-dimensional vector)</span>
<span class="hljs-string">    y is true labels(integer)</span>
<span class="hljs-string">    &quot;&quot;&quot;</span>
    grad = softmax(X)
    grad[y] -= <span class="hljs-number">1</span>
    <span class="hljs-keyword">return</span> grad</code></pre>
]]></content>
      <categories>
        <category>artificial intelligence</category>
      </categories>
      <tags>
        <tag>softmax</tag>
      </tags>
  </entry>
  <entry>
    <title>WebRTC与NAT穿透</title>
    <url>/2019/03/02/WebRTC%E4%B8%8ENAT%E7%A9%BF%E9%80%8F/</url>
    <content><![CDATA[<p>WebRTC支持点对点通信（P2P），但是其建立通讯仍需要两类服务器。</p>
<ul>
<li>信令服务器：用于客户端交换元信息以协调通讯。</li>
<li>STUN/TURN服务器：应对网络地址转换器（NAT）和防火墙。</li>
</ul>
<!--- more --->
<blockquote>
<p>以下客户端指进行通话的用户</p>
</blockquote>
<h3 id="信令服务器">信令服务器</h3>
<p>信令就是协调通讯的过程，为了建立一个webRTC的通讯过程，客户端需要交换如下信息：</p>
<ul>
<li>用于打开或关闭通信的会话控制消息。</li>
<li>错误消息。</li>
<li>媒体元数据，例如编解码器和编解码器设置，带宽和媒体类型。</li>
<li>密钥数据，用于建立安全连接。</li>
<li>网络数据，例如客户端的外网IP地址和端口。</li>
</ul>
<h3 id="stunturn服务器">STUN/TURN服务器</h3>
<p>对于建立连接所需的元数据信息，WebRTC通过信令服务器进行转发。但是对于实际的媒体和数据流，一旦建立会话，首先尝试直接连接客户端，即客户端点对点通信。也就是说，每个客户端都有一个唯一的IP地址，他能用来和其他客户端进行直接通讯。</p>
<figure>
<img src="https://www.html5rocks.com/en/tutorials/webrtc/infrastructure/p2p.png" alt="没有NAT和防火墙的理想情况" /><figcaption aria-hidden="true">没有NAT和防火墙的理想情况</figcaption>
</figure>
<center>
<i>没有NAT和防火墙的理想情况</i>
</center>
<p>但是现实中，大多数设备都位于一层或多层NAT之后，有些设备具有阻止某些端口和协议的防病毒软件，而且许多设备都在代理和企业防火墙之后。 防火墙和NAT也可以由相同的设备实现，例如家庭wifi路由器。</p>
<p><img src="https://www.html5rocks.com/en/tutorials/webrtc/infrastructure/nat.png" /></p>
<center>
<i>现实世界</i>
</center>
<p>WebRTC可以使用ICE框架来克服现实网络的复杂性。 在连接过程中，ICE将试图找到最佳的连接路径，通过尝试所有可能的选项,然后选择最合适的方案。ICE首先尝试使用从设备的操作系统和网卡获得的主机地址建立连接；如果失败（设备位于NAT后），ICE将使用STUN服务器获取设备外网地址；如果失败，则使用TURN服务器作为中继转发流量（媒体和数据流）。</p>
<p>也就是说：</p>
<ul>
<li>STUN服务器用于获取设备的外网地址。</li>
<li>TURN服务器用于转发流量，在P2P连接失败的情况下发挥作用。</li>
</ul>
<p>STUN和或TURN服务器的URL（可选）由iceServers配置对象中的WebRTC应用程序指定，该对象是RTCPeerConnection构造函数的第一个参数：</p>
<pre><code class="hljs javascript">&#123;
  <span class="hljs-string">&#x27;iceServers&#x27;</span>: [
    &#123;
      <span class="hljs-string">&#x27;urls&#x27;</span>: <span class="hljs-string">&#x27;stun:stun.l.google.com:19302&#x27;</span>
    &#125;,
    &#123;
      <span class="hljs-string">&#x27;urls&#x27;</span>: <span class="hljs-string">&#x27;turn:192.158.29.39:3478?transport=udp&#x27;</span>,
      <span class="hljs-string">&#x27;credential&#x27;</span>: <span class="hljs-string">&#x27;JZEOEt2V3Qb0y27GRntt2u2PAYA=&#x27;</span>,
      <span class="hljs-string">&#x27;username&#x27;</span>: <span class="hljs-string">&#x27;28224511:1379330808&#x27;</span>
    &#125;,
    &#123;
      <span class="hljs-string">&#x27;urls&#x27;</span>: <span class="hljs-string">&#x27;turn:192.158.29.39:3478?transport=tcp&#x27;</span>,
      <span class="hljs-string">&#x27;credential&#x27;</span>: <span class="hljs-string">&#x27;JZEOEt2V3Qb0y27GRntt2u2PAYA=&#x27;</span>,
      <span class="hljs-string">&#x27;username&#x27;</span>: <span class="hljs-string">&#x27;28224511:1379330808&#x27;</span>
    &#125;
  ]
&#125;</code></pre>
<h4 id="stun服务器">STUN服务器</h4>
<p>NAT为设备提供IP地址，以便在内网中使用，但此地址不能在外部使用。如果没有公共地址，WebRTC对等方就无法进行通信。为了解决这个问题，WebRTC使用STUN服务器。</p>
<p>STUN服务器位于<strong>公网</strong>上，其工作非常简单：检查传入请求的IP:port，并将该地址作为响应发回。也就是说，应用程序使用STUN服务器从外网是脚下发现其IP:Port。此过程使WebRTC客户端获取自己的公网地址，然后通过信令机制将其传递给另一个苦护短，以便建立P2P连接。</p>
<p><strong>STUN服务器的工作非常简单，因此可以处理大量请求</strong>，这也是有很多免费的STUN服务器提供的原因。</p>
<blockquote>
<p>根据webrtcstats.com，WebRTC使用STUN成功建立连接的概率为：86％</p>
</blockquote>
<p><img src="https://www.html5rocks.com/en/tutorials/webrtc/infrastructure/stun.png" /></p>
<center>
<i>使用STUN服务器获取公网地址</i>
</center>
<h4 id="turn服务器">TURN服务器</h4>
<p>在P2P连接失败后，WebRTC会利用TURN服务器作为中继进行通信。需要强调的是：<strong>TURN在不同客户端之间中转的是音频/视频/数据流，而不是信令数据。</strong></p>
<p>TURN服务器具有公网地址，因此即使客户端位于防火墙或代理之后，也可以与其进行联系。 TURN服务器需要中转大量的视频音频等数据，因此相对STUN服务器，它们需要消耗大量带宽。</p>
<p><img src="https://www.html5rocks.com/en/tutorials/webrtc/infrastructure/turn.png" /></p>
<center>
<i>完整的框架：STUN、TURN、SIGNALING</i>
</center>
<p>在上图中，若使用STUN无法建立连接，将会使用TURN进行中继。</p>
<h3 id="附录nat分类与stunturn">附录：NAT分类与STUN/TURN</h3>
<h4 id="基础概念">基础概念</h4>
<p>一个IP连接由两个IP组成的连接“四元组”决定：</p>
<ul>
<li>(Source IP address, source port number)</li>
<li>(Destination IP address, destination port number)</li>
</ul>
<p>即（源地址，源端口）和（目标地址，目标端口），可视化如下：</p>
<pre><code class="hljs lisp">(<span class="hljs-name">SRC_ADDR</span>, SRC_PORT) -&gt; (<span class="hljs-name">DST_ADDR</span>, DST_PORT)</code></pre>
<blockquote>
<p>NOTE</p>
<ul>
<li><code>-&gt;</code> 表示连接的方向</li>
<li><code>(ADDR, PORT)</code> 表示一个（IP地址，IP端口）元组</li>
<li><code>(SRC_ADDR, SRC_PORT)</code> 表示发起连接的机器的IP</li>
<li><code>(DST_ADDR, DST_PORT)</code> 表示接收连接的机器的IP</li>
</ul>
</blockquote>
<p>NAT(Network Address Translation) 是一种广泛应用的解决IP 短缺的有效方法， NAT 将内网地址转和端口号换成合法的公网地址和端口号，建立一个会话，与公网主机进行通信。可视化如下：</p>
<p>NAT分为两大类：锥型和对称型，而锥型又分为三种类型，分类如下：</p>
<ul>
<li>锥形 cone NAT
<ul>
<li>全锥形 Full Cone NAT</li>
<li>地址受限锥形 Address-Restricted Cone NAT</li>
<li>端口受限锥形 Port-Restricted Cone NAT</li>
</ul></li>
<li>对称形 Symmetric NAT</li>
</ul>
<h4 id="全锥形-full-cone-nat">全锥形 Full Cone NAT</h4>
<p>全锥NAT 把所有来自相同内部IP地址和端口的请求映射到相同的外部IP 地址和端口。任何一个外部主机均可通过该映射发送数据包到该内部主机，可视化如下：</p>
<pre><code class="hljs coq">    &#123;NAT internal side&#125;  |    <span class="hljs-type">&#123;NAT</span> external side&#125;  |  <span class="hljs-type">&#123;Remote</span> machine&#125;
                         |                         <span class="hljs-type">|</span>
<span class="hljs-type">1</span>. (INT_ADDR, INT_PORT) =&gt; [ (EXT_ADDR, INT_PORT) -&gt; (REM_ADDR, REM_PORT) ]
<span class="hljs-number">2.</span> (INT_ADDR, INT_PORT) &lt;= [ (EXT_ADDR, INT_PORT) &lt;- (   *    ,    *    ) ]</code></pre>
<blockquote>
<p><code>*</code>表示所有值都可以使用，任何一个外部主机都可以通过该映射建立连接。</p>
</blockquote>
<h4 id="地址受限锥形-address-restricted-cone-nat">地址受限锥形 Address-Restricted Cone NAT</h4>
<p>地址受限锥形NAT 把所有来自相同内部IP 地址和端口的请求映射到相同的外部IP 地址和端口。但是, 和全锥NAT 不同的是：只有当内部主机先给外部主机发送数据包, 该外部主机才能向该内部主机发送数据包，可视化如下：</p>
<pre><code class="hljs coq">    &#123;NAT internal side&#125;  |    <span class="hljs-type">&#123;NAT</span> external side&#125;  |  <span class="hljs-type">&#123;Remote</span> machine&#125;
                         |                         <span class="hljs-type">|</span>
<span class="hljs-type">1</span>. (INT_ADDR, INT_PORT) =&gt; [ (EXT_ADDR, INT_PORT) -&gt; (REM_ADDR, REM_PORT) ]
<span class="hljs-number">2.</span> (INT_ADDR, INT_PORT) &lt;= [ (EXT_ADDR, INT_PORT) &lt;- (REM_ADDR,    *    ) ]</code></pre>
<blockquote>
<p>在第二步中，REM_ADDR必须和第一步中相同，但是REM_PORT没有限制。</p>
</blockquote>
<h4 id="端口受限锥形-port-restricted-cone-nat">端口受限锥形 Port-Restricted Cone NAT</h4>
<p>端口受限锥形NAT 与地址受限锥形NAT 类似, 只是多了端口号的限制, 即只有内部主机先向外部地址和端口号元组发送数据包, 该外部主机才能使用特定的端口号向内部主机发送数据包，可视化如下：</p>
<pre><code class="hljs coq">    &#123;NAT internal side&#125;  |    <span class="hljs-type">&#123;NAT</span> external side&#125;  |  <span class="hljs-type">&#123;Remote</span> machine&#125;
                         |                         <span class="hljs-type">|</span>
<span class="hljs-type">1</span>. (INT_ADDR, INT_PORT) =&gt; [ (EXT_ADDR, INT_PORT) -&gt; (REM_ADDR, REM_PORT) ]
<span class="hljs-number">2.</span> (INT_ADDR, INT_PORT) &lt;= [ (EXT_ADDR, INT_PORT) &lt;- (REM_ADDR, REM_PORT) ]</code></pre>
<blockquote>
<p>在第二步中，REM_ADDR和REM_PORT都要和第一步相同。</p>
</blockquote>
<h4 id="对称形-symmetric-nat">对称形 Symmetric NAT</h4>
<p>对称形NAT 与上述3 种锥类都不同，当同一内部主机使用相同的端口与不同地址的外部主机进行通信时, 对称NAT 会重新建立一个Session ，为这个Session 分配新的外部端口号，或许还会改变IP 地址。 这意味着从同一本地端口到两个不同远程计算机的两个连续连接将具有两个不同的外部端口，即使是同一内部主机，可视化如下：</p>
<pre><code class="hljs livescript">    &#123;NAT internal side&#125;  |    &#123;NAT external side&#125;  |  &#123;Remote machine&#125;
                         |                         |
<span class="hljs-number">1.</span> <span class="hljs-function"><span class="hljs-params">(INT_ADDR, INT_PORT)</span> =&gt; [ <span class="hljs-params">(EXT_ADDR, EXT_PORT1)</span> -&gt;</span> (REM_ADDR, REM_PORT1) ]
<span class="hljs-number">2.</span> (INT_ADDR, INT_PORT) &lt;= [ (EXT_ADDR, EXT_PORT1) &lt;- (REM_ADDR, REM_PORT1) ]
...
<span class="hljs-number">3.</span> <span class="hljs-function"><span class="hljs-params">(INT_ADDR, INT_PORT)</span> =&gt; [ <span class="hljs-params">(EXT_ADDR, EXT_PORT2)</span> -&gt;</span> (REM_ADDR, REM_PORT2) ]
<span class="hljs-number">4.</span> (INT_ADDR, INT_PORT) &lt;= [ (EXT_ADDR, EXT_PORT2) &lt;- (REM_ADDR, REM_PORT2) ]</code></pre>
<blockquote>
<p>在第一步中建立连接，分配的外部端口号为EXT_PORT1</p>
<p>在第三步中建立新的连接，分配的外部端口号为EXT_PORT2，<strong>外部端口号发生变化</strong>。</p>
</blockquote>
<h4 id="stunturn与nat的关系">STUN/TURN与NAT的关系</h4>
<p>假设客户端A和客户端B需要建立P2P连接，在锥形NAT中，由于<code>(EXT_ADDR, EXT_PORT)</code>是不会发生变化的，所以我们可以通过STUN获取该客户段A和B的外部IP和端口，然后让两者建立连接。</p>
<p>在对称形NAT中，由于<code>(EXT_ADDR, EXT_PORT)</code>中的端口会发生变化，当客户端A和B与STUN服务器建立连接，获取外部IP和端口后。两者利用该外部IP和端口建立P2P连接，对称NAT会为A和B分配新的外部端口，因此无法建立连接。所以，对于对称形NAT，我们需要通过TURN服务器进行信息转发。</p>
]]></content>
      <categories>
        <category>技术分析</category>
      </categories>
      <tags>
        <tag>WebRTC</tag>
      </tags>
  </entry>
  <entry>
    <title>ZELDA-男人的浪漫</title>
    <url>/2018/03/19/ZELDA-%E7%94%B7%E4%BA%BA%E7%9A%84%E6%B5%AA%E6%BC%AB/</url>
    <content><![CDATA[<p>如果一款游戏，如果能让你回到儿时，你会去玩吗？</p>
<a id="more"></a>
<p>游戏时间 100 小时，四神兽和boss全灭，120神庙解锁一半，支线任务完成1/4。</p>
<p>拿到游戏到现在过去了10天，除了实习时间，我几乎都在玩这款游戏，这段时间我没有看动漫，甚至连手机都懒得碰。直到现在，在天气晴朗的时候，登上海拉尔大陆的顶端远眺，留给我的依旧是无数秘密。</p>
<figure>
<img src="https://ws4.sinaimg.cn/large/005PPQ5Ily1g0ymv18a9mj30l60b4adc.jpg" alt="9c29d14f-ae6e-42b4-9c36-c1f81c9217af" /><figcaption aria-hidden="true">9c29d14f-ae6e-42b4-9c36-c1f81c9217af</figcaption>
</figure>
<p>我无法忘记游戏开始走出出生地的震撼，一个广阔的大陆等着你探索。作为一个普通玩家，或许无法从专业性的角度去阐述这款游戏的优秀，但是我可以慢慢的回想我在游戏中的故事。</p>
<p>开放世界、沙盒玩法可以说是游戏界经常提到的概念，在我以前玩的游戏中，我按照游戏设计者设计好的路线去解锁游戏，在这个过程中，总是会有一种压迫感，游戏设计者在不停的推着我前进。终于，游戏主线支线结束，站在广阔的地图上，我却不知道干什么。</p>
<p>荒野之息却完全不同，游戏剧情“老套”，主线就是打魔王救公主。在游戏的过程中，我却忘记了主线，沉迷于爬山，炸鱼，采矿，做菜等等等等。看到一座很高的山，我费劲力气爬上去，只为看到更好的风景。为发现隐藏在海拉尔大陆的120个神庙而惊喜，进入神庙后又是另一番体验。就连npc都设计的非常巧妙，记得有一个npc，她对我说：“别摸鱼了，快去拯救世界”，令人爆笑。如果这些还不够，还有900个“呀哈哈”等着你去寻找，可能在石头下，也可能在山顶上。</p>
<p>在大概两天后，我终于想起了公主，于是我开始进入主线，四大神兽每个都有不同的故事和解谜方式，不会让人厌烦。打败四大神兽后，我又开始收集13个回忆，慢慢的找到了所有林克和公主的故事，一个傲娇的公主是怎么慢慢成长，怎么喜欢上我的（笑）。接着就是面对最终boss加农了，这时候我却不想推进了，我不想结束这段旅行，还有那么多神庙没有解锁，那么多支线没有做，现在打boss总有种半途而废的感觉。不过游戏总有分别，又过了两天，我带着全村最好的剑，最好的弓，最好的盾，最好的药冲到boss，然后boss就死了，一段回忆后游戏戛然而止······</p>
<p>在这段时间，做梦的时候我总是会梦到游戏剧情，在室外的时候，看到高楼大厦，我也不经意的会思考怎么爬上去，爬上去会看到什么样的风景。现在我大概知道为什么会沉迷这款游戏了，大概是这款游戏满足了我所有的浪漫。</p>
<p>在童年时候，我会为钓到小鱼小虾而激动，会为爬到大坝上而激动。那时候会做梦，觉得自己是个大侠、冒险家。和小伙伴们去野外（隔壁村）探险，会去招惹村口的凶狗，会骑姥姥家养的小花狗。实际上，直到现在我还想做这些事，还对其乐此不疲，只不过没人陪我了。是的，我长大了，虽然还在校园，但烦恼却不少。所以，我感谢这款游戏，让我找到了冒险的感觉，为游戏中每一个新事物，为前方而满怀期待！</p>
<p>最后，塞尔达天下第一！</p>
]]></content>
      <categories>
        <category>杂谈</category>
      </categories>
      <tags>
        <tag>游戏</tag>
      </tags>
  </entry>
  <entry>
    <title>google map 比例尺算法分析</title>
    <url>/2017/12/24/google-map-%E6%AF%94%E4%BE%8B%E5%B0%BA%E7%AE%97%E6%B3%95%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<p>比例尺即地图右下角显示地图距离与实际距离比例的控件，由于Google map自带比例尺控件存在的局限性——无法调整位置和格式，所以通过此文章，介绍比例尺算法及具体实现。</p>
<a id="more"></a>
<h3 id="什么是比例尺">什么是比例尺</h3>
<p>比例尺是表示<a href="http://baike.baidu.com/view/5454725.htm">图上距离</a>比实地距离缩小的程度，因此也叫<a href="http://baike.baidu.com/view/1559050.htm">缩尺</a>。用公式表示为：比例尺=<a href="http://baike.baidu.com/view/5454725.htm">图上距离</a>/实地距离。在Google map上比例尺显示在右下角。</p>
<p>在创建地图时只需增加设置项</p>
<pre><code class="hljs yaml"><span class="hljs-attr">scaleControl:</span> <span class="hljs-literal">true</span></code></pre>
<p>如下例</p>
<iframe scrolling="no" width="100%" height="300" src="https://jsfiddle.net/yinxiaojian/yL9mxdzp/embedded/js,resources,html,css,result/light" frameborder="0" loading="lazy" allowfullscreen></iframe>
<p>遗憾的是这样添加的比例尺会存在于右下角，而且不像其他控件一样可以调整位置。如果我们希望修改其位置或者样式，就会无从下手。</p>
<h3 id="自制比例尺">自制比例尺</h3>
<p>实现一个比例尺的关键在于如何获取到地图距离与实际距离的比例和缩放等级及维度之间的关系。google官方api未提供相关函数，因此我们需要自己计算。核心公式为</p>
<pre><code class="hljs apache"><span class="hljs-attribute">ScaleValue</span> = <span class="hljs-number">156543</span>.<span class="hljs-number">03392</span> * Math.cos(latLng.lat() * Math.PI / <span class="hljs-number">180</span>) / Math.pow(<span class="hljs-number">2</span>, zoom)</code></pre>
<p>其中zoom为当前缩放等级，latLng.lat()即目标点维度值。该公式是在地球半径为6378137m的基础上计算的，这个值即google地图所采用的值。</p>
<p>有了计算公式后，我们还需要一张表——缩放等级和比例尺对应表，也就是在什么样的缩放等级下使用多大的比例尺，表格如下:</p>
<pre><code class="hljs angelscript">Zoom    Scale
<span class="hljs-number">0</span>    <span class="hljs-number">10000</span>km
<span class="hljs-number">1</span>    <span class="hljs-number">5000</span>km
<span class="hljs-number">2</span>    <span class="hljs-number">2000</span>km
<span class="hljs-number">3</span>    <span class="hljs-number">1000</span>km
<span class="hljs-number">4</span>    <span class="hljs-number">500</span>km
<span class="hljs-number">5</span>    <span class="hljs-number">200</span>km
<span class="hljs-number">6</span>    <span class="hljs-number">200</span>km
<span class="hljs-number">7</span>    <span class="hljs-number">100</span>km
<span class="hljs-number">8</span>    <span class="hljs-number">50</span>km
<span class="hljs-number">9</span>    <span class="hljs-number">20</span>km
<span class="hljs-number">10</span>   <span class="hljs-number">10</span>km
<span class="hljs-number">11</span>   <span class="hljs-number">5</span>km
<span class="hljs-number">12</span>   <span class="hljs-number">2</span>km
<span class="hljs-number">13</span>   <span class="hljs-number">1</span>km
<span class="hljs-number">14</span>   <span class="hljs-number">500</span>m
<span class="hljs-number">15</span>   <span class="hljs-number">200</span>m
<span class="hljs-number">16</span>   <span class="hljs-number">200</span>m
<span class="hljs-number">17</span>   <span class="hljs-number">100</span>m
<span class="hljs-number">18</span>   <span class="hljs-number">50</span>m
<span class="hljs-number">19</span>   <span class="hljs-number">20</span>m
<span class="hljs-number">20</span>   <span class="hljs-number">10</span>m
<span class="hljs-number">21</span>   <span class="hljs-number">5</span>m
<span class="hljs-number">22</span>   <span class="hljs-number">2</span>m
<span class="hljs-number">23</span>   <span class="hljs-number">1</span>m
<span class="hljs-number">24</span>   <span class="hljs-number">1</span>m
<span class="hljs-number">25</span>   <span class="hljs-number">1</span>m
<span class="hljs-number">26</span>   <span class="hljs-number">1</span>m</code></pre>
<p>通过监听地图变化事件（缩放和平移），获取当前屏幕中心点缩放等级和维度获取到scale和scalevalue，那么比例尺的长度（px） = scale/scalevalue。</p>
<p>获取当前比例尺长度的核心代码如下：</p>
<pre><code class="hljs javascript"><span class="hljs-comment">/**</span>
<span class="hljs-comment"> * 根据缩放等级和维度获取KM数(m数)和像素</span>
<span class="hljs-comment"> */</span>
<span class="hljs-function"><span class="hljs-keyword">function</span> <span class="hljs-title">setScaleInfos</span>(<span class="hljs-params">zoomLevel, lat, map</span>) </span>&#123;
	<span class="hljs-comment">// 缩放等级-比例尺</span>
	<span class="hljs-keyword">var</span> zoomList = [&#123;
		text: <span class="hljs-string">&quot;10000KM&quot;</span>,
		value: <span class="hljs-number">10000</span> * <span class="hljs-number">1000</span>
	&#125;, &#123;
		text: <span class="hljs-string">&quot;5000KM&quot;</span>,
		value: <span class="hljs-number">5000</span> * <span class="hljs-number">1000</span>
	&#125;, &#123;
		text: <span class="hljs-string">&quot;2000KM&quot;</span>,
		value: <span class="hljs-number">2000</span> * <span class="hljs-number">1000</span>
	&#125;, &#123;
		text: <span class="hljs-string">&quot;1000KM&quot;</span>,
		value: <span class="hljs-number">1000</span> * <span class="hljs-number">1000</span>
	&#125;, &#123;
		text: <span class="hljs-string">&quot;500KM&quot;</span>,
		value: <span class="hljs-number">500</span> * <span class="hljs-number">1000</span>
	&#125;, &#123;
		text: <span class="hljs-string">&quot;200KM&quot;</span>,
		value: <span class="hljs-number">200</span> * <span class="hljs-number">1000</span>
	&#125;, &#123;
		text: <span class="hljs-string">&quot;200KM&quot;</span>,
		value: <span class="hljs-number">200</span> * <span class="hljs-number">1000</span>
	&#125;, &#123;
		text: <span class="hljs-string">&quot;100KM&quot;</span>,
		value: <span class="hljs-number">100</span> * <span class="hljs-number">1000</span>
	&#125;, &#123;
		text: <span class="hljs-string">&quot;50KM&quot;</span>,
		value: <span class="hljs-number">50</span> * <span class="hljs-number">1000</span>
	&#125;, &#123;
		text: <span class="hljs-string">&quot;20KM&quot;</span>,
		value: <span class="hljs-number">20</span> * <span class="hljs-number">1000</span>
	&#125;, &#123;
		text: <span class="hljs-string">&quot;10KM&quot;</span>,
		value: <span class="hljs-number">10</span> * <span class="hljs-number">1000</span>
	&#125;, &#123;
		text: <span class="hljs-string">&quot;5KM&quot;</span>,
		value: <span class="hljs-number">5000</span>
	&#125;, &#123;
		text: <span class="hljs-string">&quot;2KM&quot;</span>,
		value: <span class="hljs-number">2000</span>
	&#125;, &#123;
		text: <span class="hljs-string">&quot;1KM&quot;</span>,
		value: <span class="hljs-number">1000</span>
	&#125;, &#123;
		text: <span class="hljs-string">&quot;500m&quot;</span>,
		value: <span class="hljs-number">500</span>
	&#125;, &#123;
		text: <span class="hljs-string">&quot;200m&quot;</span>,
		value: <span class="hljs-number">200</span>
	&#125;, &#123;
		text: <span class="hljs-string">&quot;200m&quot;</span>,
		value: <span class="hljs-number">200</span>
	&#125;, &#123;
		text: <span class="hljs-string">&quot;100m&quot;</span>,
		value: <span class="hljs-number">100</span>
	&#125;, &#123;
		text: <span class="hljs-string">&quot;50m&quot;</span>,
		value: <span class="hljs-number">50</span>
	&#125;, &#123;
		text: <span class="hljs-string">&quot;20m&quot;</span>,
		value: <span class="hljs-number">20</span>
	&#125;, &#123;
		text: <span class="hljs-string">&quot;10m&quot;</span>,
		value: <span class="hljs-number">10</span>
	&#125;, &#123;
		text: <span class="hljs-string">&quot;5m&quot;</span>,
		value: <span class="hljs-number">5</span>
	&#125;, &#123;
		text: <span class="hljs-string">&quot;2m&quot;</span>,
		value: <span class="hljs-number">2</span>
	&#125;, &#123;
		text: <span class="hljs-string">&quot;1m&quot;</span>,
		value: <span class="hljs-number">1</span>
	&#125;, &#123;
		text: <span class="hljs-string">&quot;1m&quot;</span>,
		value: <span class="hljs-number">1</span>
	&#125;, &#123;
		text: <span class="hljs-string">&quot;1m&quot;</span>,
		value: <span class="hljs-number">1</span>
	&#125;, &#123;
		text: <span class="hljs-string">&quot;1m&quot;</span>,
		value: <span class="hljs-number">1</span>
	&#125;];
	<span class="hljs-comment">// 宽度</span>
	<span class="hljs-keyword">var</span> pxValue = <span class="hljs-built_in">Math</span>.floor(zoomList[zoomLevel].value / (<span class="hljs-number">156543.03392</span> * <span class="hljs-built_in">Math</span>.cos(lat * <span class="hljs-built_in">Math</span>.PI / <span class="hljs-number">180</span>) / <span class="hljs-built_in">Math</span>.pow(<span class="hljs-number">2</span>, zoomLevel)));
	<span class="hljs-comment">// 更新经纬度数据</span>
	$W.id(<span class="hljs-string">&quot;scaleText&quot;</span>).innerHTML = zoomList[zoomLevel].text;
	$W.id(<span class="hljs-string">&quot;scaleSize&quot;</span>).style.width = pxValue + <span class="hljs-string">&quot;px&quot;</span>;
&#125;;</code></pre>
<p>下面是通过上述思路实现的例子，在地图右下角实现一个比例尺。</p>
<iframe scrolling="no" width="100%" height="300" src="https://jsfiddle.net/yinxiaojian/6eutbuyr/embedded/js,resources,html,css,result/light" frameborder="0" loading="lazy" allowfullscreen></iframe>
]]></content>
      <categories>
        <category>前端</category>
      </categories>
      <tags>
        <tag>google map</tag>
      </tags>
  </entry>
  <entry>
    <title>java类与成员访问控制</title>
    <url>/2017/10/27/java%E7%B1%BB%E4%B8%8E%E6%88%90%E5%91%98%E8%AE%BF%E9%97%AE%E6%8E%A7%E5%88%B6/</url>
    <content><![CDATA[<p>java访问控制是基础中的基础，有public/private/protected/default四个类型，一般分为类的访问控制和成员访问控制两个类型。 <a id="more"></a></p>
<h3 id="修饰类">修饰类</h3>
<p>修饰类只能使用public和default，不可以声明为protected或private。用public修饰的类任何情况下都可以访问。用default即不加任何修饰词，权限为包访问权限，在同一个包内的类可以访问。</p>
<p>在修饰类的时候有以下几点需要注意</p>
<ul>
<li>每个编译单元（文件）只能有一个public类，如果有一个以上的public类，编译器会报错</li>
<li>实际上类可以既不是public也可以不失default，这涉及到内部类，此处不介绍。</li>
</ul>
<h3 id="修饰成员">修饰成员</h3>
<table>
<thead>
<tr class="header">
<th>权限修饰符</th>
<th>同类</th>
<th>同包</th>
<th>不同包的子类</th>
<th>不同包的非子类</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>public</td>
<td>Y</td>
<td>Y</td>
<td>Y</td>
<td>Y</td>
</tr>
<tr class="even">
<td>protected</td>
<td>Y</td>
<td>Y</td>
<td>Y</td>
<td>N</td>
</tr>
<tr class="odd">
<td>default</td>
<td>Y</td>
<td>Y</td>
<td>N</td>
<td>N</td>
</tr>
<tr class="even">
<td>private</td>
<td>Y</td>
<td>N</td>
<td>N</td>
<td>N</td>
</tr>
</tbody>
</table>
<h3 id="有趣的类比">有趣的类比</h3>
<p>public：全世界共享 default：只属于中国这个国家，权限收缩 protected：属于中国这个国家，当然不在中国的国人也有权使用 private：只属于单一的中国人</p>
]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java基础</tag>
      </tags>
  </entry>
  <entry>
    <title>meta learning notes</title>
    <url>/2018/10/21/meta-learning-notes/</url>
    <content><![CDATA[<h3 id="definition">Definition</h3>
<p>Meta-learning is equal to learning to learn. In practice, very closely related to multi-task learning.</p>
<p>In the typical machine learning setting, we are interested in a dataset <span class="math inline">\(D\)</span> and usually split <span class="math inline">\(D\)</span> so that we optimize parameters <span class="math inline">\(\theta\)</span> on a training set <span class="math inline">\(D_{train}\)</span> and evaluate its generalization on the test set <span class="math inline">\(D_{test}\)</span>. In meta-learning, however, we are dealing with meta-sets <span class="math inline">\(\mathcal{D}\)</span> containing multiple regular datasets, where each <span class="math inline">\(D \in \mathcal{D}\)</span> has a split of <span class="math inline">\(D_{train}\)</span> and <span class="math inline">\(D_{test}\)</span>.</p>
<!--- more --->
<ul>
<li>Consider a typical learning algorithm
<ul>
<li>Input: training set <span class="math inline">\(D_{train} = \{(X_t,Y_t)\}_{t=1}^{T}\)</span></li>
<li>Output: parameters <span class="math inline">\(\theta\)</span> of model <span class="math inline">\(M\)</span></li>
<li>Objective: good performance on test set <span class="math inline">\(D_{test} = (X,Y)\)</span></li>
</ul></li>
<li>Desire a meta-learning algorithm
<ul>
<li>Input: meta-training set <span class="math inline">\(\mathcal{D}_{meta\_train} = \{(D_{train}^{(n)},D_{test}^{(n)})\}_{n=1}^{N}\)</span></li>
<li>Output: parameters <span class="math inline">\(\Theta\)</span> representing a training algorithm</li>
<li>Objective: good performance on meta-test set <span class="math inline">\(\mathcal{D}_{meta\_test} = \{(D_{train}^{(n&#39;)},D_{test}^{(n&#39;)})\}_{n&#39;=1}^{N}\)</span></li>
</ul></li>
</ul>
<figure>
<img src="https://ws2.sinaimg.cn/large/005PPQ5Ily1g0yn2gf0muj31he0xe7wh.jpg" alt="meta learning, ref:Ravi&amp;Larochelle&#39;17" /><figcaption aria-hidden="true">meta learning, ref:Ravi&amp;Larochelle'17</figcaption>
</figure>
<h3 id="paper-and-method">Paper and Method</h3>
<p>####OPTIMIZATION AS A MODEL FOR FEW-SHOT LEARNING</p>
<p>The standard optimization algorithms used to train deep neural networks are some variant of gradient descent, which uses updates of the form <span class="math display">\[
\theta_t = \theta_{t-1} - \alpha_t \nabla_{\theta_{t-1}}\mathcal{L}_t.
\]</span> Key Observation is that this update resembles the update for the cell state in an LSTM <span class="math display">\[
c_t = f_t \odot c_{t-1} + i_t \odot \tilde{c}_t,
\]</span> if <span class="math inline">\(f_t = 1\)</span>, <span class="math inline">\(c_{t-1} = \theta_{t-1}\)</span>, <span class="math inline">\(i_t = \alpha_t\)</span> and <span class="math inline">\(\tilde{c}_t = - \nabla_{\theta_{t-1}} \mathcal{L}_t\)</span>.</p>
<p>Thus, training a meta-learner LSTM to learn an update rule for training a neural network is key idea.</p>
<p><img src="https://ws4.sinaimg.cn/large/005PPQ5Ily1g0yn09xsp9j313q0mewo9.jpg" /></p>
<p>####Learning to learn by gradient descent by gradient descent</p>
<p>Use LSTM to learn the update strategy of neural network, learn an optimizer by gradient descent method, and then use this optimizer to optimize other network parameters. The task of LSTM optimizer is learning <span class="math inline">\(\nabla_{\theta_{t-1}}\mathcal{L}_t\)</span>.</p>
<p><img src="https://ws1.sinaimg.cn/large/005PPQ5Ily1g0ymzujzn9j31420g80ut.jpg" /></p>
<h4 id="model-agnostic-meta-learning-for-fast-adaptation-of-deep-networks">Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks</h4>
<p>The goal of meta-learning is to train a model that can quickly adapt to a new task using only a few training iterations. To accomplish this, the model or learner is trained during a meta-learning phase on a set of tasks, such that the trained model can quickly adapt to new tasks using only a small number of examples or trials.</p>
]]></content>
      <categories>
        <category>paper notes</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习-决策树</title>
    <url>/2018/01/15/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%86%B3%E7%AD%96%E6%A0%91/</url>
    <content><![CDATA[<p>机器学习——周志华</p>
<p>读书笔记</p>
<p>第四章——决策树</p>
<a id="more"></a>
<h3 id="基本流程">4.1 基本流程</h3>
<p>基本思想：分而治之，每次选择最优属性进行划分</p>
<figure>
<img src="https://ws3.sinaimg.cn/large/005PPQ5Ily1g0ymxx39wkj313q0u2tex.jpg" alt="算法" /><figcaption aria-hidden="true">算法</figcaption>
</figure>
<h3 id="划分选择">4.2 划分选择</h3>
<p>目标：随着划分过程的不断进行，决策树的分支结点所包含的样本尽可能属于同一类别，即结点的纯度（purity）越来越高。</p>
<h4 id="信息增益information-gain">4.2.1 信息增益（information gain)</h4>
<p>信息熵（information entropy）：度量样本集合纯度常用指标，熵越小，纯度越高，假定当前集合D中第k类样本所占的比例为 <span class="math inline">\(p_k(k=1,2,…,|y|)\)</span>，则D的信息熵定义为 <span class="math display">\[
Ent(D) = - \sum_{k=1}^{|y|}p_{k}log{2}p_{k}
\]</span> 信息增益：可以认为是划分前后的熵差，信息增益越大，则意味划分所获的纯度提升越大，即属性“越好”。假定离散属性 a 有 V 个可能的取值<span class="math inline">\({a_{1},a_{2},…a_{v}}\)</span>，若采用 a 对样本集 D 进行划分，则会产生 V 个分支结点，其中第 v 个分支结点包含了 D 中所有在属性 a 上取值为 <span class="math inline">\(a^{v}\)</span> 的样本，记为<span class="math inline">\(D^{v}\)</span>，可以计算出<span class="math inline">\(D^{v}\)</span>的信息熵，再考虑不同分支结点所包含的样本数不同，给分支结点赋予权重<span class="math inline">\(|D^v|/|D|\)</span>，即样本数越多的分支影响越大，于是可计算出用属性 a 对样本集 D 进行划分所获得的信息增益: <span class="math display">\[
Gain(D,a) = Ent(D) - \sum_{v=1}^{V}\frac{|D^v|}{|D|}Ent(D^v)
\]</span> ID3 决策树学习算法 [Quinlan, 1986]即采用该算法来选择划分属性。</p>
<h4 id="增益率gain-ratio">4.2.2 增益率（gain ratio）</h4>
<p>使用信息增益准则的决策树，会对可取值数目较多的属性有所偏好，这种偏好会弱化决策树的泛化能力。</p>
<p>为了减少这种偏好带来的不利影响，C4.5 决策树算法 [Quinlan, 1986]采用增益率来选择最优划分属性，增益率定义如下： <span class="math display">\[
Gain\_ratio(D,a) = \dfrac{Gain(D,a)}{IV(a)}
\]</span> 其中 <span class="math display">\[
IV(a) = - \sum_{v=1}^{V}\frac{|D^v|}{|D|}\log{2}^{\dfrac{|D^{v}|}{|D|}}
\]</span> 属于属性 a 的固有值（intrinsic value），属性 a 的取值数目越多，则 IV(a) 越大。</p>
<p>tips：增益率会对可取值数目较少的属性有所偏好，因此，C4.5算法并不是选择增益率最大的候选划分属性，而是采用启发式[Quinlan, 1993]：</p>
<ol type="1">
<li>从候选属性中找出信息增益高于平均的属性组成集合 W</li>
<li>从 W 中选出增益率最高的作为最优划分属性</li>
</ol>
<h4 id="基尼指数gini-index">4.2.3 基尼指数（Gini index）</h4>
<p>CART 决策树算法[Breiman et al.,1984] 使用基尼系数来选择划分属性，数据集的纯度采用基尼值度量： <span class="math display">\[
\begin{equation}
\begin{aligned}Gini(D) &amp; = \sum_{k=1}^{|y|}\sum_{k^{&#39;}\not{=}k}{p_{k}p_{k^{&#39;}}}\\
&amp; =1-\sum_{k=1}^{|y|}p_{k}^{2}
\end{aligned}
\end{equation}
\]</span> Gini(D)越小，数据集D的纯度越高，属性 a 的基尼指数定义为 <span class="math display">\[
Gini\_index(D,a) = \sum_{v=1}^{V}\frac{|D|}{|D^v|}Gini(D^v)
\]</span></p>
<blockquote>
<p>注意：在原书上此部分讲解较少，CART决策树算法采用二分递归分割技术，每次将当前结点分割为两个样本集，最终生成的是一棵二叉树。因此上述的Gini(D)公式的 y = 2：若属性为离散值且可能的取值大于2，则针对每个可能的取值a，根据样本对 k = a 测试的是否将样本分为两类，计算出Gini值和Gini_index，然后选择使Gini_index最小的值作为划分基准；若属性为连续值，参考下一节离散值处理.</p>
</blockquote>
<p>在候选属性集合A中，选择哪个使得划分后基尼指数最小的属性作为最优划分属性，即 <span class="math inline">\(a_* = \arg min_{a\in{A}}Gini\_index(D,a)\)</span>.</p>
<h3 id="剪枝处理pruning">4.3 剪枝处理（pruning）</h3>
<p>目的：降低过拟合的风险</p>
<p>基本策略：预剪枝（prepruning）、后剪枝（post-pruning）</p>
<blockquote>
<p>采用留出法，将数据集分为训练集和验证集。</p>
</blockquote>
<h4 id="预剪枝">4.3.1 预剪枝</h4>
<p>在决策树生成过程中，对每个结点在划分前先进行估计，若当前结点的划分不能带来决策树泛化能力的提升，则停止划分并将当前结点标记为叶结点。</p>
<ul>
<li>计算划分前在验证集上的精度S1</li>
<li>计算划分后在验证集上的精度S2</li>
<li>若S1 &lt; S2，则划分，反正则不划分，归为叶结点</li>
</ul>
<h4 id="后剪枝">4.3.2 后剪枝</h4>
<p>先从训练集生成一棵完整的决策树，然后自底向上地对非叶结点进行考察，若将该结点对应的子树换成叶结点能带来泛化性能提升，则将该字数换成叶结点。</p>
<ul>
<li>生成完整决策树，然后自底向上执行下面三步</li>
<li>计算当前结点在验证集上的精度S1</li>
<li>计算将当前结点领衔的分支剪除后的精度S2</li>
<li>若S1 &lt; S2 ，则剪枝（将该结点的分支剪除，替换为叶结点），否则不剪枝</li>
</ul>
<h3 id="连续与缺失值">4.4 连续与缺失值</h3>
<h4 id="连续值处理">4.4.1 连续值处理</h4>
<p>对于连续属性，需要进行离散化。最简单的策略是采用二分法（bi-partition），这正是C4.5决策树算法中采用的机制[Quinlan,1993].</p>
<blockquote>
<p>CART决策树算法也采用该方法</p>
</blockquote>
<p>给定样本集D和连续属性a，假定a在D上出现n个不同的取值，将这些值从小到大排序，记为<span class="math inline">\({a^1,a^2,…,a^n}\)</span>. 基于划分点 t 可将D分为子集<span class="math inline">\(D_t^-\)</span>和<span class="math inline">\(D_t^+\)</span>，其中<span class="math inline">\(D_t^-\)</span>包含那些在属性 a 上取值不大于 t 的样本，而 <span class="math inline">\(D_t^+\)</span> 则包含那些在属性 a 上取值大于 t 的样本。对于连续属性 a，我们可考察包含 n-1 个元素的候选划分点集合 <span class="math display">\[
T_a = \left\{\frac{a^i+a^{i+1}}{2} | 1 \le i \le n-1\right\}
\]</span> 即把区间<span class="math inline">\([a^i,a^{i+1})\)</span>的中位数<span class="math inline">\(\frac{a^i+a^{i+1}}{2}\)</span>作为候选划分点，然后，可以像离散属性值一样来考察这些划分点，选择最优的划分点进行样本集合划分。</p>
<h4 id="缺失值处理">4.4.2 缺失值处理</h4>
<p>针对不完整样本，若样本出现大量缺失，简单的放弃是对数据极大的浪费，因此需要考虑利用有缺失属性值的训练样本。</p>
<p>给定训练集 D 和属性 a，令 <span class="math inline">\(\widetilde{D}\)</span> 表示 D 中在属性 a 上没有缺失值的样本子集，假定属性 a 有 V 个可取值 <span class="math inline">\({a^1,a^2,…,a^V}\)</span>, 样本有 y 个类，假设我们为每个样本 x 赋予一个权重 <span class="math inline">\(w_x\)</span> ，定义： <span class="math display">\[
\rho=\frac{\sum_{x\in{\widetilde{D}}}w_x}{\sum_{x\in{D}}w_x}
\\
\widetilde{p}_k=\frac{\sum_{x\in{\widetilde{D}_k}}w_x}{\sum_{x\in{\widetilde{D}}}w_x}
(1\le k \le |y|)
\\
\rho=\frac{\sum_{x\in{\widetilde{D}^v}}w_x}{\sum_{x\in{\widetilde{D}}}w_x}
(1\le v \le V)
\]</span> 对属性a，<span class="math inline">\(\rho\)</span> 表示无缺失样本所占的比例，<span class="math inline">\(\widetilde{p}_k\)</span> 表示无缺失样本中第 k 类所占的比例， <span class="math inline">\(\widetilde{r}_v\)</span> 表示无缺失样本中在属性 a 上取值 <span class="math inline">\(a^v\)</span> 的样本所占的比例，显然： <span class="math display">\[
\sum_{k=1}^{|y|}\widetilde{p}_k = 1, \sum_{v=1}^{V}\widetilde{r}_v=1
\]</span> 基于上述定义，信息增益计算公式推广为 <span class="math display">\[
\begin{equation}
\begin{aligned}
Gain(D,a)&amp;=\rho\times Gain(\widetilde{D},a)
\\
&amp;=\rho \times \left(Ent(\widetilde{D})-\sum_{v=1}^V\widetilde{r}_vEnt(\widetilde{D}^v)\right)
\end{aligned}
\end{equation}
\]</span> 其中 <span class="math display">\[
Ent(\widetilde{D}) = -\sum_{k=1}^{|y|}\widetilde{p}_klog_2\widetilde{p}_k
\]</span> 若样本 x 在划分属性 a 上的取值已知，则将x划入与其取值对应的子结点，且缺中不变。若样本在属性 a 上<strong>取值未知</strong>，则将 x 同时划分到<strong>所有子结点</strong>，且样本权值在与属性值 <span class="math inline">\(a^v\)</span> 对应的子结点中调整为 <span class="math inline">\(\widetilde{r}_v\cdot{w_x}\)</span>, 也就是让同一个样本以不同的概率划分到不同的子结点。</p>
<p>C4.5 算法采用了上述解决方案[Quinlan,1993].</p>
<h3 id="多变量决策树">4.5 多变量决策树</h3>
<p>将每个属性视为坐标空间中的一个坐标轴，则 d 个属性描述的样本就对应了 d 维空间的一个数据点，对样本分类就相当于在这个坐标空间寻找不同类样本的分类边界。传统的单变量决策树形成的分类边界有一个明显的特点：轴平行（axis-parallel），即分类边界是由若干个与坐标轴平行的分段组成。</p>
<p>若采用斜的划分边界，则决策树模型将会简化，这就是多变量决策树（multivariate decision tree），每个非叶节点就是一个线性分类器。如下图</p>
<figure>
<img src="https://wx2.sinaimg.cn/large/005PPQ5Ily1g0ymxxcoa4j30hw0d8wf1.jpg" alt="斜划分边界" /><figcaption aria-hidden="true">斜划分边界</figcaption>
</figure>
<h3 id="个人总结">个人总结</h3>
<h4 id="id3">ID3</h4>
<p>ID3缺点</p>
<ul>
<li>ID3算法不能处理具有连续值的属性（由于ID3以信息增益为准则选择划分属性，对可取值多的属性有所偏好，这样一来，用二分法进行连续属性的离散化处理时，可取值多的属性就越有可能成为分裂属性，而这样其实是没有意义的）</li>
<li>ID3算法不能处理属性具有缺失值的样本</li>
<li>算法会生成很深的树，容易产生过拟合现象</li>
<li>算法一般会优先选择有较多属性值的特征，因为属性值多的特征会有相对较大的信息增益</li>
</ul>
<p>ID4.5是对ID3的改进，修正了其对较多属性值的偏好。C4.5还弥补了ID3中不能处理特征属性值连续的问题。但是，其存在如下缺点</p>
<ul>
<li>算法低效，在构造树的过程中，需要对数据集进行多次的顺序扫描和排序，因而导致算法的低效</li>
<li>内存受限，适合于能够驻留于内存的数据集，当训练集大得无法在内存容纳时程序无法运行</li>
</ul>
<p>CART是二叉树，采用二元切分，即可用于分类也可用于回归。</p>
]]></content>
      <categories>
        <category>artificial intelligence</category>
      </categories>
      <tags>
        <tag>machine learing</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习CTR模型的早期演变</title>
    <url>/2020/05/23/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0CTR%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%97%A9%E6%9C%9F%E6%BC%94%E5%8F%98/</url>
    <content><![CDATA[<blockquote>
<p>Deep Crossing - Web-Scale Modeling without Manually Crafted Combinatorial Features</p>
<p>Deep Learning over Multi-field Categorical Data</p>
<p>Product-based Neural Networks for User Response Prediction</p>
<p>Wide &amp; Deep Learning for Recommender Systems</p>
<p>DeepFM: A Factorization-Machine based Neural Network for CTR Prediction</p>
</blockquote>
<!--- more --->
<h3 id="deep-crossing-2016">Deep Crossing (2016)</h3>
<p>倒退回最开始的时候，如果让我们设计一个最简单的DNN用于CTR，那么很容易引入一个embedding layer将稀疏特征（one-hot encoding）转换为密集嵌入（dense embedding），然后将其concat后输入到一个多层的神经网络，最终获得输入。而这就是Deep Cross的思想，所以说如果可以穿越到过去做科研还真是一大幸事。</p>
<p>框架图如下，和普通的DNN不同的是，Deep Crossing 还引入了残差网络，效果有明显提升。</p>
<p><img src="https://cdn.jsdelivr.net/gh/yinxiaojian/blog-img@master/20200524182256.png" /></p>
<h3 id="fnn-2016">FNN (2016)</h3>
<p>和 Deep Crossing相比，FNN使用FM的隐层向量作为作为DNN的输入，也就是对Embedding层进行预训练。</p>
<blockquote>
<p>从时间来看FNN和Deep Crossing属于同一时间的产物，如果FNN晚于Deep Crossing，那么单单凭借预训练这一工程技巧恐怕贡献是不足的。两篇文章思考的角度不同带来的结果也有所不同。</p>
</blockquote>
<p>框架图如下：</p>
<p><img src="https://raw.githubusercontent.com/yinxiaojian/blog-img/master/20200524194525.png" /></p>
<h3 id="pnn-2016">PNN (2016)</h3>
<p>PNN 在embedding layer 和 hidden layer之间引入了product layer，在改层引入了特征交叉项。相比较FNN使用多层感知器对特征关联进行建模，PNN这种方式更加直接有效。</p>
<blockquote>
<p>从某种意义上说，如果将输入的dense embedding看作 LR 和 FM 中的 one-hot encoding的话，FNN 和 PNN 的关系就很像 LR 和 FM。</p>
</blockquote>
<p>框架图如下，事实上去掉product layer中的 p 部分（特征交叉部分），那么PNN将会退化为FNN，就像FM去掉特征交叉会退化为LR一样。</p>
<p><img src="https://raw.githubusercontent.com/yinxiaojian/blog-img/master/20200524193722.png" /></p>
<h3 id="wide-deep-2016">Wide &amp; Deep (2016)</h3>
<p>google这篇文章的出发点为如何同时实现memorization和generalization。</p>
<p>memorization：记忆能力，学习历史数据集中已经出现过的物品或特征关联。</p>
<p>generalization：泛化能力，基于相关性的传递原理，探究以前未出现或者极少出现的特征关联。</p>
<blockquote>
<p>人可以很好的达到memorization和generalization。就像做题一样，我们可以记住很多类型的题（memorization），然后举一反三（generalization）。</p>
</blockquote>
<p>传统的线形模型使用one-hot编码处理离散特征，但是不能对特征关联进行建模。因此一般通过人工构建特征组合（cross-product transformations）的方式，获取非线性建模能力，也就是记忆能力。但是历史数据中未出现过的特征关联则无法构建，因此LR仍不具备泛化能力。</p>
<p>对于FM和DNN这类embedding-based的模型，不需要特征工程，通过对低维度的dense embedding进行组合可以学习到历史数据中未出现的特征组合，具备泛化能力。但是如果query-item矩阵稀疏、高秩的，学习有效的embedding是很困难的。在这种情况下，会导致过度泛化，从而导致一些不太相关的推荐。相对的，线性模型+cross product则能用很少的参数记住exception rules。</p>
<blockquote>
<p>这里的exception relues指的是？个人理解就是历史数据中存在的规则。</p>
</blockquote>
<p>因此，本文提出一个Wide &amp; deep 学习框架，通过对线形模型（wide model）和神经网络（wide model）进行联合训练，以达到记忆和泛化能力。</p>
<p>框架图如下，本质就是对线性模型和神经网络的预测结果进行加权，最后联合训练。在实践中，用哪些作为wide和deep的输入是一个需要仔细考虑的问题。</p>
<p><img src="https://raw.githubusercontent.com/yinxiaojian/blog-img/master/20200523184007.png" /></p>
<h3 id="deepfm-2017">DeepFM (2017)</h3>
<p>DeepFM强调线形模型只能捕捉一阶特征，而FM可以捕捉二阶特征关联，DNN可以捕捉更高阶特征关联。因此其在wide &amp; deep基础上，将wide部分替换为FM模型（FM的线性部分可以捕捉一阶特征），从而达到捕捉一阶、二阶、甚至是高阶的特征关联。</p>
<p>事实上wide &amp; deep也可以捕捉多阶特征关联，但是其wide部分需要进行特征工程，也就是上文提到的 cross-product transormations。而DeepFM则无需特征工程，可以直接进行训练。</p>
<p>框架图如下：</p>
<p><img src="https://raw.githubusercontent.com/yinxiaojian/blog-img/master/20200524180808.png" /></p>
<p>该文还对以上几种方法进行了简单的对比：</p>
<p><img src="https://raw.githubusercontent.com/yinxiaojian/blog-img/master/20200524180944.png" /></p>
]]></content>
      <categories>
        <category>paper notes</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title>CNN反向传播算法</title>
    <url>/2018/03/21/CNN%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<blockquote>
<p>全文翻译自 Notes on Convolutional Neural Networks</p>
<p><a href="http://web.mit.edu/jvb/www/papers/cnn_tutorial.pdf">论文链接</a></p>
<p>释部分为自己的补充说明，可能存在错误</p>
</blockquote>
<!--- more --->
<p>###1 介绍</p>
<p>本文档讨论CNN的推导和实现，并加上一些简单的扩展。相比权值，卷积神经网络涉及到更多的连接。其架构本身实现了正则化。此外，CNN在某种程度上实现了平移不变性。这种特殊的神经网络假设我们希望通过数据驱动的方式学习过滤器，用来提取输入特征。推导是在二维的数据和卷积的基础上展开的，但可以很容易的推广到任意维度。</p>
<p>我们先对经典的BP算法做一个描述，然后推导2D卷积神经网络中卷积层和子采样层（池化层）的BP权值更新方法。在整个过程中，我们强调了视线的效率，给出MATLAB代码片段配合方程讲解。然后讨论如何自动组合前一层的feature map，最后是feature map的稀疏组合。</p>
<p>免责声明：这个笔记可能有错误。</p>
<blockquote>
<p>feature map 就是特征矩阵，一张图片的每个像素点的RGB值可分别组成矩阵（R矩阵，G矩阵，B矩阵），这些就是feature map</p>
</blockquote>
<p>###2 全连接的反向传播算法</p>
<p>一个典型的卷积神经网络，开始是卷积层和子采样层交替，最后几层（最接近输出）是全连接的一维网络（全连接层）。当你准备将2D feature map作为一维全连接网络的输入时，直接将所有特征连接为一个一维向量，然后就可以利用BP算法。因此在讨论CNN之前，这里先对BP算法进行解释。</p>
<p>####2.1 前向传播 Feedforward Pass</p>
<p>在接下来的推导中，我们考虑平方和误差函数，对于一个有c个类别和N个训练样本的多分类问题，计算公式如下： <span class="math display">\[
E^N=\frac{1}{2}\sum_{n=1}^{N}\sum_{k=1}^{c}(t_k^n-y_k^n)^2
\]</span> <span class="math inline">\(t_k^n\)</span>是第n个样本对应标签的第k个维度的值，<span class="math inline">\(y_k^n\)</span>表示第n个样本对应的网络输出的第k个值。对于多分类问题，输出一般为“one-of-c”形式。如果样本<span class="math inline">\(x^n\)</span>属于第k类，那么<span class="math inline">\(t_k^n\)</span>为正，<span class="math inline">\(t^n\)</span>的其他维度即<span class="math inline">\(t_{k\neq1}^n\)</span>为0或负数。这取决于输出激活函数的选择，此处不做讨论。</p>
<p>因为整个训练集的误差为每个训练样本的误差和，所以我们先讨论一个样本。第n个样本的误差如下： <span class="math display">\[
E^N=\frac{1}{2}\sum_{n=1}^{N}\sum_{k=1}^c(t_k^n-y_k^n)^2=\frac{1}{2}||t^n-y^n||_2^2\tag{1}
\]</span> 在全连接层，我们可以根据反向传播规则计算E对网络权值的偏导数。用<span class="math inline">\(l\)</span>代表当前层，输出层被指定为<span class="math inline">\(L\)</span>层，输入层为第1层，那么当前层的输出为 <span class="math display">\[
x^l=f(u^l), with \space u^l=W^lx^{l-1}+b^l\tag{2}
\]</span></p>
<p>输出激活函数<span class="math inline">\(f(\cdot)\)</span>一般选择logistic(sigmoid)函数<span class="math inline">\(f(x)=(1+e^{-\beta x})^{-1}\)</span>或者hyperbolic tangent函数<span class="math inline">\(f(x)=a\space tanh(bx)\)</span>。logistic函数将<span class="math inline">\([-\infty,\infty]\)</span>映射到<span class="math inline">\([0,1]\)</span>，hyperbolic tangent函数将<span class="math inline">\([-\infty,\infty]\)</span>映射到<span class="math inline">\([-a,a]\)</span>，因此hyperbolic tangent函数的输出平均值一般趋于0，sigmoid函数的输出平均为非零。但是如果将训练数据归一化到均值为0方差为1，可以在梯度下降时增加收敛。对于归一化的数据集来说，hyperbolic tangent函数是较好的选择。LeCun推荐<span class="math inline">\(a=1.7159,b=2/3\)</span>，所以最大非线性点在<span class="math inline">\(f(\pm1)=\pm1\)</span>，因此对预期训练目标进行正则化将会避免训练饱和。</p>
<p>####2.2 反向传播 Backpropagation Pass</p>
<p>通过网络反向传播的误差可以看作每个神经元关于偏差扰动的敏感度（sensitivities），通过链式法则可得：</p>
<blockquote>
<p>敏感度就是误差对目标的偏导数</p>
</blockquote>
<p><span class="math display">\[
\begin{equation}
\frac{\partial{E}}{\partial{b}}=\frac{\partial{E}}{\partial{u}}\frac{\partial{u}}{\partial{b}}=\delta
\end{equation}\tag{3}
\]</span></p>
<p>因为<span class="math inline">\(\frac{\partial{u}}{\partial{b}}=1\)</span>，所以bias的敏感度和误差对一个神经元的输入的偏导数是相等的。利用这个关系将高层误差反向传播到底层，使用下面的递推关系式： <span class="math display">\[
\delta^t=(W^{l+1})^T\delta^{l+1}\circ f&#39;(u^t)\tag{4}
\]</span> 其中<span class="math inline">\(\circ\)</span>表示对应位置元素相乘，从公式（1）可以看到，输出层神经元的敏感度会有一些不同： <span class="math display">\[
\delta^L=f&#39;(u^L)\circ(y^n-t^n)
\]</span> 最后，对每个神经元运用delta规则进行更行，即对神经元的输入使用delta进行缩放。用向量的形式表述就是：输入向量（前一层的输出）和灵敏度向量之间的一个外积： <span class="math display">\[
\frac{\partial{E}}{\partial{W^l}}=x^{l-1}(\delta^l)^T\tag{5}
\]</span></p>
<p><span class="math display">\[
\bigtriangleup W^l=-\eta\frac{\partial{E}}{\partial{W^l}}\tag{6}
\]</span></p>
<p>更新偏置值（bias）也是相同的原理，在实际中通常为每个权值赋予一个特定的学习率<span class="math inline">\(\eta_{ij}\)</span>。</p>
<h3 id="卷积神经网络-convolutional-neural-networks">3 卷积神经网络 Convolutional Neural Networks</h3>
<blockquote>
<p>公式符号释意</p>
<p>在本文中卷积核有多个，每个卷积核和feature map有多个维度，例如一张图片有RGB值，那么就有三个维度</p>
<p><span class="math inline">\(i\)</span>----卷积核或者feature的第几维度 <span class="math inline">\(j\)</span>----第几个卷积核 <span class="math inline">\(up()\)</span>----上采样函数 <span class="math inline">\(down()\)</span>----下采样函数 <span class="math inline">\(l\)</span>----当前层</p>
</blockquote>
<p>通常卷积层和子采样层交替以减少计算时间并逐步建立空间和结构不变性。为了同时保持特异性，需要小的子采样因子。这并不是什么新鲜的概念，但简洁有效。这种模型在哺乳动物的视觉皮层非常常见，在过去十年，听觉神经科学发现，这些相同的设计范例可以在多种不同动物皮层的主要和带状听觉区域中找到。 分级分析和学习架构可能仍然是听觉领域的关键。</p>
<p>####3.1 卷积层 Convolution Layers</p>
<p>我们现在推导卷积层的BP更行。在一个卷积层，上一层的feature map与一个卷积核进行卷积运算，然后通过一个激活函数获得输出feature map。每个输出map可能是多个卷积结果的结合，公式表述： <span class="math display">\[
x_j^l=f\left(\sum_{i\in M_j}x_i^{l-1}*k_{ij}^{l}+b_j^l\right)
\]</span></p>
<p><span class="math inline">\(M_j\)</span>表示输入maps的集合，在MATLAB中卷积的运算需采用"valid"模式。通常情况下，输入maps选择一对或者三个。后面我们会讨论如何自动选择需要组合的特征map。每一个输出map都会加上一个偏执项<span class="math inline">\(b\)</span>，但是对于一个特定的输出map，输入maps会和相同的卷积核进行卷积运算，也就是说，如果输出map <span class="math inline">\(j\)</span> 和 <span class="math inline">\(k\)</span> 都是由map <span class="math inline">\(i\)</span>通过卷积运算的结果，那么对应的卷积核是不同的。</p>
<blockquote>
<p>一张图片有RGB三个颜色通道，则对应的卷积核也是三维的，输出的feature map就是三个卷积结果的结合</p>
<figure>
<img src="https://ws3.sinaimg.cn/large/005PPQ5Ily1g0ymwrjv8jg30m50iqaob.gif" alt="CNN_02" /><figcaption aria-hidden="true">CNN_02</figcaption>
</figure>
</blockquote>
<p>3.1.1 Computing the Gradients</p>
<p>我们假设每个卷积层<span class="math inline">\(l\)</span>后面都接一个子采样层 <span class="math inline">\(l+1\)</span>。在使用BP算法计算 <span class="math inline">\(l\)</span> 层每个节点的敏感度时，我们需要先对下一层（<span class="math inline">\(l+1\)</span>层）连接当前层（<span class="math inline">\(l\)</span>层）的节点的敏感度求和，再乘以这些连接对应的权值（连接<span class="math inline">\(l\)</span>层和<span class="math inline">\(l+1\)</span>层的权值），最后乘以当前层<span class="math inline">\(l\)</span>的该神经元节点的输入<span class="math inline">\(u\)</span>的激活函数<span class="math inline">\(f\)</span>的导数值。因为下采样的存在，采样层的一个像素（神经元）对应的灵敏度<span class="math inline">\(\delta\)</span>对应于卷积层（上一层）的输出map的一块像素（采样窗口大小）。因此，<span class="math inline">\(l\)</span>层中的map的每个节点只与<span class="math inline">\(l+1\)</span>层中相应map的一个节点连接。为了有效计算<span class="math inline">\(l\)</span>层的敏感度，我们可以对下采样层的敏感，度map进行“上采样(upsample)”操作，这样可以使下采样层的敏感度map的大小和卷积层的map大小相同，然后将<span class="math inline">\(l\)</span>层的激活值偏导map和<span class="math inline">\(l+1\)</span>层的上采样敏感度map对应元素相乘。因为下采样层map的权值均是一个常数<span class="math inline">\(\beta\)</span>（见3.2节），所以我们只需要将上一步得到的结果乘以 $ $ 就可以得到<span class="math inline">\(l\)</span>层的敏感度map <span class="math inline">\(\delta^l\)</span>。我们可以在卷积层的每个feature map进行相同的计算过程： <span class="math display">\[
\delta_j^l=\beta_j^{l+1}\left(f&#39;(u_j^l)\circ up(\delta_j^{l+1})\right)
\]</span> 其中的 <span class="math inline">\(up(\cdot)\)</span>表示上采样操作，如果下采样过程的采样因子为<span class="math inline">\(n\)</span>，只需将每个像素点在水平方向和垂直方向复制<span class="math inline">\(n\)</span>次就可以实现，一种有效的实现方式是利用 Kronecker product: <span class="math display">\[
up(x)=x\otimes1_{n\times n}
\]</span></p>
<blockquote>
<p>Ref:<a href="https://zh.wikipedia.org/zh/克罗内克积">克罗内克积</a></p>
</blockquote>
<p>现在对于一个给定的map，我们已经可以计算其敏感度map，然后我们可以简单的对敏感度map的所有节点求和计算出其bias（偏置项）梯度: <span class="math display">\[
\frac{\partial{E}}{\partial{b_j}}=\sum_{u,v}(\delta_j^l)_{uv}
\]</span> 最后，使用BP算法计算权重（卷积核）的梯度，很多权重在连接中是共享的，因此我们需要对所有与该权值有联系（权值共享的连接）的连接对该点求梯度，然后对这些梯度进行求和，就像计算bias梯度一样： <span class="math display">\[
\frac{\partial{E}}{\partial{k_{ij}^l}}=\sum_{u,v}(\delta_j^l)_{uv}(p_i^{l-1})_{uv}\tag{7}
\]</span> 其中<span class="math inline">\((p_i^{l-1})_{uv}\)</span>是<span class="math inline">\(x_i^{l-1}\)</span>在卷积运算时和<span class="math inline">\(k_{ij}^l\)</span>按元素相乘得到输出map的<span class="math inline">\((u,v)\)</span>位置值的patch。看上去好像我们我刻意追踪输入map中哪些patch与输出map中的哪些像素（及其对应的敏感map）相对应，但是公式(7)可以通过卷积运算获得: <span class="math display">\[
\frac{\partial{E}}{\partial{k_{ij}^l}}=rot180(conv2(x_i^{l-1},rot180(\delta_j^l),&#39;valid&#39;))
\]</span> 这里我们对<span class="math inline">\(\delta\)</span>敏感度map做旋转操作是为了这样就可以进行互相关计算而不是卷积，之后将输出旋转回来，以便当我们在前馈阶段执行卷积时，卷积核将具有正确的方向。</p>
<p>####3.2 子采样层 Sub-sampling Layers</p>
<p>子采样层对输入maps进行downsample操作（池化），输入有N个maps，输出也会有N个maps，只是每个maps变小了，公式表述如下： <span class="math display">\[
x_j^l=f\left(\beta_j^ldown(x_j^{l-1})+b_j^l\right)
\]</span></p>
<p>其中的<span class="math inline">\(down(\cdot)\)</span>是下采样函数。典型的操作一般是对输入图像的不同<span class="math inline">\(n\times n\)</span>的块的像素进行求和。这样输出图像在两个维度上都缩小了n倍。每个输出map都有一个属于自己的乘性偏置β和一个加性偏置b。</p>
<p>3.2.1 Computing the Gradients</p>
<p>这里的困难在于如何计算敏感度maps。只要我们能得到敏感度maps，需要更新的只有bias参数 <span class="math inline">\(\beta\)</span> 和 <span class="math inline">\(b\)</span> 。如果子采样层的下一层是全连接层，那么子采样层的敏感度maps只需要利用BP算法就可以容易获得。所以我们假设子采样层的上一层和下一层都是卷积层。</p>
<p>当我们在3.1.1节计算卷积核梯度时，我们需要找到输入map中patch和输出map的像素的对应关系。这里就是必须找到当前层的敏感度map中那个patch与下一层的敏感度矩阵的的给定像素对应，这样就可以像公式 (4)那样应用<span class="math inline">\(\delta\)</span>递推。 当然，将输入patch和输出像素之间连接相乘的权重恰好是（旋转的）卷积核的权重。利用卷积可以实现该计算： <span class="math display">\[
\delta_j^l=f&#39;(u_j^l)\circ conv2(\delta_j^{l+1},rot180(k_j^{l+1}),full)
\]</span> 如前所述，我们旋转内核是为了是卷积函数实施互相关计算。同时，我们需要对卷积边界进行“full”处理，Matlab的卷积函数会自动执行这个过程，对缺少的输入像素进行补零操作。</p>
<p>现在我们可以很容易的计算 <span class="math inline">\(b\)</span> 和 <span class="math inline">\(\beta\)</span> 的梯度，加性基 <span class="math inline">\(b\)</span> 的处理和之前一样，将敏感度map中所有元素相加即可 ： <span class="math display">\[
\frac{\partial E}{\partial b_j}=\sum_{u,v}(\delta_j^l)_{uv}
\]</span> 对于乘性偏置<span class="math inline">\(\beta\)</span> ，参与了前向传播时下采样map的运算，所以在前向计算时将这些maps保存，这样避免了在反向传播时重复计算。定义： <span class="math display">\[
d_j^l=down(x_j^{l-1})
\]</span></p>
<p><span class="math inline">\(\beta\)</span> 梯度计算公式如下： <span class="math display">\[
\frac{\partial E}{\partial \beta_j}=\sum_{u,v}(\delta_j^l)
\]</span></p>
<blockquote>
<p>补充说明：对rot180的解释</p>
<p>假设第<span class="math inline">\(l-1\)</span>层输出<span class="math inline">\(x^l\)</span>是一个3x3矩阵，第<span class="math inline">\(l\)</span>层卷积核<span class="math inline">\(k^l\)</span>是一个2x2的矩阵，步幅为1，输出一个2x2的矩阵<span class="math inline">\(z^l\)</span>，则有（简化<span class="math inline">\(b^l=0\)</span>，单卷积核) <span class="math display">\[
x^{l-1}*k^{l}=z^{l}
\]</span> 矩阵形式表达： <span class="math display">\[
\left( \begin{array}{}
x_{11}&amp;x_{12}&amp;x_{13}\\
x_{21}&amp;x_{22}&amp;x_{23}\\
x_{31}&amp;x_{32}&amp;x_{33}
\end{array}
\right)
*
\left( \begin{array}{}
k_{11}&amp;k_{12}\\
k_{21}&amp;k_{22}
\end{array}
\right)
=
\left( \begin{array}{}
z_{11}&amp;z_{12}\\
z_{21}&amp;z_{22}
\end{array}
\right)
\]</span> 根据卷积的定义可得： <span class="math display">\[
z_{11}=x_{11}k_{11}+x_{12}k_{12}+x_{21}k_{21}+x_{22}k_{22}\\
z_{12}=x_{12}k_{11}+x_{13}k_{12}+x_{22}k_{21}+x_{23}k_{22}\\
z_{21}=x_{21}k_{11}+x_{22}k_{12}+x_{31}k_{21}+x_{32}k_{22}\\
z_{22}=x_{22}k_{11}+x_{23}k_{12}+x_{32}k_{21}+x_{33}k_{22}
\]</span> 第<span class="math inline">\(l\)</span>层的梯度误差为<span class="math inline">\(\delta^l\)</span>，反向传导： <span class="math display">\[
\frac{\partial{E}}{\partial{k^l}}=\frac{\partial{E}}{\partial{z^l}}\frac{\partial{z^l}}{\partial{k^l}}=\delta^l\frac{z^l}{k^l}
\]</span> 因此对于卷积核<span class="math inline">\(k^l\)</span>的梯度为第<span class="math inline">\(l\)</span>层的敏感度（梯度）乘上<span class="math inline">\(\frac{\partial{z^l}}{\partial{k^l}}\)</span>，分别计算可得 <span class="math display">\[
\bigtriangledown{k_{11}}=\delta_{11}x_{11}+\delta_{12}x_{22}+\delta_{21}x_{21}+\delta_{22}x_{22}\\
\bigtriangledown{k_{12}}=\delta_{11}x_{12}+\delta_{12}x_{13}+\delta_{21}x_{22}+\delta_{22}x_{23}\\
\bigtriangledown{k_{21}}=\delta_{11}x_{21}+\delta_{12}x_{22}+\delta_{21}x_{31}+\delta_{22}x_{32}\\
\bigtriangledown{k_{22}}=\delta_{11}x_{22}+\delta_{12}x_{23}+\delta_{21}x_{32}+\delta_{22}x_{33}
\]</span> 上面4个式子用矩阵卷积形式表示： <span class="math display">\[
\left( \begin{array}{}
x_{11}&amp;x_{12}&amp;x_{13}\\
x_{21}&amp;x_{22}&amp;x_{23}\\
x_{31}&amp;x_{32}&amp;x_{33}
\end{array}
\right)
*
\left( \begin{array}{}
\delta_{11}&amp;\delta_{12}\\
\delta_{21}&amp;\delta_{22}
\end{array}
\right)
=
\left( \begin{array}{}
\bigtriangledown k_{11}&amp;\bigtriangledown k_{12}\\
\bigtriangledown k_{21}&amp;\bigtriangledown k_{22}
\end{array}
\right)
\]</span> 公式化表达为： <span class="math display">\[
\frac{\partial{E}}{\partial{k^l}}=x^{l-1}*\delta^l
\]</span></p>
<p>这里需要注意，在论文中进行了两次旋转，这是因为在MATLAB中conv2函数在计算卷积时除了会对矩阵进行“0”扩展，还会将卷积核进行旋转，然后再计算。例如：</p>
<pre><code class="hljs matlab">a =
     <span class="hljs-number">1</span>     <span class="hljs-number">1</span>     <span class="hljs-number">1</span>
     <span class="hljs-number">1</span>     <span class="hljs-number">1</span>     <span class="hljs-number">1</span>
     <span class="hljs-number">1</span>     <span class="hljs-number">1</span>     <span class="hljs-number">1</span>
k =
     <span class="hljs-number">1</span>     <span class="hljs-number">2</span>     <span class="hljs-number">3</span>
     <span class="hljs-number">4</span>     <span class="hljs-number">5</span>     <span class="hljs-number">6</span>
     <span class="hljs-number">7</span>     <span class="hljs-number">8</span>     <span class="hljs-number">9</span>

&gt;&gt; convn(a,k,<span class="hljs-string">&#x27;full&#x27;</span>)
<span class="hljs-built_in">ans</span> =

     <span class="hljs-number">1</span>     <span class="hljs-number">3</span>     <span class="hljs-number">6</span>     <span class="hljs-number">5</span>     <span class="hljs-number">3</span>
     <span class="hljs-number">5</span>    <span class="hljs-number">12</span>    <span class="hljs-number">21</span>    <span class="hljs-number">16</span>     <span class="hljs-number">9</span>
    <span class="hljs-number">12</span>    <span class="hljs-number">27</span>    <span class="hljs-number">45</span>    <span class="hljs-number">33</span>    <span class="hljs-number">18</span>
    <span class="hljs-number">11</span>    <span class="hljs-number">24</span>    <span class="hljs-number">39</span>    <span class="hljs-number">28</span>    <span class="hljs-number">15</span>
     <span class="hljs-number">7</span>    <span class="hljs-number">15</span>    <span class="hljs-number">24</span>    <span class="hljs-number">17</span>     <span class="hljs-number">9</span></code></pre>
<p>因此在编写代码时需要注意保持一致（要么旋转，要么不旋转）。</p>
<p>在3.2节计算 <span class="math inline">\(\delta^l_j\)</span> 也是同理</p>
<p>利用反向传播： <span class="math display">\[
\frac{\partial{E}}{\partial{x^{l-1}}}=\frac{\partial{E}}{\partial{z^l}}\frac{\partial{z^l}}{\partial{x^{l-1}}}=\delta^l\frac{z^l}{x^{l-1}}
\]</span> 利用上式，可得： <span class="math display">\[
\bigtriangledown{x_{11}}=\delta_{11}k_{11}\\
\bigtriangledown{x_{12}}=\delta_{11}k_{12}+\delta_{12}k_{11}\\
\bigtriangledown{x_{13}}=\delta_{12}k_{12}\\
\bigtriangledown{x_{21}}=\delta_{11}k_{21}+\delta_{21}k_{11}\\
\bigtriangledown{x_{22}}=\delta_{11}k_{22}+\delta_{12}k_{21}+\delta_{21}k_{12}+\delta_{22}k_{11}\\
\bigtriangledown{x_{23}}=\delta_{12}k_{22}+\delta_{22}k_{12}\\
\bigtriangledown{x_{31}}=\delta_{21}k_{21}\\
\bigtriangledown{x_{32}}=\delta_{21}k_{22}+\delta_{22}k_{21}\\
\bigtriangledown{x_{33}}=\delta_{22}k_{22}
\]</span> 上面九个式子用矩阵卷积形式表示： <span class="math display">\[
\left( \begin{array}{}
0&amp;0&amp;0&amp;0\\
0&amp;\delta_{11}&amp;\delta_{12}&amp;0\\
0&amp;\delta_{21}&amp;\delta_{22}&amp;0\\
0&amp;0&amp;0&amp;0
\end{array}
\right)
*
\left( \begin{array}{}
k_{22}&amp;k_{21}\\
k_{12}&amp;k_{11}
\end{array}
\right)
=
\left( \begin{array}{}
\bigtriangledown x_{11}&amp;\bigtriangledown x_{12}&amp;\bigtriangledown x_{13}\\
\bigtriangledown x_{21}&amp;\bigtriangledown x_{22}&amp;\bigtriangledown x_{23}\\
\bigtriangledown x_{31}&amp;\bigtriangledown x_{32}&amp;\bigtriangledown x_{33}
\end{array}
\right)
\]</span> 公式化表达为： <span class="math display">\[
\frac{\partial{E}}{\partial{x^{l-1}}}=\delta^l * rot180(k^l)
\]</span></p>
</blockquote>
<p>####3.3 学习特征maps组合 Learning Combinations of Feature Maps</p>
<p>通常，对不同的maps进行卷积并对结果求和获得一个输出map，往往能取得不错的效果。在一些文献中，通过人工选择输入maps进行组合。但是我们可以尝试通过训练获得这个组合。让 <span class="math inline">\(a_{ij}\)</span> 表示得到第 <span class="math inline">\(j\)</span> 个输出map中第 <span class="math inline">\(i\)</span> 个输入map的权重，那么第 <span class="math inline">\(j\)</span> 个输出map的定义如下： <span class="math display">\[
x_j^l=f\left( \sum_{i=1}^{N_{in}}a_{ij}(x_i^{l-1}*k_i^l)+b_j^l\right)
\]</span> 同时需满足以下约束： <span class="math display">\[
\sum_ia_ij=1,and \space 0\leq a_{ij}\leq1
\]</span> 这些约束可以通过将变量<span class="math inline">\(a_{ij}\)</span>表示为softmax形式来加强： <span class="math display">\[
a_{ij}=\frac{exp(c_{ij})}{\sum_kexp(c_{kj})}
\]</span></p>
<p>因为对于固定的 <span class="math inline">\(j\)</span> 来说，每组权值 <span class="math inline">\(c_{ij}\)</span> 都和其他组权值相独立，所以为了方便描述，我们把下标 <span class="math inline">\(j\)</span> 去掉，只考虑单个map的更新，其他map的更新方式是相同的过程，只是索引 <span class="math inline">\(j\)</span> 不同。</p>
<p>softmax函数的导数： <span class="math display">\[
\frac{\partial a_k}{\partial c_i}=\delta_{ki}a_i-a_ia_k\tag{8}
\]</span> 这里的 <span class="math inline">\(\delta\)</span> 是 kronecker delta，参照公式 (1) 我们可以得到在 <span class="math inline">\(l\)</span> 层误差对 <span class="math inline">\(a_i\)</span> 的偏导： <span class="math display">\[
\frac{\partial E}{\partial a_i}=\frac{\partial E}{\partial u^l}\frac{\partial u^l}{\partial a_i}=\sum_{u,v}(\delta^l \circ (x_i^{l-1}*k_i^l))_{uv}
\]</span> 这里的<span class="math inline">\(\delta^l\)</span>对应具有输入 <span class="math inline">\(u\)</span> 的输出map的敏感度map。和前面一样，这里的卷积运算也是“valid”类型，目的是使结果和sensitivity map大小匹配。最后使用链式法则计算损失函数对权值 <span class="math inline">\(c_i\)</span> 的偏导数： <span class="math display">\[
\begin{align}
\frac{\partial E}{\partial c_i}&amp;=\sum_k\frac{\partial E}{\partial a_k}\frac{\partial a_k}{\partial c_i}\tag{9}\\
&amp;=a_i\left( \frac{\partial E}{\partial a_i}-\sum_k\frac{\partial E}{\partial a_k}a_k\right)\tag{10}
\end{align}
\]</span> 3.3.1 Enforcing Sparse Combinations</p>
<p>为了给 <span class="math inline">\(a_i\)</span> 增加稀疏约束（限制一个输出map只与某些而不是全部输入map相连接），我们在代价函数中添加正则项惩罚 <span class="math inline">\(\Omega(a)\)</span> 。这样就可以使某些权值趋于0，最后只有部分输入maps参与输出map相连接，代价函数为： <span class="math display">\[
\widetilde{E}^n=E^n + \lambda\sum{i,j}|(a){ij}|\tag{11}
\]</span> 然后求这个正则项对 <span class="math inline">\(c_i\)</span>梯度的影响： <span class="math display">\[
\frac{\partial\Omega}{\partial a_i}=\lambda sign(a_i)\tag{12}
\]</span></p>
<p>结合公式 (8) 的结果： <span class="math display">\[
\begin{align}
\frac{\partial\Omega}{\partial c_i}&amp; = \sum_k\frac{\partial\Omega}{\partial a_k}\frac{\partial a_k}{\partial c_i} \\
&amp;=
\lambda\left(|a_i|-a_i\sum_k|a_k|\right)
\end{align}
\]</span> 最后结合公式 (13) 和公式 (9) ，可以求的权重 <span class="math inline">\(c_i\)</span> 的梯度： <span class="math display">\[
\frac{\partial\widetilde{E}^n}{\partial c_i} = \frac{\partial E^n}{\partial c_i} +\frac{\partial \Omega}{\partial c_i}
\]</span></p>
<h4 id="加快matlab训练速度-making-it-fast-with-matlab">3.4 加快MATLAB训练速度 Making it Fast with MATLAB</h4>
<blockquote>
<p>与CNN关系不大，不做翻译，可看原文</p>
</blockquote>
<h3 id="实际训练问题-practical-training-issues-incomplete">4 实际训练问题 Practical Training Issues (Incomplete)</h3>
<blockquote>
<p>与CNN关系不大，不做翻译，可看原文</p>
</blockquote>
<p>$$</p>
<p>$$</p>
]]></content>
      <categories>
        <category>artificial intelligence</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title>SpringMVC Controllers 说明文档</title>
    <url>/2018/01/05/SpringMVC-Controllers-%E8%AF%B4%E6%98%8E%E6%96%87%E6%A1%A3/</url>
    <content><![CDATA[<p>翻译自官方文档，并加上了自己的理解，解释更加直白。</p>
<p>官方文档地址：<a href="https://docs.spring.io/spring/docs/current/spring-framework-reference/web.html#mvc">Spring Web MVC</a></p>
<a id="more"></a>
<h3 id="declaration">1 Declaration</h3>
<p>使用@Controller注解标记一个类，这个类就是一个SpringMVC Controller对象。</p>
<pre><code class="hljs java"><span class="hljs-meta">@Controller</span>
<span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">HelloController</span> </span>&#123;

    <span class="hljs-meta">@GetMapping(&quot;/hello&quot;)</span>
    <span class="hljs-function"><span class="hljs-keyword">public</span> String <span class="hljs-title">handle</span><span class="hljs-params">(Model model)</span> </span>&#123;
        model.addAttribute(<span class="hljs-string">&quot;message&quot;</span>, <span class="hljs-string">&quot;Hello World!&quot;</span>);
        <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;index&quot;</span>;
    &#125;
&#125;</code></pre>
<p>在XML文件进行配置，告诉Spring应该到哪里去找Controller控制器，加上如下一行，base-package 即controller所在位置</p>
<pre><code class="hljs java">&lt;context:component-scan base-<span class="hljs-keyword">package</span>=<span class="hljs-string">&quot;org.example.web&quot;</span>/&gt;</code></pre>
<p>完整XML配置文件</p>
<pre><code class="hljs xml"><span class="hljs-meta">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span>
<span class="hljs-tag">&lt;<span class="hljs-name">beans</span> <span class="hljs-attr">xmlns</span>=<span class="hljs-string">&quot;http://www.springframework.org/schema/beans&quot;</span></span>
<span class="hljs-tag">    <span class="hljs-attr">xmlns:xsi</span>=<span class="hljs-string">&quot;http://www.w3.org/2001/XMLSchema-instance&quot;</span></span>
<span class="hljs-tag">    <span class="hljs-attr">xmlns:p</span>=<span class="hljs-string">&quot;http://www.springframework.org/schema/p&quot;</span></span>
<span class="hljs-tag">    <span class="hljs-attr">xmlns:context</span>=<span class="hljs-string">&quot;http://www.springframework.org/schema/context&quot;</span></span>
<span class="hljs-tag">    <span class="hljs-attr">xsi:schemaLocation</span>=<span class="hljs-string">&quot;</span></span>
<span class="hljs-tag"><span class="hljs-string">        http://www.springframework.org/schema/beans</span></span>
<span class="hljs-tag"><span class="hljs-string">        http://www.springframework.org/schema/beans/spring-beans.xsd</span></span>
<span class="hljs-tag"><span class="hljs-string">        http://www.springframework.org/schema/context</span></span>
<span class="hljs-tag"><span class="hljs-string">        http://www.springframework.org/schema/context/spring-context.xsd&quot;</span>&gt;</span>

    <span class="hljs-tag">&lt;<span class="hljs-name">context:component-scan</span> <span class="hljs-attr">base-package</span>=<span class="hljs-string">&quot;org.example.web&quot;</span>/&gt;</span>

    <span class="hljs-comment">&lt;!-- ... --&gt;</span>

<span class="hljs-tag">&lt;/<span class="hljs-name">beans</span>&gt;</span></code></pre>
<h3 id="request-mapping">2 Request Mapping</h3>
<p><span class="citation" data-cites="RequestMapping">@RequestMapping</span> 注解用于映射Request请求与与controller的处理方法。该注解有多个参数可以配置Request请求的属性，如URL，HTTP method，request parameters，headers，media types。</p>
<p><span class="citation" data-cites="RequestMapping">@RequestMapping</span> 可以用于类也可以用于类方法。当@RequestMapping 标记在Controller 类上的时候，里面使用@RequestMapping 标记的方法的请求地址都是相对于类上的@RequestMapping 而言的；当Controller 类上没有标记@RequestMapping 注解时，方法上的@RequestMapping 都是绝对路径。最终路径都是相对于跟路径"/"的。通过这种组合的方法可以限制Request的匹配。</p>
<p>在SpringMVC 4.3 引入了组合注解来简化@RequestMapping的写法。</p>
<ul>
<li><span class="citation" data-cites="GetMapping">@GetMapping</span></li>
<li><span class="citation" data-cites="PostMapping">@PostMapping</span></li>
<li><span class="citation" data-cites="PutMapping">@PutMapping</span></li>
<li><span class="citation" data-cites="DeleteMapping">@DeleteMapping</span></li>
<li><span class="citation" data-cites="PatchMapping">@PatchMapping</span></li>
</ul>
<p>如@GetMapping等价于<span class="citation" data-cites="RequestMapping">@RequestMapping</span>(method = RequestMethod.GET)，组合注解通常用于method上，如下</p>
<pre><code class="hljs java"><span class="hljs-meta">@RestController</span>
<span class="hljs-meta">@RequestMapping(&quot;/persons&quot;)</span>
<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">PersonController</span> </span>&#123;

    <span class="hljs-meta">@GetMapping(&quot;/&#123;id&#125;&quot;)</span>
    <span class="hljs-function"><span class="hljs-keyword">public</span> Person <span class="hljs-title">getPerson</span><span class="hljs-params">(<span class="hljs-meta">@PathVariable</span> Long id)</span> </span>&#123;
        <span class="hljs-comment">// ...</span>
    &#125;

    <span class="hljs-meta">@PostMapping</span>
    <span class="hljs-meta">@ResponseStatus(HttpStatus.CREATED)</span>
    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">add</span><span class="hljs-params">(<span class="hljs-meta">@RequestBody</span> Person person)</span> </span>&#123;
        <span class="hljs-comment">// ...</span>
    &#125;
&#125;</code></pre>
<h4 id="uri-patterns">2.1 URI patterns</h4>
<p>我们可以使用通配符去匹配request</p>
<ul>
<li><code>?</code>匹配一个字符</li>
<li><code>*</code>匹配一个路径段中的零个或多个字符</li>
<li><code>**</code>匹配零个或多个路径段</li>
</ul>
<p>可以使用@PathVariable来声明URI变量并获取其值:</p>
<pre><code class="hljs java"><span class="hljs-meta">@GetMapping(&quot;/owners/&#123;ownerId&#125;/pets/&#123;petId&#125;&quot;)</span>
<span class="hljs-function"><span class="hljs-keyword">public</span> Pet <span class="hljs-title">findPet</span><span class="hljs-params">(<span class="hljs-meta">@PathVariable</span> Long ownerId, <span class="hljs-meta">@PathVariable</span> Long petId)</span> </span>&#123;
    <span class="hljs-comment">// ...</span>
&#125;</code></pre>
<p>URI变量即可以在类层级声明，也可以在方法层级声明:</p>
<pre><code class="hljs java"><span class="hljs-meta">@Controller</span>
<span class="hljs-meta">@RequestMapping(&quot;/owners/&#123;ownerId&#125;&quot;)</span>
<span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">OwnerController</span> </span>&#123;

    <span class="hljs-meta">@GetMapping(&quot;/pets/&#123;petId&#125;&quot;)</span>
    <span class="hljs-function"><span class="hljs-keyword">public</span> Pet <span class="hljs-title">findPet</span><span class="hljs-params">(<span class="hljs-meta">@PathVariable(&quot;ownerId&quot;)</span> Long ownerId, <span class="hljs-meta">@PathVariable</span> Long petId)</span> </span>&#123;
        <span class="hljs-comment">// ...</span>
    &#125;
&#125;</code></pre>
<p>URI变量会进行自动类型转换或者抛出<code>TypeMismatchException</code>异常，对于简单的类型，如<code>int, long, Data</code>，默认自动转换，对于复杂类型在此不做详解。</p>
<p>URI变量作为参数时，有两种声明方式：</p>
<ol type="1">
<li>显性声明，如上代码块： <code>@PathVariable("ownerId")</code>，这种声明方式明确规定使用的是URI模版里的ownerId变量。</li>
<li>直接使用@PathVariable，如上代码块<code>@PathVariable Long petId</code>，这种情况下会默认去URI模版寻找跟参数名相同的变量，但只能在使用debug模式才可以。</li>
</ol>
<p>Request还支持正则匹配，语法格式：<code>&#123;varName:regex&#125;</code>，用regex声明了一个URI变量varName，例如：</p>
<pre><code class="hljs java"><span class="hljs-meta">@GetMapping(&quot;/&#123;name:[a-z-]+&#125;-&#123;version:\\d\\.\\d\\.\\d&#125;&#123;ext:\\.[a-z]+&#125;&quot;)</span>
<span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">handle</span><span class="hljs-params">(<span class="hljs-meta">@PathVariable</span> String version, <span class="hljs-meta">@PathVariable</span> String ext)</span> </span>&#123;
    <span class="hljs-comment">// ...</span>
&#125;</code></pre>
<h4 id="pattern-comparison">2.2 Pattern comparison</h4>
<p>当有多个patterns（模版）匹配到URL时，通过<code>AntPathMatcher.getPatternComparator(String path)</code>去获取最合适的patterns。</p>
<p>对于每一个pattern，根据URI变量和通配符的个数计算出分数，分数越低优先度越高。相同分数则较长者优先度高。</p>
<p>默认映射模版<code>/**</code>不参与比较，优先度最低。</p>
<h4 id="matrix-variables">2.3 Matrix variables</h4>
<p>Matrix variables可以出现在任意路径段，每个matrix variable由 ";" 分割，例如<code>/cars;color=red;year=2012</code>。多个值既可以用 "," 分割，如<code>color=red,green,blue</code>，也可以重复变量名，如<code>color=red;color=green;color=blue</code>。</p>
<p>如果一个URL可能含有matrix variables，那么请求映射模版必须使用URI模版去表示。这样可以确保匹配正确，即使matrix variables的位置不固定或不存在。</p>
<p>如下例子，获取matrix variable "q"</p>
<pre><code class="hljs java"><span class="hljs-comment">// GET /pets/42;q=11;r=22</span>

<span class="hljs-meta">@GetMapping(&quot;/pets/&#123;petId&#125;&quot;)</span>
<span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">findPet</span><span class="hljs-params">(<span class="hljs-meta">@PathVariable</span> String petId, <span class="hljs-meta">@MatrixVariable</span> <span class="hljs-keyword">int</span> q)</span> </span>&#123;

    <span class="hljs-comment">// petId == 42</span>
    <span class="hljs-comment">// q == 11</span>
&#125;</code></pre>
<p>多个路径段包含matrix variables的情况：</p>
<pre><code class="hljs java"><span class="hljs-comment">// GET /owners/42;q=11/pets/21;q=22</span>

<span class="hljs-meta">@GetMapping(&quot;/owners/&#123;ownerId&#125;/pets/&#123;petId&#125;&quot;)</span>
<span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">findPet</span><span class="hljs-params">(</span></span>
<span class="hljs-function"><span class="hljs-params">        <span class="hljs-meta">@MatrixVariable(name=&quot;q&quot;, pathVar=&quot;ownerId&quot;)</span> <span class="hljs-keyword">int</span> q1,</span></span>
<span class="hljs-function"><span class="hljs-params">        <span class="hljs-meta">@MatrixVariable(name=&quot;q&quot;, pathVar=&quot;petId&quot;)</span> <span class="hljs-keyword">int</span> q2)</span> </span>&#123;

    <span class="hljs-comment">// q1 == 11</span>
    <span class="hljs-comment">// q2 == 22</span>
&#125;</code></pre>
<p>可以设置matrix variable的required属性，<code>required = false</code>表示该参数不是必须存在的，同时可以设置defaultValue赋予默认值。如下：</p>
<pre><code class="hljs Java"><span class="hljs-comment">// GET /pets/42</span>

<span class="hljs-meta">@GetMapping(&quot;/pets/&#123;petId&#125;&quot;)</span>
<span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">findPet</span><span class="hljs-params">(<span class="hljs-meta">@MatrixVariable(required=false, defaultValue=&quot;1&quot;)</span> <span class="hljs-keyword">int</span> q)</span> </span>&#123;

    <span class="hljs-comment">// q == 1</span>
&#125;</code></pre>
<p>可以将所有的matrix variables放置于一个Map中：</p>
<pre><code class="hljs java"><span class="hljs-comment">// GET /owners/42;q=11;r=12/pets/21;q=22;s=23</span>

<span class="hljs-meta">@GetMapping(&quot;/owners/&#123;ownerId&#125;/pets/&#123;petId&#125;&quot;)</span>
<span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">findPet</span><span class="hljs-params">(</span></span>
<span class="hljs-function"><span class="hljs-params">        <span class="hljs-meta">@MatrixVariable</span> MultiValueMap&lt;String, String&gt; matrixVars,</span></span>
<span class="hljs-function"><span class="hljs-params">        <span class="hljs-meta">@MatrixVariable(pathVar=&quot;petId&quot;&quot;)</span> MultiValueMap&lt;String, String&gt; petMatrixVars)</span> </span>&#123;

    <span class="hljs-comment">// matrixVars: [&quot;q&quot; : [11,22], &quot;r&quot; : 12, &quot;s&quot; : 23]</span>
    <span class="hljs-comment">// petMatrixVars: [&quot;q&quot; : 22, &quot;s&quot; : 23]</span>
&#125;</code></pre>
<p>最后注意：matrix variables默认是不启用的，因此我们需要在xml文件中进行配置:</p>
<pre><code class="hljs Xml"><span class="hljs-meta">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span>
<span class="hljs-tag">&lt;<span class="hljs-name">beans</span> <span class="hljs-attr">xmlns</span>=<span class="hljs-string">&quot;http://www.springframework.org/schema/beans&quot;</span></span>
<span class="hljs-tag">    <span class="hljs-attr">xmlns:mvc</span>=<span class="hljs-string">&quot;http://www.springframework.org/schema/mvc&quot;</span></span>
<span class="hljs-tag">    <span class="hljs-attr">xmlns:xsi</span>=<span class="hljs-string">&quot;http://www.w3.org/2001/XMLSchema-instance&quot;</span></span>
<span class="hljs-tag">    <span class="hljs-attr">xsi:schemaLocation</span>=<span class="hljs-string">&quot;</span></span>
<span class="hljs-tag"><span class="hljs-string">        http://www.springframework.org/schema/beans</span></span>
<span class="hljs-tag"><span class="hljs-string">        http://www.springframework.org/schema/beans/spring-beans.xsd</span></span>
<span class="hljs-tag"><span class="hljs-string">        http://www.springframework.org/schema/mvc</span></span>
<span class="hljs-tag"><span class="hljs-string">        http://www.springframework.org/schema/mvc/spring-mvc.xsd&quot;</span>&gt;</span>

    <span class="hljs-tag">&lt;<span class="hljs-name">mvc:annotation-driven</span> <span class="hljs-attr">enable-matrix-variables</span>=<span class="hljs-string">&quot;true&quot;</span>/&gt;</span>

<span class="hljs-tag">&lt;/<span class="hljs-name">beans</span>&gt;</span></code></pre>
<h4 id="consumable-media-types">2.4 Consumable media types</h4>
<p>利用<code>Content-Type</code>对请求匹配范围进行限制，从而缩小请求的映射范围。</p>
<pre><code class="hljs java"><span class="hljs-meta">@PostMapping(path = &quot;/pets&quot;, consumes = &quot;application/json&quot;)</span>
<span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">addPet</span><span class="hljs-params">(<span class="hljs-meta">@RequestBody</span> Pet pet)</span> </span>&#123;
    <span class="hljs-comment">// ...</span>
&#125;</code></pre>
<p>consumes 属性支持否定表达，<code>!text/plain</code>表示除了 "text/plain" 的所有content type。</p>
<p>consumes可以声明在class层级，与其他request mapping attributes不同的是，当声明在class层级时，method层级的consumes属性会覆盖而不是扩展class层级的声明。</p>
<h4 id="producible-media-types">2.5 Producible media types</h4>
<p>利用<code>Accept</code>对请求匹配范围进行限制，从而缩小请求的映射范围。类似2.4:</p>
<pre><code class="hljs java"><span class="hljs-meta">@GetMapping(path = &quot;/pets/&#123;petId&#125;&quot;, produces = &quot;application/json;charset=UTF-8&quot;)</span>
<span class="hljs-meta">@ResponseBody</span>
<span class="hljs-function"><span class="hljs-keyword">public</span> Pet <span class="hljs-title">getPet</span><span class="hljs-params">(<span class="hljs-meta">@PathVariable</span> String petId)</span> </span>&#123;
    <span class="hljs-comment">// ...</span>
&#125;</code></pre>
<h4 id="params-method-headers">2.6 params, method, headers</h4>
<ul>
<li>params 属性用于指定请求参数</li>
<li>method 属性用于限制能够访问的方法类型 //和组合注解@GetMapping等类似</li>
<li>headers 属性用于指定请求头信息</li>
</ul>
<p>三者都可以缩小请求的映射范围，支持否定表达。</p>
<pre><code class="hljs java"><span class="hljs-meta">@GetMapping(path = &quot;/pets/&#123;petId&#125;&quot;, params = &quot;myParam=myValue&quot;)</span>
<span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">findPet</span><span class="hljs-params">(<span class="hljs-meta">@PathVariable</span> String petId)</span> </span>&#123;
    <span class="hljs-comment">// ...</span>
&#125;
<span class="hljs-meta">@GetMapping(path = &quot;/pets&quot;, headers = &quot;myHeader=myValue&quot;)</span>
<span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">findPet1</span><span class="hljs-params">(<span class="hljs-meta">@PathVariable</span> String petId)</span> </span>&#123;
    <span class="hljs-comment">// ...</span>
&#125;
<span class="hljs-comment">//和findPet1等价</span>
<span class="hljs-meta">@RequestMapping(path = &quot;/pets&quot;, headers = &quot;myHeader=myValue&quot;, method = RequestMethod. GET)</span>
<span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">findPet2</span><span class="hljs-params">(<span class="hljs-meta">@PathVariable</span> String petId)</span> </span>&#123;
    <span class="hljs-comment">// ...</span>
&#125;</code></pre>
<h3 id="handler-methods">3 Handler Methods</h3>
<h4 id="method-arguments">3.1 Method Arguments</h4>
<blockquote>
<p>引用自官方文档</p>
</blockquote>
<table>
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr class="header">
<th>Controller method argument</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>WebRequest</code>, <code>NativeWebRequest</code></td>
<td>Generic access to request parameters, request &amp; session attributes, without direct use of the Servlet API.</td>
</tr>
<tr class="even">
<td><code>javax.servlet.ServletRequest</code>, <code>javax.servlet.ServletResponse</code></td>
<td>Choose any specific request or response type — e.g. <code>ServletRequest</code>, <code>HttpServletRequest</code>, or Spring’s <code>MultipartRequest</code>, <code>MultipartHttpServletRequest</code>.</td>
</tr>
<tr class="odd">
<td><code>javax.servlet.http.HttpSession</code></td>
<td>Enforces the presence of a session. As a consequence, such an argument is never <code>null</code>.<strong>Note:</strong> Session access is not thread-safe. Consider setting the<code>RequestMappingHandlerAdapter</code>'s "synchronizeOnSession" flag to "true" if multiple requests are allowed to access a session concurrently.</td>
</tr>
<tr class="even">
<td><code>javax.servlet.http.PushBuilder</code></td>
<td>Servlet 4.0 push builder API for programmatic HTTP/2 resource pushes. Note that per Servlet spec, the injected <code>PushBuilder</code> instance can be null if the client does not support that HTTP/2 feature.</td>
</tr>
<tr class="odd">
<td><code>java.security.Principal</code></td>
<td>Currently authenticated user; possibly a specific <code>Principal</code> implementation class if known.</td>
</tr>
<tr class="even">
<td><code>HttpMethod</code></td>
<td>The HTTP method of the request.</td>
</tr>
<tr class="odd">
<td><code>java.util.Locale</code></td>
<td>The current request locale, determined by the most specific <code>LocaleResolver</code> available, in effect, the configured <code>LocaleResolver</code>/<code>LocaleContextResolver</code>.</td>
</tr>
<tr class="even">
<td>Java 6+: <code>java.util.TimeZone</code>Java 8+: <code>java.time.ZoneId</code></td>
<td>The time zone associated with the current request, as determined by a <code>LocaleContextResolver</code>.</td>
</tr>
<tr class="odd">
<td><code>java.io.InputStream</code>, <code>java.io.Reader</code></td>
<td>For access to the raw request body as exposed by the Servlet API.</td>
</tr>
<tr class="even">
<td><code>java.io.OutputStream</code>, <code>java.io.Writer</code></td>
<td>For access to the raw response body as exposed by the Servlet API.</td>
</tr>
<tr class="odd">
<td><code>@PathVariable</code></td>
<td>For access to URI template variables. See <a href="https://docs.spring.io/spring/docs/current/spring-framework-reference/web.html#mvc-ann-requestmapping-uri-templates">URI patterns</a>.</td>
</tr>
<tr class="even">
<td><code>@MatrixVariable</code></td>
<td>For access to name-value pairs in URI path segments. See <a href="https://docs.spring.io/spring/docs/current/spring-framework-reference/web.html#mvc-ann-matrix-variables">Matrix variables</a>.</td>
</tr>
<tr class="odd">
<td><code>@RequestParam</code></td>
<td>For access to Servlet request parameters. Parameter values are converted to the declared method argument type. See <span class="citation" data-cites="RequestParam">[@RequestParam]</span>(https://docs.spring.io/spring/docs/current/spring-framework-reference/web.html#mvc-ann-requestparam).</td>
</tr>
<tr class="even">
<td><code>@RequestHeader</code></td>
<td>For access to request headers. Header values are converted to the declared method argument type. See <span class="citation" data-cites="RequestHeader">[@RequestHeader]</span>(https://docs.spring.io/spring/docs/current/spring-framework-reference/web.html#mvc-ann-requestheader).</td>
</tr>
<tr class="odd">
<td><code>@RequestBody</code></td>
<td>For access to the HTTP request body. Body content is converted to the declared method argument type using <code>HttpMessageConverter</code>s. See <span class="citation" data-cites="RequestBody">[@RequestBody]</span>(https://docs.spring.io/spring/docs/current/spring-framework-reference/web.html#mvc-ann-requestbody).</td>
</tr>
<tr class="even">
<td><code>HttpEntity&lt;B&gt;</code></td>
<td>For access to request headers and body. The body is converted with <code>HttpMessageConverter</code>s. See <a href="https://docs.spring.io/spring/docs/current/spring-framework-reference/web.html#mvc-ann-httpentity">HttpEntity</a>.</td>
</tr>
<tr class="odd">
<td><code>@RequestPart</code></td>
<td>For access to a part in a "multipart/form-data" request. See <span class="citation" data-cites="RequestPart">[@RequestPart]</span>(https://docs.spring.io/spring/docs/current/spring-framework-reference/web.html#mvc-multipart-forms-non-browsers) and <a href="https://docs.spring.io/spring/docs/current/spring-framework-reference/web.html#mvc-multipart">Multipart requests</a>.</td>
</tr>
<tr class="even">
<td><code>java.util.Map</code>, <code>org.springframework.ui.Model</code>, <code>org.springframework.ui.ModelMap</code></td>
<td>For access and updates of the implicit model that is exposed to the web view.</td>
</tr>
<tr class="odd">
<td><code>RedirectAttributes</code></td>
<td>Specify attributes to use in case of a redirect — i.e. to be appended to the query string, and/or flash attributes to be stored temporarily until the request after redirect. See <a href="https://docs.spring.io/spring/docs/current/spring-framework-reference/web.html#mvc-redirecting-passing-data">Redirect attributes</a> and <a href="https://docs.spring.io/spring/docs/current/spring-framework-reference/web.html#mvc-flash-attributes">Flash attributes</a>.</td>
</tr>
<tr class="even">
<td>Command or form object (with optional <code>@ModelAttribute</code>)</td>
<td>Command object whose properties to bind to request parameters — via setters or directly to fields, with customizable type conversion, depending on <code>@InitBinder</code> methods and/or the HandlerAdapter configuration (see the <code>webBindingInitializer</code> property on<code>RequestMappingHandlerAdapter</code>).Command objects along with their validation results are exposed as model attributes, by default using the command class name - e.g. model attribute "orderAddress" for a command object of type "some.package.OrderAddress". <code>@ModelAttribute</code> can be used to customize the model attribute name.</td>
</tr>
<tr class="odd">
<td><code>Errors</code>, <code>BindingResult</code></td>
<td>Validation results for the command/form object data binding; this argument must be declared immediately after the command/form object in the controller method signature.</td>
</tr>
<tr class="even">
<td><code>SessionStatus</code></td>
<td>For marking form processing complete which triggers cleanup of session attributes declared through a class-level <code>@SessionAttributes</code>annotation.</td>
</tr>
<tr class="odd">
<td><code>UriComponentsBuilder</code></td>
<td>For preparing a URL relative to the current request’s host, port, scheme, context path, and the literal part of the servlet mapping also taking into account <code>Forwarded</code> and <code>X-Forwarded-*</code> headers.</td>
</tr>
<tr class="even">
<td><code>@SessionAttribute</code></td>
<td>For access to any session attribute; in contrast to model attributes stored in the session as a result of a class-level <code>@SessionAttributes</code>declaration.</td>
</tr>
<tr class="odd">
<td><code>@RequestAttribute</code></td>
<td>For access to request attributes.</td>
</tr>
</tbody>
</table>
<h4 id="return-values">3.2 Return Values</h4>
<blockquote>
<p>引用自官方文档</p>
</blockquote>
<table>
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr class="header">
<th>Controller method return value</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>@ResponseBody</code></td>
<td>The return value is converted through <code>HttpMessageConverter</code>s and written to the response. See <span class="citation" data-cites="ResponseBody">[@ResponseBody]</span>(https://docs.spring.io/spring/docs/current/spring-framework-reference/web.html#mvc-ann-responsebody).</td>
</tr>
<tr class="even">
<td><code>HttpEntity&lt;B&gt;</code>, <code>ResponseEntity&lt;B&gt;</code></td>
<td>The return value specifies the full response including HTTP headers and body be converted through <code>HttpMessageConverter</code>s and written to the response. See <a href="https://docs.spring.io/spring/docs/current/spring-framework-reference/web.html#mvc-ann-httpentity">HttpEntity</a>.</td>
</tr>
<tr class="odd">
<td><code>HttpHeaders</code></td>
<td>For returning a response with headers and no body.</td>
</tr>
<tr class="even">
<td><code>String</code></td>
<td>A view name to be resolved with <code>ViewResolver</code>'s and used together with the implicit model — determined through command objects and <code>@ModelAttribute</code>methods. The handler method may also programmatically enrich the model by declaring a <code>Model</code>argument (see above).</td>
</tr>
<tr class="odd">
<td><code>View</code></td>
<td>A <code>View</code> instance to use for rendering together with the implicit model — determined through command objects and <code>@ModelAttribute</code> methods. The handler method may also programmatically enrich the model by declaring a <code>Model</code> argument (see above).</td>
</tr>
<tr class="even">
<td><code>java.util.Map</code>, <code>org.springframework.ui.Model</code></td>
<td>Attributes to be added to the implicit model with the view name implicitly determined through a <code>RequestToViewNameTranslator</code>.</td>
</tr>
<tr class="odd">
<td><code>ModelAndView</code> object</td>
<td>The view and model attributes to use, and optionally a response status.</td>
</tr>
<tr class="even">
<td><code>void</code></td>
<td>A method with a <code>void</code> return type (or <code>null</code> return value) is considered to have fully handled the response if it also has a <code>ServletResponse</code>, or an <code>OutputStream</code> argument, or an <code>@ResponseStatus</code> annotation. The same is true also if the controller has made a positive ETag or lastModified timestamp check (see <span class="citation" data-cites="Controller">[@Controller caching]</span>(https://docs.spring.io/spring/docs/current/spring-framework-reference/web.html#mvc-caching-etag-lastmodified) for details).If none of the above is true, a <code>void</code>return type may also indicate "no response body" for REST controllers, or default view name selection for HTML controllers.</td>
</tr>
<tr class="odd">
<td><code>Callable&lt;V&gt;</code></td>
<td>Produce any of the above return values asynchronously in a Spring MVC managed thread.</td>
</tr>
<tr class="even">
<td><code>DeferredResult&lt;V&gt;</code></td>
<td>Produce any of the above return values asynchronously from any thread — e.g. possibly as a result of some event or callback.</td>
</tr>
<tr class="odd">
<td>ListenableFuture<V>, java.util.concurrent.CompletionStage<V>, java.util.concurrent.CompletableFuture<V></td>
<td>Alternative to <code>DeferredResult</code> as a convenience for example when an underlying service returns one of those.</td>
</tr>
<tr class="even">
<td><code>ResponseBodyEmitter</code>, <code>SseEmitter</code></td>
<td>Emit a stream of objects asynchronously to be written to the response with<code>HttpMessageConverter</code>'s; also supported as the body of a <code>ResponseEntity</code>.</td>
</tr>
<tr class="odd">
<td><code>StreamingResponseBody</code></td>
<td>Write to the response <code>OutputStream</code> asynchronously; also supported as the body of a<code>ResponseEntity</code>.</td>
</tr>
<tr class="even">
<td>Reactive types — Reactor, RxJava, or others via <code>ReactiveAdapterRegistry</code></td>
<td>Alternative to `<code>DeferredResult</code>with multi-value streams (e.g. <code>Flux</code>, <code>Observable</code>) collected to a <code>List</code>.For streaming scenarios — .e.g. <code>text/event-stream</code>, <code>application/json+stream</code>,<code>SseEmitter</code> and <code>ResponseBodyEmitter</code> are used instead, where <code>ServletOutputStream</code> blocking I/O is performed on a Spring MVC managed thread and back pressure applied against the completion of each write.See <a href="https://docs.spring.io/spring/docs/current/spring-framework-reference/web.html#mvc-ann-async-reactive-types">Reactive return values</a>.</td>
</tr>
<tr class="odd">
<td>Any other return type</td>
<td>A single model attribute to be added to the implicit model with the view name implicitly determined through a <code>RequestToViewNameTranslator</code>; the attribute name may be specified through a method-level <code>@ModelAttribute</code> or otherwise a name is selected based on the class name of the return type.</td>
</tr>
</tbody>
</table>
<h4 id="requestparam">3.3 <span class="citation" data-cites="RequestParam">@RequestParam</span></h4>
<p>使用 <span class="citation" data-cites="RequestParam">@RequestParam</span> 绑定 HttpServletRequest 请求参数到控制器方法参数：</p>
<pre><code class="hljs java"><span class="hljs-meta">@Controller</span>
<span class="hljs-meta">@RequestMapping(&quot;/pets&quot;)</span>
<span class="hljs-meta">@SessionAttributes(&quot;pet&quot;)</span>
<span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">EditPetForm</span> </span>&#123;

    <span class="hljs-comment">// ...</span>

    <span class="hljs-meta">@GetMapping</span>
    <span class="hljs-function"><span class="hljs-keyword">public</span> String <span class="hljs-title">setupForm</span><span class="hljs-params">(<span class="hljs-meta">@RequestParam(&quot;petId&quot;)</span> <span class="hljs-keyword">int</span> petId, ModelMap model)</span> </span>&#123;
        Pet pet = <span class="hljs-keyword">this</span>.clinic.loadPet(petId);
        model.addAttribute(<span class="hljs-string">&quot;pet&quot;</span>, pet);
        <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;petForm&quot;</span>;
    &#125;

    <span class="hljs-comment">// ...</span>

&#125;</code></pre>
<p>绑定的参数默认必须存在，可以通过required属性修改，如<code>@RequestParam(name="id", required=false)</code>。</p>
<p>如果控制器方法参数类型不是string，将会执行自动类型转换。</p>
<h4 id="requestheader">3.4 <span class="citation" data-cites="RequestHeader">@RequestHeader</span></h4>
<p>使用@RequestHeader绑定绑定 HttpServletRequest 头信息到控制器方法参数。</p>
<p>一个request header例子:</p>
<pre><code class="hljs apache"><span class="hljs-attribute">Host</span>                    localhost:<span class="hljs-number">8080</span>
<span class="hljs-attribute">Accept</span>                  text/html,application/xhtml+xml,application/xml;q=<span class="hljs-number">0</span>.<span class="hljs-number">9</span>
<span class="hljs-attribute">Accept</span>-Language         fr,en-gb;q=<span class="hljs-number">0</span>.<span class="hljs-number">7</span>,en;q=<span class="hljs-number">0</span>.<span class="hljs-number">3</span>
<span class="hljs-attribute">Accept</span>-Encoding         gzip,deflate
<span class="hljs-attribute">Accept</span>-Charset          ISO-<span class="hljs-number">8859</span>-<span class="hljs-number">1</span>,utf-<span class="hljs-number">8</span>;q=<span class="hljs-number">0</span>.<span class="hljs-number">7</span>,*;q=<span class="hljs-number">0</span>.<span class="hljs-number">7</span>
<span class="hljs-attribute">Keep</span>-Alive              <span class="hljs-number">300</span></code></pre>
<p>下面的例子演示了如何获取 <code>Accept-Encoding</code>和 <code>Keep-Alive</code> 的值：</p>
<pre><code class="hljs java"><span class="hljs-meta">@RequestMapping(&quot;/displayHeaderInfo.do&quot;)</span>
<span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">displayHeaderInfo</span><span class="hljs-params">(<span class="hljs-meta">@RequestHeader(&quot;Accept-Encoding&quot;)</span> String encoding,</span></span>
<span class="hljs-function"><span class="hljs-params">        <span class="hljs-meta">@RequestHeader(&quot;Keep-Alive&quot;)</span> <span class="hljs-keyword">long</span> keepAlive)</span> </span>&#123;
    <span class="hljs-comment">//...</span>
&#125;</code></pre>
<p>如果控制器方法参数类型不是string，将会执行自动类型转换。</p>
<h4 id="cookievalue">3.5 <span class="citation" data-cites="CookieValue">@CookieValue</span></h4>
<p>使用@CookieValue绑定绑定 HttpServletRequest 的cookie信息到控制器方法参数。</p>
<p>假设http request中有如下cookie信息：</p>
<pre><code class="hljs apache"><span class="hljs-attribute">JSESSIONID</span>=<span class="hljs-number">415</span>A<span class="hljs-number">4</span>AC<span class="hljs-number">178</span>C<span class="hljs-number">59</span>DACE<span class="hljs-number">0</span>B<span class="hljs-number">2</span>C<span class="hljs-number">9</span>CA<span class="hljs-number">727</span>CDD<span class="hljs-number">84</span></code></pre>
<p>下面的例子演示了如何获取JESSIONID的值：</p>
<pre><code class="hljs java"><span class="hljs-meta">@RequestMapping(&quot;/displayHeaderInfo.do&quot;)</span>
<span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">displayHeaderInfo</span><span class="hljs-params">(<span class="hljs-meta">@CookieValue(&quot;JSESSIONID&quot;)</span> String cookie)</span> </span>&#123;
    <span class="hljs-comment">//...</span>
&#125;</code></pre>
<h4 id="modelattribute">3.6 <span class="citation" data-cites="ModelAttribute">@ModelAttribute</span></h4>
<p>//TODO</p>
]]></content>
      <categories>
        <category>SpringMVC</category>
      </categories>
      <tags>
        <tag>Controllers</tag>
      </tags>
  </entry>
</search>
